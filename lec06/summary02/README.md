# Lecture 06 - Quantization (Part II)

## 6.5 Quantization-Aware Training(QAT)

**Quantization-Aware Training**(QAT)ì´ë€ training í˜¹ì€ re-training(fine-tuning)ì„ ê±°ì³ ìµœì ì˜ quantization schemeì„ ì°¾ëŠ” ë°©ë²•ì´ë‹¤.

| | PTQ | QAT |
| :---: | :---: | :---: |
| ì†ë„ | ëŒ€ì²´ë¡œ ë¹ ë¥´ë‹¤ | ëŠë¦¬ë‹¤ |
| re-training ì—¬ë¶€ | ë¶ˆí•„ìš” | í•™ìŠµ í˜¹ì€ finetune í•„ìš” |
| Plug and Play | ê°€ëŠ¥ | re-trainingì´ í•„ìš” |
| accuracy | ì •í™•ë„ í•˜ë½ì„ ì¡°ì ˆí•˜ê¸° í˜ë“¤ë‹¤. | ë¹„êµì  ì •í™•ë„ í•˜ë½ì„ ì¡°ì ˆí•  ìˆ˜ ìˆë‹¤. |

> Plug and Play(PnP): ë³„ë‹¤ë¥¸ ì„¤ì •ì—†ì´ ë°”ë¡œ ì ìš©ì´ ê°€ëŠ¥í•˜ë©´ PnPë¼ê³  í•œë‹¤.

> PTQëŠ” data, backpropagationë„ í•„ìš”í•˜ì§€ ì•Šê³  êµ¬ì¡°ì˜ ë³€í™”ë„ ì—†ì–´ì„œ í¸ë¦¬í•˜ë‹¤.

---

### 6.5.1 Considerations for QAT

> [Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding ë…¼ë¬¸(2015)](https://arxiv.org/abs/1510.00149)

ìš°ì„  K-means-based quantization, linear quantization ë°©ì‹ì˜ forward ê³¼ì •ê³¼ backward ê³¼ì •ì„ ë³µìŠµí•´ ë³´ì.

- K-means-based quantization

    ![K-means-based quantization forward, backward](images/K-means-based_forward_backward.png)

    - forward, backward ê³¼ì •ì„ ê±°ì¹˜ë©° centroidsë¥¼ fine-tuningí•œë‹¤. 
    
    - weightê°€ ì–´ë–¤ clusterì— ì†í•˜ëŠ”ê°€ì— ë”°ë¼ ê·¸ë£¹ìœ¼ë¡œ ë¶„ë¥˜í•œ ë’¤ í‰ê· ê°’ìœ¼ë¡œ updateê°€ ì§„í–‰ë˜ë‹¤.

- linear quantization

    ![linear quantization forward, backward](images/linear_forward_backward.png)

    - Activation quantization

        input range [a, b]ì˜ Exponential Moving Averages(ì´ë™ í‰ê· )ì„ ê¸°ë¡í•œë‹¤.

        forward passì—ì„œëŠ” EMAë¥¼ ì°¸ê³ í•´ scaling factorë¥¼ ë„ì¶œí•˜ì—¬ ì–‘ìí™”í•œë‹¤.

    - Weight quantization

        min-max ë°©ë²•ìœ¼ë¡œ ì–‘ìí™”í•œë‹¤.

ê·¸ëŸ°ë° ìœ„ linear quantization ê³¼ì •ì—ì„œ weight quantization, activation quantization ë‹¨ê³„ëŠ”, **fake quantization**(simulated quantization) ë°©ë²•ì„ í†µí•´ ë‹¤ì–‘í•œ ì¡°ê±´ì„ ì‹¤í—˜í•  ìˆ˜ ìˆë‹¤.

- full precision ë³µì‚¬ë³¸ì€ ê·¸ëŒ€ë¡œ ìœ ì§€í•œ ì±„, simulationí•˜ë©´ì„œ quantization ì„±ëŠ¥ì„ ê²€ì¦í•œë‹¤. 

> ë‹¨, í•™ìŠµ ì´ˆê¸°(ì´ˆê¸° 5ë§Œ~200ë§Œ step)ì—ëŠ” fake quantizationì„ í•˜ì§€ ì•ŠëŠ” í¸ì´ ì¢‹ë‹¤.

---

### 6.5.2 Simulated/Fake Quantization

linear quantizationì€ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.

$$ r = S(q-Z) $$

ì´ë•Œ linear quantizationì€ integerë¥¼ real numberë¡œ ë³€í™˜í•˜ëŠ” affine mappingìœ¼ë¡œë„ ë³¼ ìˆ˜ ìˆì—ˆë‹¤.

![linear quantization](images/linear_quant.png)

- Weight quantization

$$ W \rightarrow S_{W}(q_{W} - Z_{W}) = Q(W) $$

- Activation quantization

$$ Y \rightarrow S_{Y}(q_{Y} - Z_{Y}) = Q(Y) $$

---

### 6.5.3 Straight-Through Estimator(STE)

> [Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation ë…¼ë¬¸(2013)](https://arxiv.org/abs/1308.3432)

> [UNDERSTANDING STRAIGHT-THROUGH ESTIMATOR IN TRAINING ACTIVATION QUANTIZED NEURAL NETS ë…¼ë¬¸(2019)](https://arxiv.org/abs/1903.05662)

ê·¸ë ‡ë‹¤ë©´ (simulated) quantizationì—ì„œ gradient backpropagationì„ ì–´ë–»ê²Œ ìˆ˜í–‰í•´ì•¼ í• ê¹Œ? ì˜ˆì‹œë¡œ ì•„ë˜ì˜ weight quantizationë¥¼ ë³´ë©´ ê±°ì˜ ëª¨ë“  êµ¬ê°„ì—ì„œ gradientê°€ 0ì¸ ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.

![discreted values](images/quant_ex.png)

$$ {{\partial Q(W)} \over {\partial W}} = 0 $$

- gradientê°€ 0ì´ë¯€ë¡œ backpropagationë„ ìˆ˜í–‰í•  ìˆ˜ ì—†ë‹¤.

$$ g_{W} = {{\partial L} \over {\partial W}} = {{\partial L} \over {\partial Q(W)}} \cdot {{\partial Q(W)} \over {\partial W}} = 0 $$

ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ **Straight-Through Estimator**(STE)ì„ ë„ì…í•œë‹¤. ìœ„ backpropagationì—ì„œ quantization(threshold function)ì˜ derivative(ë„í•¨ìˆ˜)ë¥¼ ë¬´ì‹œí•˜ê³ , ë§ˆì¹˜ identity functionì²˜ëŸ¼ ë™ì‘í•˜ë„ë¡ í•œë‹¤. 

$$ g_{W} = {{\partial L} \over {\partial W}} = {{\partial L} \over {\partial Q(W)}} $$

---

### 6.5.2 INT8 Linear Quantization-Aware Training

> [Quantizing deep convolutional networks for efficient inference: A whitepaper ë…¼ë¬¸(2017)](https://arxiv.org/abs/1806.08342)

ì‘ì€ ëª¨ë¸ì—ì„œ INT8 linear QATì„ ìˆ˜í–‰í•œ ê²°ê³¼ë¥¼ ë³´ë©´, PTQë³´ë‹¤ ë” accuracyë¥¼ íšŒë³µí•œ ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.

![QAT mobilenet](images/QAT_mobilenet.png)

ê²Œë‹¤ê°€ quantizationìœ¼ë¡œ ì¸í•´ accuracyëŠ” ê°ì†Œí–ˆì§€ë§Œ, latency ì¸¡ë©´ì—ì„œ ìš°ìˆ˜í•´ì§€ëŠ” trade-offë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.

![acc vs lat](images/acc_lat_trade_off.png)

---

## 6.6 Binary Quantization

**Binary Quantization**ì€ ë” ë‚˜ì•„ê°€ 1 bitë§Œì„ ì‚¬ìš©í•˜ëŠ” ì–‘ìí™” ë°©ì‹ì´ë‹¤. memory storage ì¸¡ë©´ì´ë‚˜ ë‹¨ìˆœí•œ computationì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì—ì„œ êµ‰ì¥íˆ í° ì´ì ì„ ê°€ì§„ë‹¤. 

| | FP32 | K-Means-based<br/>Quantization | Linear<br/>Quantization | Binary/Ternary<br/>Quantization |
| :---: | :---: |  :---: |  :---: |  :---: | 
| Storage | FP | INT weights<br/>FP Codebook | INT weights | Binary/Ternary<br/>weights |
| Computation | FP | FP | INT | Bit Operations |

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ 1: Binary Weight Quantization &nbsp;&nbsp;&nbsp;</span>

![binary quantization ex 1](images/binary_ex_1.png)

$$ y_i = \sum_{j}{W_{ij} \cdot x_{j}} $$

ìœ„ì™€ ê°™ì€ í–‰ë ¬ ì—°ì‚°ì´ ìˆë‹¤. ë‹¤ìŒ real number, binary quantization ê²½ìš°ì˜ memory, computationì„ ë¹„êµí•˜ë¼.

- real number

    ![binary quantization ex 2](images/binary_ex_2.png)

- binary quantization

    ![binary quantization ex 3](images/binary_ex_3.png)

### <span style='background-color: #C2B2B2; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ” í’€ì´&nbsp;&nbsp;&nbsp;</span>

- real number

$$ 8 \times 5 + (-3) \times 2 + 5 \times 0 + (-1) \times 1 $$

- binary quantization

$$ 5 - 2 + 0 - 1 $$

| input | weight | operations | memory | computation |
| :---: | :---: | :---: | :---: | :---: |
| $\mathbb{R}$ | $\mathbb{R}$ | + x | 1x | 1x |
| $\mathbb{R}$ | $\mathbb{B}$ | + - | ~32x less | ~2x less |

---

### 6.6.1 Binarization

> [BinaryConnect: Training Deep Neural Networks with binary weights during propagations ë…¼ë¬¸(2015)](https://arxiv.org/abs/1511.00363)

**Binarization**ì€ í¬ê²Œ ë‘ ê°€ì§€ ë°©ë²•ìœ¼ë¡œ ë‚˜ë‰œë‹¤.

- **Deterministic Binarization**

    ì •í•´ë‘” thresholdì— ë”°ë¼ ê°™ê±°ë‚˜ ë†’ì€ ê°’ì€ 1, ë‚®ì€ ê°’ì€ -1ë¡œ quantizeí•œë‹¤.

```math
q = sign(r) = \begin{cases} +1, & r \ge 0 \\ -1, & r < 0 \end{cases}
```

- **Stochastic Binarization**

    global statistics í˜¹ì€ input data ê°’ì„ ë°”íƒ•ìœ¼ë¡œ -1, +1ì´ ë  probabilityë¥¼ ê²°ì •í•œë‹¤.

    - ì˜ˆë¥¼ ë“¤ì–´ BinaryConnection ë…¼ë¬¸ì—ì„œëŠ”, probabilityë¥¼ **hard sigmoid**ë¥¼ ì´ìš©í•´ì„œ ê²°ì •í•œë‹¤.( $\sigma (r)$ ) 

      ![hard sigmoid](images/hard_sigmoid.png)

```math
q = \begin{cases} +1, & with \, probability \, p = \sigma(r) \\ -1, & with \, probability \, 1 - p \end{cases}
```

$$ \sigma (r) = \min (\max ({{r+1} \over {2}}), 1) $$

---

### 6.6.2 Binarize the weights and activations

> [XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks ë…¼ë¬¸(2016)](https://arxiv.org/abs/1603.05279)

ê·¸ëŸ¬ë‚˜ binarizationì€ íŠ¹ì„±ìƒ errorê°€ í´ ìˆ˜ë°–ì— ì—†ë‹¤. ë‹¤ìŒ weight tensor(`fp32`)ë¥¼ binaryconnection ë…¼ë¬¸ ë°©ì‹ìœ¼ë¡œ binarizationí•œ ì˜ˆì‹œë¥¼ ë³´ì.

![binaryconnection ex](images/binaryconnection_ex.png)

$$ W \approx \alpha W^{\mathbb{B}} $$

- scaling factor $\alpha$

$$ \alpha = {1 \over n}||W||_1 $$

> ì—°ì‚°ì´ ëë‚œ feature mapì€ ë‹¤ì‹œ scaling factor $\alpha$ ë¥¼ ê³±í•´ì§„ ê°’ì„ ì‚¬ìš©í•œë‹¤.

- quantization error

```math
|| W - W^{\mathbb{B}} |{|}^{2}_{F} = 9.28
```

> ImageNet ëŒ€ìƒ AlexNetì—ì„œ Top1 accuracyëŠ” -21.2%p ê°ì†Œí•œë‹¤.

XNOR-Net ë…¼ë¬¸ì—ì„œëŠ” input feature mapì—ë„ ë§ˆì°¬ê°€ì§€ë¡œ binarizationì„ ë„ì…í•˜ì—¬ quantization errorë¥¼ ì¤„ì˜€ë‹¤.(ë”°ë¼ì„œ inputì„ ìœ„í•œ scaling factor $\beta$ ë„ ì‚¬ìš©í•œë‹¤.)

$$ \beta = {1 \over n}||X||_1 $$

![XNOR-Net ex](images/XNOR-Net_ex.png)

> ImageNet ëŒ€ìƒ AlexNetì—ì„œ Top1 accuracyê°€ ì˜¤íˆë ¤ 0.2%p ì¦ê°€í•œë‹¤.

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ 2: XNOR-Net &nbsp;&nbsp;&nbsp;</span>

weight, activationì— binary quantizationì„ ì ìš©í•œ í–‰ë ¬ ì—°ì‚°ì„ ìˆ˜í–‰í•˜ê³ , ê°€ëŠ¥í•œ ì—°ì‚°ì˜ ì§„ë¦¬í‘œë¥¼ ì‘ì„±í•˜ë¼.

![binary quantization ex 4](images/binary_ex_4.png)

### <span style='background-color: #C2B2B2; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ” í’€ì´&nbsp;&nbsp;&nbsp;</span>

$$ y_i = \sum_{j}{W_{ij} \cdot x_{j}} $$

$$ = 1 \times 1 + (-1) \times 1 + 1 \times (-1) + (-1) \times 1 = -2 $$

ê°€ëŠ¥í•œ ì—°ì‚°ì˜ ì§„ë¦¬í‘œë¥¼ ì‘ì„±í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

| W | X | Y=WX |
| :---: | :---: | :---: |
| 1 | 1 | 1 |
| 1 | -1 | -1 |
| -1 | -1 | 1 |
| -1 | 1 | -1 |

ìœ„ ì§„ë¦¬í‘œì˜ ê°’ì„ ì¹˜í™˜í•˜ë©´ XNOR ì—°ì‚°ê³¼ ì™„ì „íˆ ì¼ì¹˜í•œë‹¤.

| $b_w$ | $b_x$ | XNOR( $b_w, b_x$ ) |
| :---: | :---: | :---: |
| 1 | 1 | 1 |
| 1 | 0 | 0 |
| 0 | 0 | 1 |
| 0 | 1 | 0 |

ì•ì„œ ìˆ˜í–‰í•œ ê³„ì‚°ì„ xnorë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$ = 1 \, \mathrm{xnor} \, 0 + 0 \, \mathrm{xnor} \, 1 + 1  \, \mathrm{xnor} \, 0 + 0 \, \mathrm{xnor} \, 1 = 1 $$

> ì›ì†Œ ì¤‘ 1ì˜ ê°œìˆ˜ê°€ ì–¼ë§ˆë‚˜ ìˆëŠ”ì§€(**popcount** ì—°ì‚°)ë¡œ ëŒ€ì‹ í•  ìˆ˜ ìˆë‹¤.

ê·¸ëŸ°ë° ì´ëŸ¬í•œ ê²½ìš° ì •ë‹µì´ -2ì™€ 1ë¡œ ì¼ì¹˜í•˜ì§€ ì•Šê²Œ ë˜ë¯€ë¡œ ë³´ì •ì´ í•„ìš”í•˜ë‹¤.

$$ y_i = -n + 2 \cdot \sum_{j} W_{ij} \, \mathrm{xnor} \, x_j $$

$$ = -4 + 2 \times (1 \, \mathrm{xnor} \, 0 + 0 \, \mathrm{xnor} \, 1 + 1  \, \mathrm{xnor} \, 0 + 0 \, \mathrm{xnor} \, 1) $$

$$ -4 + 2 \times (1 + 0 + 0 + 0) = -2 $$

![XNOR-Net ex 2](images/XNOR-Net_ex_2.png)

| input | weight | operations | memory | computation |
| :---: | :---: | :---: | :---: | :---: |
| $\mathbb{R}$ | $\mathbb{R}$ | + x | 1x | 1x |
| $\mathbb{R}$ | $\mathbb{B}$ | + - | ~32x less | ~2x less |
| $\mathbb{B}$ | $\mathbb{B}$ | xnor, popcount | ~32x less | ~58x less |

---

### 6.6.3 Accuracy Degradation of Binarization

binarizationì— ë”°ë¥¸ ì •í™•ë„ í•˜ë½ì„ ì •ë¦¬í•œ ë„í‘œë¥¼ ì‚´í´ë³´ì.

![binary accuracy degradation](images/binary_network_acc.png)

- BWN: **Binary Weight Network** with scale for weight binarization

- BNN: **Binarized Neural Network** without scale factors

---
