# Lecture 06 - Quantization (Part II)

> [Lecture 06 - Quantization (Part II) | MIT 6.S965](https://youtu.be/3nqUFSSJYKQ)

> [EfficientML.ai Lecture 6 - Quantization (Part II) (MIT 6.6940, Fall 2023, Zoom recording)](https://youtu.be/n72ndSimkB8?si=xU98arzumiap6buV)

> [Neural Network Quantization Technique - Post Training Quantization](https://medium.com/mbeddedwithai/neural-network-quantization-technique-post-training-quantization-ff747ed9aa95)

> [A Comprehensive Survey on Model Quantization for Deep Neural Networks ë…¼ë¬¸(2022)](https://arxiv.org/abs/2205.07877)

---

## 6.1 Quantization Granularity

quantization granularityìœ¼ë¡œëŠ” ëŒ€í‘œì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ì„¸ ê°€ì§€ ë°©ë²•ì´ ìˆë‹¤.

| Per-Tensor | Per-Channel | Group |
| :---: | :---: | :---: |
| ![Per-Tensor](images/granularity_per-tensor.png) | ![Per-Channel](images/granularity_per-channel.png) | ![Group](images/granularity_group.png) |  

ì˜¤ë¥¸ìª½ì— ìœ„ì¹˜í• ìˆ˜ë¡ ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì§•ì„ ê°–ëŠ”ë‹¤.

- (+) coarse-grained quantizationìœ¼ë¡œ, ë” ë†’ì€ ì •í™•ë„ë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆë‹¤.

- (-) í•˜ë“œì›¨ì–´ì—ì„œ ì§€ì›í•˜ì§€ ì•Šê±°ë‚˜ ìµœì í™”ê°€ ì–´ë µë‹¤.

---

## 6.2 Post-Training Quantization: Weight Quantization

> [Data-Free Quantization through Weight Equalization and Bias Correction ë…¼ë¬¸(2019)](https://arxiv.org/abs/1906.04721)

ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ì„ ì–‘ìí™”í•˜ëŠ”, **Post-Training Quantization**(PTQ)ë¥¼ ì‚´í´ë³¼ ê²ƒì´ë‹¤. ë‹¤ìŒ ê·¸ë¦¼ì€ MobileNetV2ì˜ ì²« ë²ˆì§¸ depthwise-separable ë ˆì´ì–´ê°€ ê°–ëŠ”, ì¶œë ¥ ì±„ë„ë³„ weight rangeì´ë‹¤.

![weight range per output channel](images/weight_range_per_output_channel.png)

ìœ„ì™€ ê°™ì€ ìƒí™©ì—ì„œ, ë‹¤ìŒê³¼ ê°™ì€ ë‘ PTQ ê¸°ë²•ì„ ê³ ë ¤í•´ ë³´ì.

- **Per-Tensor Quantization**

  tensor 32ê°œ ì „ì²´ì—, ë™ì¼í•œ scale $S$ ë¥¼ ì‚¬ìš©í•œë‹¤.

  - (-) outlier weightì— ì˜í–¥ì„ ë°›ê¸° ì‰½ë‹¤.

  - (-) í° ëª¨ë¸ì€ ì˜ ì ìš©ë˜ë‚˜, ì‘ì€ ëª¨ë¸ì—ì„œëŠ” ì •í™•ë„ í•˜ë½ì´ í¬ë‹¤.

     ì£¼ë¡œ output channelsì˜ weight range ì°¨ì´ê°€ í¬ë©´(100x ì´ìƒ) ì •í™•ë„ í•˜ë½ì´ í¬ë‹¤.

- **Per-Channel Quantization**

  ê° ì±„ë„ë³„ë¡œ scale $S$ ë¥¼ ì‚¬ìš©í•œë‹¤.

  - (-) ì§€ì›ë˜ì§€ ì•Šì€ í•˜ë“œì›¨ì–´ê°€ ìˆë‹¤.

  - (-) ë§¤ ì±„ë„ë§ˆë‹¤ ê°œë³„ì ì¸ scaling ì—°ì‚°ì„ ì ìš©í•˜ê¸° ë•Œë¬¸ì— overheadê°€ í¬ë‹¤.

---

### 6.2.1 Per-Tensor vs Per-Channel Quantization

í•œ í–‰ë ¬ì„ per-tensor quantization, per-channel quantizationì„ ì ìš©í•˜ì—¬ ì–´ë–¤ ì°¨ì´ê°€ ìˆëŠ”ì§€ ì•Œì•„ë³´ì.

| | Per-Tensor | Per-Channel |
| :---: | :---: | :---: |
| | ![Per-Tensor ex](https://github.com/erectbranch/TinyML_and_Efficient_DLC/blob/master/lec06/summary01/images/tensor_ex_1.png) | ![Per-Channel ex](https://github.com/erectbranch/TinyML_and_Efficient_DLC/blob/master/lec06/summary01/images/tensor_ex_2.png) |
| $\|r\|_{max}$ | 2.12 | 2.09 <br/> 2.12 <br/> 1.92 <br/> 1.87 |


- Per-Tensor Quantization

    ```math
    S = {{|r|}_{max} \over {q_{max}}} = {{2.12} \over {2^{2-1} - 1}} = 2.12
    ```

    | Quantized | Reconstructed |
    | :---: | :---: |
    | ![per-tensor quantized](images/per-tensor_ex_1.png) | ![per-tensor reconstructed](images/per-tensor_ex_2.png) |

    errorëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

    ```math
    ||W - Sq_{W}||_F = 2.28
    ```

- Per-Channel Quantization

    ```math
    S_0 = {{|r|}_{max} \over {q_{max}}} = {{2.09} \over {2^{2-1} - 1}} = 2.09
    ```

    ```math
    S_1 = {{|r|}_{max} \over {q_{max}}} = {{2.12} \over {2^{2-1} - 1}} = 2.12
    ```

    ```math
    S_2 = {{|r|}_{max} \over {q_{max}}} = {{1.92} \over {2^{2-1} - 1}} = 1.92
    ```

    ```math
    S_3 = {{|r|}_{max} \over {q_{max}}} = {{1.87} \over {2^{2-1} - 1}} = 1.87
    ```

    | Quantized | Reconstructed |
    | :---: | :---: |
    | ![per-channel quantized](images/per-channel_ex_1.png) | ![per-channel reconstructed](images/per-channel_ex_2.png) |

    errorëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

    ```math
    ||W - S \odot q_{W}||_F = 2.08
    ```

---

### 6.2.2 Weight Equalization

> [Data-Free Quantization through Weight Equalization and Bias Correction ë…¼ë¬¸(2019)](https://arxiv.org/abs/1906.04721)

ë°˜ë©´ weight rangeë¥¼ ì±„ë„ë³„ë¡œ ë¹„ìŠ·í•˜ê²Œ ì¡°ì ˆí•˜ì—¬, Per-Tensor weight quantizationì„ ì ìš©í•˜ëŠ” ì ‘ê·¼ë„ ê°€ëŠ¥í•˜ë‹¤. (**Weight Equalization**) 

ë…¼ë¬¸ì—ì„œëŠ” ì–‘ìí™”ì—ì„œ ì‚¬ìš©í•˜ëŠ” scaling factorë¥¼ ë³€í˜•í•´ì„œ, weight rangeë¥¼ í•¨ê»˜ ì¡°ì ˆí•  ìˆ˜ ìˆë‹¤ê³  ì£¼ì¥í•œë‹¤.(scaling equivariance)

1. ë‘ ê°œ ë ˆì´ì–´ ì¸µì„ í†µê³¼í•˜ëŠ” ì—°ì‚°ì„ ìˆ˜ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

    - $f$ : activation function
  
    $$y = f(W^{(2)} f(W^{(1)}x + b^{(1)})+b^{(2)})$$

2. (ì–‘ìí™”) scaling factorë¡œ êµ¬ì„±ëœ diagonal matrix $S$ ë¥¼ ì‹ì— í¬í•¨í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ë³€í˜•ëœë‹¤.

    $$= f(W^{(2)} S \hat{f}(S^{-1}W^{(1)}x + S^{-1}b^{(1)})+b^{(2)})$$

    ì´ë•Œ $S$ ì™€ $S^{-1}$ ë¥¼ ë‹¤ë¥¸ ë‹¤ë¥¸ í–‰ë ¬ê³¼ ë¬¶ì–´ì„œ ì¹˜í™˜í•  ìˆ˜ ìˆë‹¤.

    $a. \quad {\widehat{W}}^{(2)} = W^{(2)}S$

    $b. \quad {\widehat{W}}^{(1)} = S^{(-1)}W^{(1)}$

    $c. \quad {\widehat{b}}^{(1)} = S^{(-1)}b^{(1)}S$

$$ \quad = f({\widehat{W}}^{(2)} \hat{f}({\widehat{W}}^{(1)} x + {\widehat{b}}^{(1)})+b^{(2)}) $$

---

#### 6.2.2.1 Equalization ranges over multiple layers

ì´ì œ ê° channelë³„ weight rangeë¥¼ ë°”ê¿”ì¤„ scaling matrix $S$ ë¥¼ ì°¾ì•„ë³´ì. ë…¼ë¬¸ì—ì„œëŠ” ê° channel $i$ ë³„ ìµœì ì˜ ë²”ìœ„ë¥¼ ì•Œê¸° ìœ„í•´, precision $\hat{p_i}$ ë¥¼ ë‘”ë‹¤.

- ${\hat{p_i}}^{(1)}$ : $\quad {\widehat{W}}^{(1)}$ ì˜ channel $i$ ê°€ ê°–ëŠ” quantization range

- ${\hat{R}}^{(1)}$ : $\quad {\widehat{W}}^{(1)}$ ì˜ total range

$$ {\hat{p_i}}^{(1)} = {{{\hat{r_i}}^{(1)}} \over {{\hat{R}}^{(1)}}} $$

ì´ì œ ìµœì ì˜ $S$ ë¥¼ ì°¾ëŠ” ë¬¸ì œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.

$$ \max_{S} \sum_{i} {\hat{p_i}}^{(1)} {\hat{p_i}}^{(2)} $$

ì—¬ê¸°ì„œ symmetric quantizationìœ¼ë¡œ ìƒê°í•˜ë©´, ë‹¤ìŒê³¼ ê°™ì´ precisionì„ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.

- ${\hat{r_i}}^{(1)} = 2 \cdot \max_{j} |{\widehat{W}_{ij}^{(1)}}|$

- ${\hat{R}}^{(1)} = 2 \cdot \max_{ij} |{\widehat{W}_{ij}^{(1)}}|$

ë…¼ë¬¸ì—ì„œëŠ” ìµœì ì˜ $S$ settingì„ ë‹¤ìŒê³¼ ê°™ì´ ë„ì¶œí•´ ë‚¸ë‹¤.

$$ s_i = {{1} \over {r_{i}^{(2)}}}\sqrt{r_{i}^{(1)}r_{i}^{(2)}} $$

---

### 6.2.3 Group Quantization: Per-Vector Quantization

> [VS-Quant: Per-Vector Scaled Quantization for Accurate Low-Precision Neural Network Inference ë…¼ë¬¸(2021)](https://arxiv.org/abs/2102.04503)

VS-Quant ë…¼ë¬¸ì€ vector ë‹¨ìœ„, tensor ë‹¨ìœ„ë¡œ ê°ê° scalingí•˜ëŠ”, **two-level scaling**ì„ í†µí•œ **group quantization**ë¥¼ ì†Œê°œí•œë‹¤.

![per-vector quantization](images/per-vector.png)

$$ r = S(q - Z) \rightarrow r = \gamma \cdot S_q(q-Z) $$

- tensor ë‹¨ìœ„: scaling factor $\gamma$ 

- vector ë‹¨ìœ„: scaling factor $S_q$ 

ì´ë•Œ scaling factorì˜ numertic typeì„ ì–´ë–¤ ì¢…ë¥˜ë¥¼ ì‚¬ìš©í•˜ëŠ”ê°€ì— ë”°ë¼ì„œë„ trade-offê°€ ë°œìƒí•œë‹¤.

- integer scaling factor: ì •í™•ë„ëŠ” ë‚®ì•„ì§€ë‚˜ ì—°ì‚°ì´ íš¨ìœ¨ì ì´ë‹¤. 
   
- floating-point scaling factor: ì •í™•ë„ëŠ” ë†’ì•„ì§€ì§€ë§Œ ì—°ì‚° ë¹„ìš©ì´ ì»¤ì§„ë‹¤.

ë”°ë¼ì„œ coarse granularity(tensor)ì—ì„œ ì—°ì‚° ë¹„ìš©ì´ í° floating scaling factorë¥¼ ì‚¬ìš©í•˜ê³ , fine granularity(vector)ì—ì„œ ì—°ì‚° ë¹„ìš©ì´ ì‘ì€ integer scaling factorë¥¼ ì‚¬ìš©í•˜ë©° ê· í˜•ì„ ë§ì¶œ ìˆ˜ ìˆë‹¤.

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ 1: Per-Vector Quantization: Memory Overhead, Effective Bitwidth &nbsp;&nbsp;&nbsp;</span>

ë‹¤ìŒê³¼ ê°™ì€ ì¡°ê±´ì—ì„œ, N=4, M=4, V=16ì¼ ë•Œì˜ (1) memory overheadì™€ (2) effective bitwidthë¥¼ êµ¬í•˜ë¼.

- weight(or activation): N-bit integer

- per-vector scaling factor: M-bit integer

  ì´ë•Œ Vê°œ element vectorê°€ M-bit scaling factorë¥¼ ê³µìœ í•œë‹¤.

### <span style='background-color: #C2B2B2; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ” í’€ì´&nbsp;&nbsp;&nbsp;</span>

(1) memory overheadëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.

$$ M/(VN) = 4/(16 \times 4) = 0.0625 $$

ë”°ë¼ì„œ 16ê°œ element vectorë§ˆë‹¤ 6.25%ì˜ memory overheadê°€ ë°œìƒí•œë‹¤.

(2) effective bitwidthëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.

$$ N + M / V = 4 + 4 / 16 = 4.25 \mathrm{bits} $$

---

### 6.2.4 Group Quantization: Multi-level Scaling Scheme

> [With Shared Microexponents, A Little Shifting Goes a Long Way ë…¼ë¬¸(2023)](https://arxiv.org/abs/2302.08007)

ì•ì„œ ë³¸ two-level quantizationì„, ë‹¤ìŒê³¼ ê°™ì´ multi-level quantization ìˆ˜ì‹ìœ¼ë¡œ ì¼ë°˜í™”í•  ìˆ˜ ìˆë‹¤.

$$ r = (q - z) \cdot s_{l_0} \cdot s_{l_1} \cdot \cdots $$

- Per-Channel Quantization

  í•œ channelì´ í•˜ë‚˜ì˜ scaling factorë¥¼ ê³µìœ í•œë‹¤.

  ![per-channel quantization](images/multi_level_scaling_2.png)

  $$ r = (q - z) \cdot s_{l_0} $$

  - $s_{l_0}$ : FP16

  - $q$ : INT4

  - Effective Bitwidth : 4

- Two-Level Quantization(VS-Quant)

  vectorì™€ channel ë‹¨ìœ„ë¡œ, ê°ê°ì˜ scaling factorë¥¼ ê°–ëŠ”ë‹¤.

  ![two-level quantization](images/multi_level_scaling_3.png)

  $$ r = (q - z) \cdot s_{l_0} \cdot s_{l_1} $$

  - $s_{l_0}$ : UINT4

    4ê°œ vector(16ê°œ elements)ê°€ í•˜ë‚˜ì˜ UINT4 scaling factor $s_{l_0}$ ë¥¼ ê³µìœ í•œë‹¤.

  - $s_{l_1}$ : FP16

    í•˜ë‚˜ì˜ channelì´ FP16 scaling factor $s_{l_1}$ ë¥¼ ê³µìœ í•œë‹¤.

  - Effective Bitwidth : 4.25

- formats based on shared microexponents(MX)

  shared microexponents ë‹¨ìœ„ë¡œ scaling factorë¥¼ ê³µìœ í•˜ë„ë¡ í•˜ëŠ” ë°©ì‹ì´ë‹¤.

  ![shared microexponents](images/multi_level_scaling_4.png)

  > S1M2: 1 Sign bit, 2 Mantissas, E1M0: Exponent 1, Mantissa 0, E8M0: Exponent 8, Mantissa 0

  | Approach | Data Type | L0 group size | data type | L1 group size | l1 scale<br/>data type | Effective Bitwidth |
  | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
  | MX4 | S1M2 | 2 | E1M0 | 16 | E8M0 | 3+1/2+8/16=4 |
  | MX6 | S1M4 | 2 | E1M0 | 16 | E8M0 | 5+1/2+8/16=6 |
  | MX9 | S1M7 | 2 | E1M0 | 16 | E8M0 | 8+1/2+8/16=9 |

---

### 6.2.5 Adaptive Rounding

> [Up or Down? Adaptive Rounding for Post-Training Quantization ë…¼ë¬¸](https://arxiv.org/abs/2004.10568)

ì–‘ìí™”ì—ì„œ ì •í™•ë„ë¥¼ ìƒëŠ” ê°€ì¥ í° ì›ì¸ ì¤‘ í•˜ë‚˜ê°€ ë°”ë¡œ **rounding**(ë°˜ì˜¬ë¦¼)ì´ë‹¤. ë”°ë¼ì„œ roundingìœ¼ë¡œ ìƒëŠ” ì„±ëŠ¥ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ **Adaptive Rounding**ì´ë¼ëŠ” ë°©ë²•ì´ ì œì•ˆë˜ì—ˆë‹¤.

![AdaRound](images/AdaRound.png)

AdaRound ë…¼ë¬¸ì€ í•™ìŠµ ê°€ëŠ¥í•œ parameterë¥¼ ë‘ì–´, weightì— ë‚´ë¦¼( $\lfloor W \rfloor$ )ê³¼ ì˜¬ë¦¼( $\lceil W \rceil$ )ì„ ì ìš©í• ì§€ ê²°ì •í•œë‹¤.

- quantized value $\tilde{W}$ 

- weightì— $\triangle w = \delta$ (perturbation)ì„ ì¶”ê°€í•˜ì—¬, ì–´ëŠ ë°©í–¥ì˜ roundingì´ ë” ì¢‹ì€ì§€ íŒë‹¨í•œë‹¤.

$$ \tilde{W} = \lfloor | W | + \delta \rceil , \, \delta \in [0,1] $$

í›ˆë ¨ì€ ë‹¤ìŒê³¼ ê°™ì€ í•¨ìˆ˜ë¥¼ ìµœì í™”í•˜ëŠ” ê³¼ì •ì„ ê±°ì¹œë‹¤.

$$ \mathbb{E} [\mathcal{L}(x,y,w + \triangle w) - \mathcal{L}(x,y,w)] $$

Taylor seriesë¡œ ê·¼ì‚¬ ì‹œ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.

```math
\mathrm{argmin}_{V} {|| Wx - \tilde{W}x ||}^{2}_{F} + \lambda f_{reg}(V)
```

```math
\mathrm{argmin}_{V} {|| Wx - \lfloor \lfloor {W} \rfloor + h(V)\rceil x ||}^{2}_{F} + \lambda f_{reg}(V)
```

- $x$ : ì…ë ¥, 

- $V$ : ì…ë ¥ê³¼ ë™ì¼í•œ í˜•íƒœì˜ random variable

- $h()$ : (0, 1) ì‚¬ì´ ê°’ìœ¼ë¡œ mappingí•˜ëŠ” í•¨ìˆ˜ 

  > ì˜ˆë¥¼ ë“¤ë©´ rectified sigmoidê°€ ë  ìˆ˜ ìˆë‹¤.

- $f_{reg}(V)$ :  regularization

  $h(V)$ ê°€ binary ê°’ì´ ë  ìˆ˜ ìˆë„ë¡ encourageí•œë‹¤.

---

## 6.3 Post-Training Quantization: Activation Quantization

ì´ë²ˆì—ëŠ” **Activation Quantization**ì— ëŒ€í•´ ì•Œì•„ë³´ì.

- "weight" vs "activation"

    - weight: staticí•˜ë¯€ë¡œ ë²”ìœ„ë¥¼ ì •í•˜ê¸° ì‰½ë‹¤.
    
    - activation: ì…ë ¥(image)ê°€ ë‹¬ë¼ì§€ë©´ activation ê°’ë„ ì²œì°¨ë§Œë³„ë¡œ ë‹¬ë¼ì§„ë‹¤.(**dynamic range**)

activation quantizationì„ ìœ„í•´ì„œëŠ”, **dynamic range**ì—ì„œ ìµœì ì˜ **clipping range**ë¥¼ íƒìƒ‰í•  í•„ìš”ê°€ ìˆë‹¤.

![dynamic range](images/dynamic_range_activation.png)

---

### 6.3.1 During training

ë¨¼ì € ëª¨ë¸ì˜ í›ˆë ¨ ì¤‘ statisticsì„ ëª¨ì•„ë‘ëŠ” ë°©ì‹ìœ¼ë¡œ, clipping rangeë¥¼ ê²°ì •í•  ìˆ˜ ìˆë‹¤.

- í›ˆë ¨ ì¤‘ activationì„ ê´€ì°°í•˜ê³  $[a;b]$ rangeë¥¼ ê¸°ë¡í•œë‹¤.

- smoothing parameterê°€ 1ì— ê°€ê¹Œìš´ **Exponential Moving Averages**(EMA)ë¥¼ í†µí•´ clipping rangeë¥¼ ì§‘ê³„í•œë‹¤.

```math
{\hat{r}}^{(t)}_{max, min} = \alpha \cdot {r}^{(t)}_{max, min} + (1-\alpha) \cdot {\hat{r}}^{(t-1)}_{max, min}
```

ë‹¨, activation rangeê°€ ê¸‰ê²©í•˜ê²Œ ë³€í•˜ëŠ” í›ˆë ¨ ì´ˆê¸°(5ë§Œ ~ 200ë§Œ step)ì—ëŠ”, computation overheadë¥¼ ê³ ë ¤í•˜ì—¬ EMAë¥¼ ì ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.

---

### 6.3.2 Calibation

> [Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation ë…¼ë¬¸(2020)](https://arxiv.org/abs/2004.09602)

í›ˆë ¨ ë°ì´í„°ì…‹ì„ ìƒ˜í”Œë§í•˜ì—¬ calibation batchë¥¼ ë§Œë“  ë’¤, ì´ë¥¼ ì¶”ë¡ í•˜ë©° dynamic rangeë¥¼ ê¸°ë¡í•œë‹¤. ëŒ€í‘œì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ë°©ë²•ì´ ìˆë‹¤.

- **min-max** 

  ê°€ì¥ ë‹¨ìˆœí•œ ë°©ë²•ì— í•´ë‹¹ëœë‹¤.
  
  - activation rangeì˜ min/maxë¥¼ ê¸°ë¡í•œ ë’¤, ìµœì¢…ì ìœ¼ë¡œ í‰ê· ê°’ì„ ì‚¬ìš©í•œë‹¤. 
  
  - (-) outlierì— ì·¨ì•½í•˜ë‹¤.

- **percentile-based**

  min/max ëŒ€ì‹ , ië²ˆì§¸ largest/smallest ê°’ì„ rangeë¡œ ì‚¬ìš©í•œë‹¤.

  - ì˜ˆë¥¼ ë“¤ì–´ 99% calibrationì˜ ê²½ìš°, ê°€ì¥ í° 1% ê°’ì€ ëª¨ë‘ clipí•œë‹¤.

  - (+) min-maxë³´ë‹¤ outlierì— ëœ ë¯¼ê°í•˜ë‹¤.

- **Kullback-Leibler divergence** (KL-divergence)

  entropyë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ì–‘ìí™” ì´ì „ê³¼ ì´í›„ì˜ ë¶„í¬ ì°¨ì´ë¥¼ ìµœì†Œí™”í•œë‹¤.

- **Mean Squared Error** (MSE)

  ìœ„ ì„¸ ê°€ì§€ ë°©ë²•(activation histogram)ê³¼ ë‹¤ë¥´ê²Œ, ì–‘ìí™” ì „/í›„ì˜ ì…ë ¥ì„ ë¹„êµ í›„ ì°¨ì´ë¥¼ ìµœì†Œí™”í•œë‹¤.

ë‹¤ìŒì€ ResNet-50ì˜ ì„¸ ë²ˆì§¸ ë ˆì´ì–´ì˜ input activationì˜ histogramìœ¼ë¡œ, 3ê°€ì§€ ë°©ë²•ì˜ calibration rangeë¥¼ ë¹„êµí•œ ê·¸ë¦¼ì´ë‹¤.

![ResNet-50 histogram](images/histogram.png)

---

### 6.3.3 Calibration: Minimize Loss of Information

> [NVIDIA: 8-bit Inference with TensorRT](https://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf): í˜„ëŒ€ GPUì—ì„œ ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” ë°©ë²•ì´ë‹¤.

min-maxì˜ ë‹¨ì ì„ ë³´ì™„í•˜ê¸° ìœ„í•´ì„œëŠ”, ìµœì ì˜ **threshold**ë¥¼ ì°¾ì•„ì„œ clippingí•  í•„ìš”ê°€ ìˆë‹¤.

| No saturate | Saturate |
| :---: | :---: |
| ![no saturate](https://github.com/erectbranch/TinyML_and_Efficient_DLC/blob/master/lec06/summary01/images/no_saturation.png) | ![saturate](https://github.com/erectbranch/TinyML_and_Efficient_DLC/blob/master/lec06/summary01/images/saturate.png) |
| FP32 \|max\| $\rightarrow$ INT8 127 | FP32 \|threshold\| $\rightarrow$ INT8 127 |

ìµœì ì˜ clipping rangeë¥¼ ì°¾ê¸° ìœ„í•´ì„œ, FP32 ì…ë ¥ ë° INT ì…ë ¥ì— ë”°ë¥¸ activation ë¶„í¬(entropy) ì°¨ì´ë¥¼ **KL divergence**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì†Œí™”í•œë‹¤.

1. ì—¬ëŸ¬ calibration batchë¥¼ FP32 modelì—ì„œ ì¶”ë¡ í•˜ì—¬, activation histogramsë¥¼ ì–»ëŠ”ë‹¤.

2. ë‹¤ì–‘í•œ saturation thresholdsë¥¼ ì‚¬ìš©í•˜ì—¬, **quantized distributions**ë¥¼ ìƒì„±í•œë‹¤.

3. KL divergenceë¥¼ ìµœì†Œí™”í•˜ëŠ”, ìµœì ì˜ thresholdë¥¼ íƒìƒ‰í•œë‹¤.

```math
D_{KL}(P||Q) = {\sum}_{i}^{N}P(x_{i})\log{{P(x_{i})} \over {Q(x_{i})}}
```

ë‹¤ìŒì€ ResNetì˜ íŠ¹ì • ë ˆì´ì–´ì—ì„œ, saturation ì „/í›„ì˜ histogramì„ ë¹„êµí•œ ê·¸ë¦¼ì´ë‹¤.

| No saturate | Saturate |
| :---: | :---: |
| ![no saturate ResNet activation](images/activation_clipping_ex_1.png) | ![saturate ResNet activation](images/activation_clipping_ex_2.png) |

---

### 6.3.4 Calibration: Minimize MSE

> [Lecture 05 - Quantization (Part I) ì •ë¦¬](https://github.com/erectbranch/TinyML_and_Efficient_DLC/tree/master/lec05#552-sources-of-quantization-error): rounding error, clipping error ì°¸ê³ 

> [Optimal Clipping and Magnitude-aware Differentiation for Improved Quantization-aware Training ë…¼ë¬¸(2022)](https://arxiv.org/abs/2206.06501): NVIDIAì—ì„œ ë°œí‘œí•œ, MSEë¥¼ ìµœì†Œí™”í•˜ëŠ” QAT ë°©ë²•

ì–‘ìí™” ì´ì „ ì…ë ¥ê³¼, ì–‘ìí™” ì´í›„ì˜ ì…ë ¥ì— ì£¼ëª©í•˜ì—¬, ë‘ ì…ë ¥ì˜ ì°¨ì´(**mean-square-error**)ë¥¼ ìµœì†Œí™”í•˜ëŠ” ì ‘ê·¼ë„ ê°€ëŠ¥í•˜ë‹¤.

- $X$ : input

- $Q(X)$ : quantized imput

$$ \underset{{|r|}_{max}}{\min} \mathbb{E}[{(X - Q(X))}^{2}] $$

ì…ë ¥ì„ Laplace(í˜¹ì€ Gaussian) distributionìœ¼ë¡œ ê°€ì •í•˜ë©´, Laplace $(0, b)$ distributionì—ì„œ ìµœì ì˜ clipping valuesëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

- $b$ : calibration input distributionì„ ë°”íƒ•ìœ¼ë¡œ ì¶”ì •í•  ìˆ˜ ìˆë‹¤.

- 2, 3, 4 bits quantization: ê°ê°ì˜ ìµœì  clipping values

$$ |r|_{max} = 2.83b, 3.89b, 5.03b $$

---

## 6.4 Post-Training Quantization: Bias Quantization

> [Data-Free Quantization through Weight Equalization and Bias Correction ë…¼ë¬¸(2019)](https://arxiv.org/abs/1906.04721)

> calibration dataê°€ ì—†ê³  ëª¨ë¸ì´ **Batch Normalization**ì„ ì“°ëŠ” ê²½ìš°, ìœ ìš©í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

weight quantization errorëŠ”, ì‡ë”°ë¼ output activationì˜ ë¶„í¬ë¥¼ shiftingì‹œí‚¤ëŠ” ë¬¸ì œë¥¼ ë‚³ì„ ìˆ˜ ìˆë‹¤.(**biased error**)

- weight quantization error

$$ \epsilon = Q(W) - W $$

- biased error

$$ \mathbb{E}[\tilde{y_j} - y_j] \approx {{1} \over {N}} \sum_{n}{(Q(W)\mathrm{x_n})_j - (W\mathrm{x_n})_j} $$

biased errorëŠ” FP32 modelê³¼ quantized modelì˜ ê²°ê³¼ë¥¼ ë¹„êµí•˜ëŠ” ê²ƒìœ¼ë¡œ ì•Œ ìˆ˜ ìˆë‹¤.

$$ \mathbb{E}[y] = \mathbb{E}[W\mathrm{x}] + \mathbb{E}[\epsilon\mathrm{x}] - \mathbb{E}[\epsilon\mathrm{x}] $$

$$ \quad = \mathbb{E}[(\tilde{y})\mathrm{x}] - \mathbb{E}[\epsilon\mathrm{x}] $$

---

### 6.4.1 Bias Correction

**bias correction** ì ˆì°¨ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì§„í–‰ëœë‹¤.

1. $\mathbb{E}[y]$ ë¥¼ ê³„ì‚°í•œë‹¤.

    FP32 modelì„ Nê°œ exampleì— ëŒ€í•´ ì¶”ë¡ í•˜ê³ , ë ˆì´ì–´ë³„ per-channel pre-activation mean( $\mathbb{E}[y]$ )ì„ íšë“í•œë‹¤.

2. $\mathbb{E}[\tilde{y}]$ ë¥¼ ê³„ì‚°í•œë‹¤.

    quantized modelì˜ ë ˆì´ì–´ë§ˆë‹¤ $\mathbb{E}[\tilde{y}]$ ë¥¼ íšë“í•œë‹¤.

3. per-channel biased quantization errorë¥¼ ê³„ì‚°í•œë‹¤.

$$\mathbb{E}[\epsilon] = \mathbb{E}[\tilde{y}] - \mathbb{E}[y]$$

4. ë ˆì´ì–´ë³„ bias correctionì„ ìˆ˜í–‰í•œë‹¤.

   ë ˆì´ì–´ë§ˆë‹¤ $\mathbb{E}[\epsilon]$ ë¥¼ ë¹¼ì£¼ëŠ” ê²ƒìœ¼ë¡œ bias correctionì„ ìˆ˜í–‰í•œë‹¤.

ì•„ë˜ ê·¸ë¦¼ì€ MobileNetV2 ëª¨ë¸ì˜ ë‘ ë²ˆì§¸ depthwise-separable convolution layerì—ì„œ, bias correction ì „/í›„ biased output errorì˜ ë¶„í¬ë¥¼ ë‚˜íƒ€ë‚¸ ê·¸ë¦¼ì´ë‹¤.

| Before Correction | After Correction |
| :---: | :---: |
| ![before bias correction](images/biased_output.png) | ![after bias correction](images/after_bias_correction.png) |

---

## 6.5 Post-Training INT8 Linear Quantization

í•˜ì§€ë§Œ large modelê³¼ ë¹„êµí•´ì„œ, ëª¨ë¸ì´ ì‘ì„ìˆ˜ë¡ PTQê°€ ê·¸ë‹¤ì§€ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ì§€ ì•ŠëŠ”ë‹¤.

![PTQ int8 models](images/PTQ_models.png)

---

## 6.6 Post-Training Quantization: Data Free Quantization

> [ZeroQ: A Novel Zero Shot Quantization Framework ë…¼ë¬¸(2020)](https://arxiv.org/abs/2001.00281)

ZeroQ ë…¼ë¬¸ì€ í›ˆë ¨ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³ , distilled dataë¥¼ ë§Œë“¤ì–´ ì–‘ìí™”í•˜ëŠ” **Zero-Shot Quantization**ì„ ì œì•ˆí–ˆë‹¤. 

ì´ì „ê¹Œì§€ëŠ” í›ˆë ¨ ë°ì´í„°ì…‹ì´ ì—†ì„ ê²½ìš°, ì£¼ë¡œ naive approachë¡œ í‰ê·  0ê³¼ ë‹¨ìœ„ ë¶„ì‚°ì„ ê°–ëŠ” Gaussian distribution $N(0, 1)$ ì„ ì‚¬ìš©í–ˆë‹¤. í•˜ì§€ë§Œ ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œëŠ” activation statisticsë¥¼ ì •í™•íˆ íŒŒì•…í•˜ê¸° ì–´ë µë‹¤.

í•˜ì§€ë§Œ ë” ë§ì€ local structureë¥¼ ê°€ì§€ëŠ” distilled dataë¥¼ ì´ìš©í•˜ë©´ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆë‹¤. ë‹¤ìŒì€ í•´ë‹¹ ë…¼ë¬¸ì—ì„œ Gaussian dataì™€ Distilled dataë¥¼ ì‹œê°í™”í•˜ì—¬ ë¹„êµí•œ ì˜ˆì‹œë‹¤.

| Gaussian data | Distilled data |
| :---: | :---: |
| ![Gaussian data](images/gaussian_data.png) | ![Distilled data](images/distilled_data.png)  |

> 8-V100 ì‹œìŠ¤í…œì—ì„œ ImageNet ëŒ€ìƒìœ¼ë¡œ í›ˆë ¨í•œ ResNet-50 ê¸°ì¤€ìœ¼ë¡œ 32ê°œ ë°ì´í„°ë¥¼ ë§Œë“œëŠ” ì‹œê°„ì€ 3ì´ˆë¡œ, computational overheadê°€ ì ë‹¤.

---

### 6.6.1 Generation of Distilled Data

ZeroQì—ì„œëŠ” batch normalization ë ˆì´ì–´ì˜ statisticì„ ë°”íƒ•ìœ¼ë¡œ distilled dataë¥¼ ìƒì„±í•œë‹¤. ì´ë•Œ distilled dataë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´, ëª¨ë¸ì„ ì¶”ë¡ í•˜ë©° ìµœì í™”í•˜ëŠ” ìˆ˜ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

```math
\min_{x^r} \sum_{i=0}^{L} ||\tilde{\mu}_i^r - {\mu}_{i}||_{2}^{2} + || \tilde{\sigma}_{i}^{r} - \tilde{\sigma}_{i} ||_{2}^{2}
```

- $x^{r}$ : reconstructed (ditilled) input data

- $i$ : layer(0~L)

- ${\mu}_i, {\sigma}_i$ : BN ë ˆì´ì–´ì— ì €ì¥ëœ í‰ê· , í‘œì¤€í¸ì°¨

---

### 6.6.2 Sensitivity Analysis for Mixed-Precision Quantization

ZeroQê°€ í•´ê²°í•˜ë ¤ëŠ” mixed-precision ë¬¸ì œëŠ” ë ˆì´ì–´ë³„ ìµœì ì˜ bit-widthë¥¼ ê³ ë¥´ëŠ” ë¬¸ì œì˜ ê²½ìš°ì˜ ìˆ˜(search space)ê°€ ë§¤ìš° ë§ì•„ì„œ ì–´ë µë‹¤. í•˜ì§€ë§Œ KL divergenceë¥¼ ì‚¬ìš©í•˜ì—¬, ë ˆì´ì–´ ë‹¨ìœ„ì˜ quantization sensitivityë¥¼ êµ¬í•˜ì—¬ ë¬¸ì œë¥¼ ë‹¨ìˆœí™” í•œë‹¤.

ë‹¤ìŒì€ ResNet-50ì—ì„œ 2,4,8 bitë¡œ weight quantizationì„ ì ìš©í–ˆì„ ë•Œ, ë ˆì´ì–´(block)ë³„ sensitivityë¥¼ ë‚˜íƒ€ë‚¸ ê·¸ë˜í”„ë‹¤.

- ë¯¼ê°í•œ ë ˆì´ì–´ëŠ” í° bit precisionë¥¼ ì‚¬ìš©í•œë‹¤.

- ëœ ë¯¼ê°í•œ ë ˆì´ì–´ëŠ” ì‘ì€ bit precisionì„ ì‚¬ìš©í•œë‹¤.

![quantization sensitivity](images/layer_bit_width_sensitivity.png)

---