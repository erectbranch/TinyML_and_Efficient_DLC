# Lecture 06 - Quantization (Part II)

> [Lecture 06 - Quantization (Part II) | MIT 6.S965](https://youtu.be/3nqUFSSJYKQ)

> [Neural Network Quantization Technique - Post Training Quantization](https://medium.com/mbeddedwithai/neural-network-quantization-technique-post-training-quantization-ff747ed9aa95)

ì ì‹œ ë‘ ê°€ì§€ ì–‘ìí™” ë°©ë²•ì„ ë³µìŠµí•˜ì.

![K-means, Linear](images/K-means_and_Linear_quantization.png)

| | ì–‘ìí™” ì „ | K-means-based | Linear |
| :---: | :---: | :---: | :---: |
| Storage | FP ê°€ì¤‘ì¹˜ | INT ê°€ì¤‘ì¹˜</br>FP ì½”ë“œë¶ | INT ê°€ì¤‘ì¹˜ |
| Computation | FP ì—°ì‚° | FP ì—°ì‚° | INT ì—°ì‚° |

ì´ë²ˆ ì •ë¦¬ëŠ” í¬ê²Œ ë„¤ ê°€ì§€ ì–‘ìí™” ë°©ë²•ì„ ë‹¤ë£¬ë‹¤.

- **Post-Training Quantization**(PTQ)

    í•™ìŠµëœ floating-point ëª¨ë¸ì„ ì–‘ìí™”

    - <U>weight</U> quantization, <U>activation</U> quantization, <U>bias</U> quantization í¬í•¨

- **Quantization-Aware Training**(QAT)

    floating-point ëª¨ë¸ ë‚´ fake quantization ëª¨ë“ˆì„ ë°°ì¹˜í•œ ë’¤, ì¶”ë¡ ì„ ì§„í–‰í•˜ë©° ì–‘ìí™”ì— ì˜í•œ ë³€í™”ë¥¼ ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” ë°©ë²•.

    training/fine-tuning ê³¼ì •ì—ì„œ QATë¥¼ ì ìš©í•  ìˆ˜ ìˆìœ¼ë©°, QATê°€ ëë‚˜ë©´ fake quantization ëª¨ë“ˆì— ê¸°ë¡í•œ ì •ë³´ë¥¼ ê°€ì§€ê³  ëª¨ë¸ì„ íš¨ê³¼ì ìœ¼ë¡œ ì–‘ìí™”í•  ìˆ˜ ìˆë‹¤.

- **binary and ternary** quantization

- **mixed-precision** quantization

---

## 6.1 Post-Training Quantization: Weight Quantization

ìš°ì„  weight quantizationì—ì„œ ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ëª‡ ê°€ì§€ ë°©ë²•ì„ ì‚´í´ë³´ì. 

---

### 6.1.1 Per-Channel Quantization

CNNì—ì„œ output channelë³„ weightê°€ ê°–ëŠ” ê°’ì˜ ë²”ìœ„ë¥¼ ë‚˜íƒ€ë‚¸ ì•„ë˜ ê·¸ë¦¼ì„ ì‚´í´ë³´ì.(MobileNetV2 ì²« ë²ˆì§¸ depthwise-seperable layer)

![weight range per output channel](images/weight_range_per_output_channel.png)

- í•„í„°ëŠ” 32ê°œ, ë”°ë¼ì„œ ì¶œë ¥ í…ì„œì˜ ì±„ë„ë„ 32ê°œ

    > í•œ ì±„ë„ = ì…ë ¥ê³¼ í•„í„° í•˜ë‚˜ë¥¼ ì—°ì‚°í•œ ê²°ê³¼ê°’ì— í•´ë‹¹

ì´ì²˜ëŸ¼ ì±„ë„ë³„ weight ê°’ì˜ ë²”ìœ„ê°€ ë‹¤ë¥´ë¯€ë¡œ, ëª¨ë“  ì±„ë„ì— ë™ì¼í•œ scaling factorë¥¼ ì ìš©í•˜ë©´ ë¬¸ì œê°€ ìƒê¸¸ ìˆ˜ ìˆë‹¤.(**Per-Tensor Quantization**) ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œ **Per-Channel Quantization**ê°€ ë“±ì¥í–ˆë‹¤.

ë³´í†µ Per-Tensor Quantizationë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ì§€ë§Œ ë‹¤ìŒê³¼ ê°™ì€ ë‹¨ì ì„ ê°–ëŠ”ë‹¤. 

- Per-Channel Quantizationë¥¼ ì§€ì›í•˜ì§€ ì•ŠëŠ” hardwareê°€ ìˆë‹¤.

- ê° ì±„ë„ë³„ scaling factorë¥¼ êµ¬í•´ì•¼ í•˜ë¯€ë¡œ computation overheadê°€ í¬ë‹¤.

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ 1: Per-Channel Quantization &nbsp;&nbsp;&nbsp;</span>

ì•„ë˜ tensorë¥¼ 2-bit linear quantizationí•œ ê²°ê³¼ì™€ ì˜¤ì°¨ë¥¼, Per-Tensor Quantization, Per-Channel Quantization ë°©ì‹ë³„ë¡œ êµ¬í•˜ë¼.

![quantization example](images/quantization_ex.png)

### <span style='background-color: #C2B2B2; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ” í’€ì´&nbsp;&nbsp;&nbsp;</span>

**Per-Tensor Quantization**

- scaling factor

$$ {|r|}_{max} = 2.12 $$

```math
S = {{|r|}_{max} \over {q_{max}}} = {{2.12} \over {2^{2-1} - 1}} = 2.12
```

- quantized, reconstructed

    ![per-tensor quantization example](images/per-tensor_quant_ex.png)

    ì˜¤ì°¨ë¥¼ êµ¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$ {||W-Sq_{W}||}_{F} = 2.28 $$

**Per-Channel Quantization**: ì±„ë„ë³„ scaling factorëŠ” ë‹¤ìŒê³¼ ê°™ì´ êµ¬í•  ìˆ˜ ìˆë‹¤.

- 1í–‰ 

$${|r|}_{max} = 2.09$$

```math
S = {{|r|}_{max} \over {q_{max}}} = {{2.09} \over {2^{2-1} - 1}} = 2.09
```

- 2í–‰

$${|r|}_{max} = 2.12$$

```math
S = {{|r|}_{max} \over {q_{max}}} = {{2.12} \over {2^{2-1} - 1}} = 2.12
```

- 3í–‰

$${|r|}_{max} = 1.92$$

```math
S = {{|r|}_{max} \over {q_{max}}} = {{1.92} \over {2^{2-1} - 1}} = 1.92
```

- 4í–‰

$${|r|}_{max} = 1.87$$

```math
S = {{|r|}_{max} \over {q_{max}}} = {{1.87} \over {2^{2-1} - 1}} = 1.87
```

- quantized, reconstructed

    ![per-channel quantization example](images/per-channel_quant_ex_2.png)

    ì˜¤ì°¨ë¥¼ êµ¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$ {||W-S \odot q_{W}||}_{F} = 2.08 $$

---

### 6.1.2 Weight Equalization

> [Data-Free Quantization through Weight Equalization and Bias Correction ë…¼ë¬¸](https://arxiv.org/abs/1906.04721): re-scaling, re-parameterization

ê·¸ë ‡ë‹¤ë©´ weight rangeë¥¼ ì±„ë„ë³„ë¡œ ë¹„ìŠ·í•˜ê²Œ ë§Œë“¤ì–´ì„œ per-tensor weight quantizationì„ ì‚¬ìš©í•˜ë©´ ì•ˆ ë ê¹Œ? ì´ëŸ¬í•œ ì ‘ê·¼ë²•ì„ **Weight Equalization**ì´ë¼ê³  í•œë‹¤.

ë‘ ê°œì˜ ì—°ì†ëœ ë ˆì´ì–´ $i, i+1$ ê°€ ìˆë‹¤ê³  í•˜ì. í–‰ë ¬ ì—°ì‚°ì€ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.

![weight equalization](images/weight_equalization.png)

$$ y^{(i+1)} = f(W^{(i+1)}x^{(i+1)} + b^{(i+1)}) $$

$$ \quad \quad = f(W^{(i+1)} \cdot f(W^{(i)}x^{(i)} + b^{(i)}) + b^{(i+1)}) $$

- $f$ : activation function

ì´ë•Œ rescalingì„ ìˆ˜í–‰í•˜ëŠ” scaling factor í–‰ë ¬ $S$ ë¥¼ ì‹ì— í‘œê¸°í•´ ë³´ì.

$$ \quad \quad = f(W^{(i+1)}S \cdot f(S^{-1}W^{(i)}x^{(i)} + S^{-1}b^{(i)}) + b^{(i+1)}) $$

- $S$ : ëŒ€ê° í–‰ë ¬ $diag(s)$ 

-  element $s_j$ : output channel $j$ ì˜ scaling factor

ì—¬ê¸°ì„œ ì´ì „ ë ˆì´ì–´( $i$ )ì˜ scaling factorê°€ 1ë³´ë‹¤ ì‘ë‹¤ë©´ activation ê°’ì´ ë” ì»¤ì§„ë‹¤.(ì˜ˆ: ${{1} \over {0.8}} = 1.25$ ) ê·¸ë ‡ë‹¤ë©´ bias ê°’ë„ ìì—°íˆ ì¦ê°€í•˜ê³  rangeëŠ” ì ì  ì»¤ì§ˆ ê²ƒì´ë‹¤. 

ë”°ë¼ì„œ ì´ì „ ë ˆì´ì–´ëŠ” ìŠ¤ì¼€ì¼ì„ ì¤„ì´ê³ , ë’¤ìª½ ë ˆì´ì–´ëŠ” ìŠ¤ì¼€ì¼ì„ ëŠ˜ë ¤ì•¼ í•œë‹¤.

- ë ˆì´ì–´ $i$ : output channel $oc = a$ ë¥¼ scaling down

- ë ˆì´ì–´ $i+1$ : input channel $ic = a$ ë¥¼ scaling up

ê·¸ë ‡ë‹¤ë©´ ì´ëŸ¬í•œ $s_j$ ê°’ì€ ì–´ë–»ê²Œ ì •í•´ì•¼ í• ê¹Œ? ë…¼ë¬¸ì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì‚°ì¶œí•œë‹¤.

$$ s_j = {{1} \over {r_{ic=j}^{(i+1)}}} \sqrt{r_{oc=j}^{(i)} \cdot r_{ic=j}^{(i+1)}} $$

- $r_{oc=j}^{(i)}$ : ë ˆì´ì–´ $i$ output channel $j$ ê°€ ê°–ëŠ” weight range

- $r_{ic=j}^{(i+1)}$ : ë ˆì´ì–´ $i+1$ input channel $j$ ê°€ ê°–ëŠ” weight range

---

### 6.1.3 Adaptive Rounding

> [Up or Down? Adaptive Rounding for Post-Training Quantization ë…¼ë¬¸](https://arxiv.org/abs/2004.10568)

ì–‘ìí™”ì—ì„œ ì •í™•ë„ë¥¼ ìƒëŠ” ê°€ì¥ í° ì›ì¸ì€ **rounding**(ë°˜ì˜¬ë¦¼)ì´ë‹¤. ë”°ë¼ì„œ roundingìœ¼ë¡œ ìƒëŠ” ì„±ëŠ¥ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ **Adaptive Rounding**ì´ë¼ëŠ” ë°©ë²•ì´ ì œì•ˆë˜ì—ˆë‹¤.

- ì¼ë°˜ì ì¸ ë°˜ì˜¬ë¦¼( $\lfloor W \rceil$ )ì´ í•­ìƒ optimalí•˜ì§€ëŠ” ì•Šë‹¤ëŠ” ì ì—ì„œ ì°©ì•ˆí–ˆë‹¤.

![AdaRound](images/AdaRound.png)

ê·¸ë ‡ë‹¤ë©´ ì–´ë– í•œ ë°©ì‹ìœ¼ë¡œ weightì— ë‚´ë¦¼( $\lfloor W \rfloor$ )ê³¼ ì˜¬ë¦¼( $\lceil W \rceil$ )ì„ ì ìš©í• ê¹Œ? AdaRoundëŠ” í•™ìŠµ ê°€ëŠ¥í•œ parameterë¥¼ ë‘ì–´ ê²°ì •í•œë‹¤. 

- quantized value $\tilde{W}$ 

$$ \tilde{W} = \lfloor | W | + \delta \rceil , \, \delta \in [0,1] $$

ìœ„ ì–‘ìí™” ê°’ì„ ìµœì í™”í•˜ëŠ” ê³¼ì •ì„ ìˆ˜í–‰í•œë‹¤.

```math
\mathrm{argmin}_{V} {|| Wx - \tilde{W}x ||}^{2}_{F} + \lambda f_{reg}(V)
```

```math
\mathrm{argmin}_{V} {|| Wx - \lfloor \lfloor {W} \rfloor + h(V)\rceil x ||}^{2}_{F} + \lambda f_{reg}(V)
```

- $x$ : input

- $V$ : ë™ì¼í•œ shapeì˜ random variable

- $h$ : (0, 1) ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ mappingí•˜ëŠ” í•¨ìˆ˜(ì˜ˆë¥¼ ë“¤ë©´ rectified sigmoid)

- $f_{reg}(V)$ : $h(V)$ ê°€ binary ê°’ì´ ë  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” regularization í•¨ìˆ˜

---

## 6.2 Post-Training Quantization: Activation Quantization

ì´ë²ˆì—ëŠ” activation quantizationì— ëŒ€í•´ ì•Œì•„ë³´ì.

- "weight" vs "activation"

    - weight: staticí•˜ë¯€ë¡œ ë²”ìœ„ë¥¼ ì •í•˜ê¸° ì‰½ë‹¤.
    
    - activation: ì…ë ¥(image)ê°€ ë‹¬ë¼ì§€ë©´ activation ê°’ë„ ì²œì°¨ë§Œë³„ë¡œ ë‹¬ë¼ì§„ë‹¤.(**dynamic range**)

---

### 6.2.1 Dynamic Range for Activation Quantization

ë”°ë¼ì„œ activation quantizationì„ ìœ„í•´ì„œëŠ” ì ì ˆí•œ ë²”ìœ„ë¥¼ íƒìƒ‰í•  í•„ìš”ê°€ ìˆë‹¤.

- clipping thresholdì¸ $q_{min}$ , $q_{max}$ ê°’ì„ íŒŒì•…í•´ì•¼ í•œë‹¤.

- thresholdë¥¼ ì„¤ì •í•˜ë©´ scaling factorì™€ zero pointë¥¼ ê³„ì‚° ê°€ëŠ¥í•˜ë‹¤. ì´ë¥¼ statisticsí•œ ë°©ì‹ìœ¼ë¡œ ì°¾ëŠ”ë‹¤.

   ![dynamic range](images/dynamic_range_for_activation.png)

```math
{\hat{r}}^{(t)}_{max, min} = \alpha \cdot {r}^{(t)}_{max, min} + (1-\alpha) \cdot {\hat{r}}^{(t-1)}_{max, min}
```

ì´ì œ ê° activation quantizationì„ ìœ„í•œ íŒ¨ëŸ¬ë¯¸í„°ë¥¼ êµ¬í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ì.

---

### 6.2.2 During training

í•™ìŠµ ë„ì¤‘ íŒ¨ëŸ¬ë¯¸í„°ë¥¼ êµ¬í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ, training stepë§ˆë‹¤ **Exponential Moving Averages**(EMA, ì´ë™í‰ê· )ì„ ê°±ì‹ í•œë‹¤.(model state ê¸°ë¡)

---

### 6.2.3 calibation batches

ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ë©° ì‚¬ìš©í•œ train datasetì—ì„œ calibation batchë¥¼ ë§Œë“¤ì–´ ì¶”ë¡ ì‹œí‚¤ë©° íŒ¨ëŸ¬ë¯¸í„°ë¥¼ êµ¬í•œë‹¤. 

- **min-max**

    ê° batchë§ˆë‹¤ ê³„ì‚°í•œ activationì˜ min/maxë¥¼ êµ¬í•œ ë’¤ í‰ê· ê°’ì„ ì‚¬ìš©í•œë‹¤.

    - activation íŠ¹ì„±ìƒ outlierê°€ ë‚˜íƒ€ë‚˜ëŠ” ê²½ìš°ê°€ ë§ê³  ì˜í–¥ì„ ë°›ê¸° ì‰¬ìš°ë¯€ë¡œ ì£¼ì˜í•´ì•¼ í•œë‹¤.

- **Mean Squared Error**(MSE)

    fp32 ì…ë ¥ $X$ ì™€ ì–‘ìí™”ëœ ì…ë ¥ $Q(X)$ ì„ ì‚¬ìš©í–ˆì„ ë•Œì˜ ì¶œë ¥ê°’ ì°¨ì´ë¥¼ ë¹„êµí•˜ë©° mean-square-errorë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì§„í–‰ëœë‹¤.
    
    - min-max ë°©ì‹ë³´ë‹¤ outlierì— ëœ ë¯¼ê°í•˜ë‹¤.

$$ \underset{{|r|}_{max}}{\min} \mathbb{E}[{(X - Q(X))}^{2}] $$

- **KL divergence**

    fp32 ì…ë ¥ê³¼ ì–‘ìí™” ì…ë ¥ì— ë”°ë¥¸ ì¶œë ¥ì˜ ë¶„í¬ ì°¨ì´ë¥¼ ìµœì†Œí™”í•œë‹¤. ë¶„í¬ëŠ” **KL divergence**(Kullback-Leibler divergence)ë¥¼ ì‚¬ìš©í•´ ì¸¡ì •í•œë‹¤.

    - ì´ë•Œ íŠ¹ì • threshold ê°’ ì´ìƒ, ì´í•˜ì˜ ê°’ì€ saturationëœë‹¤.

    ![KL divergence](images/KL_divergence.png)

```math
D_{KL}(P||Q) = {\sum}_{i}^{N}P(x_{i})\log{{P(x_{i})} \over {Q(x_{i})}}
```

---

## 6.3 Post-Training Quantization: Bias Quantization

> [Data-Free Quantization through Weight Equalization and Bias Correction ë…¼ë¬¸](https://arxiv.org/abs/1906.04721). 6.1.2ì ˆê³¼ ì´ì–´ì§

> [Batch Normalization ì •ë¦¬](https://github.com/erectbranch/Neural_Networks_and_Deep_Learning/tree/master/ch03/summary02)

> calibration dataê°€ ì—†ê³  ëª¨ë¸ì´ **Batch Normalization**ì„ ì“°ëŠ” ê²½ìš° ì´ ë°©ë²•ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

weightì— ì˜í•œ quantization errorëŠ” ì‡ë‹¬ì•„ ì¶œë ¥ ë¶„í¬ë¥¼ shiftingì‹œì¼œì„œ, activationì´ ì˜ëª»ëœ ë¶„í¬ë¥¼ ê°–ë„ë¡ ë§Œë“¤ ìˆ˜ ìˆë‹¤. ì´ë¥¼ **biased error**ë¼ê³  í•œë‹¤.

ìš°ì„  biasë¥¼ ë”í•˜ê¸° ì „ ë‹¨ê³„ì¸ **pre-activation**ì„ ì‚´í´ë³´ì. FP32 ëª¨ë¸ì˜ pre-activation $y$ , quantization errorê°€ ì¶”ê°€ëœ $\tilde{y}$ ì€ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.

$$ \tilde{y} = \tilde{W}\mathrm{x} = y + \epsilon \mathrm{x} $$

- $W$ : weight tensor

- $\tilde{W}$ : quantized weight

- $\mathrm{x}$ : input activation

- $\epsilon = \tilde{W} - W$ : quantization error

ë”°ë¼ì„œ **biased error**ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.

$$ \mathbb{E}[\tilde{y_j} - y_j] \approx {{1} \over {N}} \sum_{n}{(Q(W)\mathrm{x_n})_j - (W\mathrm{x_n})_j} $$

ì•„ë˜ ê·¸ë¦¼ì€ MobileNetV2 ëª¨ë¸ì˜ depthwise-separable convolution layerì˜ ì±„ë„ë³„ bias quantization errorë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.

![quantization bias correction](images/quantization_biased_output_error.png)

errorëŠ” ë‹¤ìŒê³¼ ê°™ì´ correctioní•  ìˆ˜ ìˆë‹¤.

$$ \mathbb{E}[y] = \mathbb{E}[W\mathrm{x_n}] + \mathbb{E}[\epsilon\mathrm{x}] - \mathbb{E}[\epsilon\mathrm{x}] $$

$$ \quad = \mathbb{E}[Q(W)\mathrm{x}] - \epsilon \mathbb{E}[\mathrm{x}] $$

- input distributions $\mathbb{E}[x]$ ëŠ” Batch Normalizationì„ í†µí•´ ì–»ì–´ì§„ë‹¤.

ì¦‰, BatchNorm statisticsì— quantization errorë¥¼ ê³±í•˜ëŠ” ê²ƒìœ¼ë¡œ biasë¥¼ ë³´ì •í•  ìˆ˜ ìˆë‹¤.

---

## 6.4 Post-Training INT8 Linear Quantization

![PTQ int8 models](images/PTQ_models.png)

large modelê³¼ MobileNetV1, MobileNetV2ë¥¼ ë¹„êµí•˜ë©´, ì‘ì€ ëª¨ë¸ì¼ìˆ˜ë¡ PTQê°€ ê·¸ë‹¤ì§€ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ì§€ ì•ŠëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.

---