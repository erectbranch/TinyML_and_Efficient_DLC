# Lecture 05 - Quantization (Part I)

> [Lecture 05 - Quantization (Part I) | MIT 6.S965](https://youtu.be/91stHPsxwig)

> [EfficientML.ai Lecture 5 - Quantization (Part I) (MIT 6.5940, Fall 2023, Zoom recording)](https://youtu.be/MK4k64vY3xo?si=ouUP5R86zYN7XPsS)

---

## 5.5 How Many Bits to Quantize Weights?

> [Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding ë…¼ë¬¸(2015)](https://arxiv.org/abs/1510.00149)

ì–‘ìí™”ë¥¼ ìœ„í•œ bit widthëŠ” ì–´ëŠ ì •ë„ê°€ ì ë‹¹í• ê¹Œ? ë‹¤ìŒì€ AlexNetì˜ Convolution, Fully-Connected ë ˆì´ì–´ì—ì„œ, bit width ë³€í™”ì— ë”°ë¥¸ ì •í™•ë„ ë³€í™”ë¥¼ ë‚˜íƒ€ë‚¸ ê·¸ë˜í”„ë‹¤.

![quantization bits](images/quantization_bits.png)

- Conv layer: 4bitsê¹Œì§€ ì •í™•ë„ ìœ ì§€

- FC layer: 2bitsê¹Œì§€ ì •í™•ë„ ìœ ì§€

ì°¸ê³ ë¡œ ëŒ€í‘œì ì¸ CNN ëª¨ë¸ì—ì„œ Conv, FC layerì´ ê°–ëŠ” ë¹„ì¤‘ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

![CNN models Conv, FC layers](images/conv_fc_cnn.png)

---

## 5.6 Deep Compression: Vector Quantization

> [Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding ë…¼ë¬¸(2015)](https://arxiv.org/abs/1510.00149)

> [Deep Compression ë…¼ë¬¸ ë¦¬ë·°](https://velog.io/@woojinn8/LightWeight-Deep-Learning-3.-Deep-Compression-%EB%A6%AC%EB%B7%B0)

Deep Compression ë…¼ë¬¸ì€ (1) iterative pruning, (2) **vector quantization**(VQ), (3) Huffman encoding ë°©ë²•ì„ ê¸°ë°˜ìœ¼ë¡œ, ê°€ì¤‘ì¹˜ê°€ ì°¨ì§€í•˜ëŠ” ë©”ëª¨ë¦¬ë¥¼ íšê¸°ì ìœ¼ë¡œ ì¤„ì´ëŠ” ë°©ë²•ì„ ì œì•ˆí–ˆë‹¤. 

| Iterative Pruning | | Vector Quantization(VQ) | | Huffman Encoding |
| :---: | :---: | :---: | :---: | :---: |
| ![Deep Compression 1](https://github.com/erectbranch/TinyML_and_Efficient_DLC/blob/master/lec05/summary02/images/deep_compression_1.png) | $\rightarrow$ | ![Deep Compression 2](https://github.com/erectbranch/TinyML_and_Efficient_DLC/blob/master/lec05/summary02/images/deep_compression_2.png) | $\rightarrow$ | ![Deep Compression 3](https://github.com/erectbranch/TinyML_and_Efficient_DLC/blob/master/lec05/summary02/images/deep_compression_3.png) |
| original network ëŒ€ë¹„<br/>í¬ê¸° 9x-13x ê°ì†Œ | | original network ëŒ€ë¹„<br/>í¬ê¸° 27x-31x ê°ì†Œ | | original network ëŒ€ë¹„<br/>í¬ê¸° 35x-49x ê°ì†Œ |

---

### 5.6.1 K-Means-based Weight Quantization

Deep Compressionì—ì„œëŠ” **K-Means Algorithm** ê¸°ë°˜ì˜, **non-uniform weight quantization**ì„ ìˆ˜í–‰í•œë‹¤.(Vector Quantization)

> \#quantization levels = \#clusters

> Computer Graphicsì—ì„œë„ 65536ê°œì˜ ìŠ¤í™íŠ¸ëŸ¼ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ì›ë˜ ìƒ‰ìƒì„, 256ê°œ bucketì„ ê°–ëŠ” codebookì„ ë§Œë“¤ì–´ì„œ ìœ ì‚¬í•˜ê²Œ ì–‘ìí™”í•œë‹¤.

- storage: **Integer** Weights, Floating-Point Codebook

  - codebook: ì˜ˆì‹œ ê¸°ì¤€ìœ¼ë¡œ FP32 bucket 4ê°œë¥¼ ì‚¬ìš©í•œë‹¤. 
  
  - cluster index: bucketì´ 4ê°œì´ë¯€ë¡œ, 2bit(index 0,1,2,3)ë¡œ ì¶©ë¶„í•˜ë‹¤.

- computation: Floating-Point Arithmetic

| Weights<br/>(FP32 x 16) | ì••ì¶• | cluster index(INT2 x 16)<br/>centroids (FP32 x 4)| ì¶”ë¡  | Reconstructed<br/>(FP32 x 16) |
| :---: | :---: | :---: | :---: | :---: |
| ![deep compression ex 1](images/deep_compression_ex_1.png) | â†’ | ![deep compression ex 2](images/deep_compression_ex_2.png) | â†’ |  ![deep compression ex 3](images/deep_compression_ex_3.png) |

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ 1: K-Means-based Quantizationì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ &nbsp;&nbsp;&nbsp;</span>

ìœ„ ì˜ˆì‹œ ê·¸ë¦¼ì—ì„œ K-Means-based Quantization ì´ì „/ì´í›„ ì‚¬ìš©í•˜ëŠ” ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ê³„ì‚°í•˜ë¼.

### <span style='background-color: #C2B2B2; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ” í’€ì´&nbsp;&nbsp;&nbsp;</span>

- ì–‘ìí™” ì „ 

  FP32 $(4 \times 4)$ weight matrix

  $$32 \ \mathrm{bits} \times (4 \times 4) = 512 \ \mathrm{bits} = 64 \ \mathrm{bytes} $$

- ì–‘ìí™” í›„

  - weight matrix: INT2 x 16

  $$2 \times (4 \times 4) = 32 \ \mathrm{bits} = 4 \ \mathrm{bytes} $$
    
  - codebook: FP32 x 4

  $$32 \times (1 \times 4) = 128 \ \mathrm{bits} = 16 \ \mathrm{bytes} $$

ë”°ë¼ì„œ ì–‘ìí™” ì „ í•„ìš”í•œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì€ 64 bytes, ì–‘ìí™” í›„ í•„ìš”í•œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì€ 20 bytesì´ë‹¤.(3.2ë°° ì‚¬ìš©ëŸ‰ ê°ì†Œ) 

> weight tensorê°€ í¬ë©´ í´ìˆ˜ë¡, ê°€ì¤‘ì¹˜ì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ íš¨ê³¼ê°€ ë” ì»¤ì§„ë‹¤.

---

### 5.6.2 Finetuning Codebook

ìœ„ ì˜ˆì‹œì—ì„œ weightë¥¼ ë‹¤ì‹œ reconstruct(decode)í•œ ë’¤, errorë¥¼ ê³„ì‚°í•´ ë³´ì.

| ì–‘ìí™” ì „ | Decompressed | Error |
| :---: | :---: | :---: |
| ![deep compression error 1](images/deep_compression_ex_1.png) | ![deep compression error 2](images/deep_compression_ex_3.png) | <br>![deep compression error 3](images/deep_compression_ex_error.png) |

ì´ëŸ¬í•œ quantization errorëŠ” ì–‘ìí™” ì „, í›„ì˜ ê°€ì¤‘ì¹˜ ê°’ ì°¨ì´ë¥¼ ì¤„ì´ëŠ” ë°©ì‹ìœ¼ë¡œ, codebookì„ fine-tuningí•˜ë©° ê°œì„ í•  ìˆ˜ ìˆë‹¤.

1. cluster indexì— ë”°ë¼ quantization errorë¥¼ ë¶„ë¥˜í•œë‹¤.

    ![fine-tuning quantized weights 1](images/K-means_fine_tune_1.png)

2. mean errorë¥¼ êµ¬í•œë‹¤.

    ![fine-tuning quantized weights 2](images/K-means_fine_tune_2.png)

3. codebookì˜ centroidë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤.

    ![fine-tuning quantized weights 3](images/K-means_fine_tune_3.png)

---

### 5.6.3 K-Means-based Quantization Limitations

ê·¸ëŸ¬ë‚˜ K-Means-based weight quantizationì€ ë‹¤ìŒê³¼ ê°™ì€ í•œê³„ì ì„ ê°–ëŠ”ë‹¤.

- (-) ì—°ì‚° ì‹œ ë‹¤ì‹œ floating pointë¡œ reconstructëœë‹¤.

- (-) reconstruction ê³¼ì •ì—ì„œ time complexity, computation overheadê°€ í¬ë‹¤.

- (-) weightê°€ ë©”ëª¨ë¦¬ì—ì„œ ì—°ì†ì ì´ì§€ ì•Šê¸° ë–„ë¬¸ì—, memory accessì—ì„œ ê¸´ ì§€ì—°ì´ ë°œìƒí•˜ê²Œ ëœë‹¤.

- (-) activationì€ ì…ë ¥ì— ë”°ë¼ dynamicí•˜ê²Œ ë³€í•˜ë¯€ë¡œ, activation quantizationìœ¼ë¡œ clustering-based approachëŠ” ì í•©í•˜ì§€ ì•Šë‹¤.

---

### 5.6.4 Huffman Coding

ì¶”ê°€ë¡œ **Huffman Coding** ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í•˜ì—¬ memory usageë¥¼ ë” ì¤„ì¼ ìˆ˜ ìˆë‹¤.

> Unixì˜ íŒŒì¼ ì••ì¶•, JPEG, MP3 ì••ì¶•ì—ì„œ ì£¼ë¡œ ì‚¬ìš©ëœë‹¤.

> Encodingì˜ ë¶„ë¥˜ëŠ” í¬ê²Œ ë‘ ê°€ì§€ë¡œ ë‚˜ë‰œë‹¤. ê³ ì •ëœ ê¸¸ì´ë¡œ encodeí•˜ëŠ” RLC(Run Length Coding), ê°€ë³€ ê¸¸ì´ë¡œ encodeí•˜ëŠ” VLC(Variable Length Coding). Huffman Codingì€ ëŒ€í‘œì ì¸ VLCì— í•´ë‹¹ëœë‹¤.

![Huffman_coding](images/huffman_coding.png)

- frequent weights: bit ìˆ˜ë¥¼ ì ê²Œ ì‚¬ìš©í•´ì„œ í‘œí˜„í•œë‹¤.

- In-frequent weights: bit ìˆ˜ë¥¼ ë§ì´ ì‚¬ìš©í•´ì„œ í‘œí˜„í•œë‹¤.

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ 2: Huffman Coding &nbsp;&nbsp;&nbsp;</span>

a, b, c ì•ŒíŒŒë²³ì„ Huffman Codingì„ ì´ìš©í•´ ì••ì¶•í•˜ë¼.

> ASCII codeë¡œ í‘œí˜„í•˜ë ¤ê³  í•œë‹¤ë©´ INT8 x 3ìœ¼ë¡œ 24bitsë¥¼ ì‚¬ìš©í•´ì•¼ í•œë‹¤. í•˜ì§€ë§Œ Huffman codingì„ ì ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì¼ ìˆ˜ ìˆë‹¤. 

### <span style='background-color: #C2B2B2; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ” í’€ì´&nbsp;&nbsp;&nbsp;</span>

a, b, cë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì••ì¶•í•˜ì—¬ ì •ì˜í–ˆë‹¤ê³  í•˜ì.

- Try 1
  
    | a | b | c |
    | :---: | :---: | :---: |
    | 01 | 101 | 010 |

   $\rightarrow$ aì™€ cì˜ ì ‘ë‘ì–´ ë¶€ë¶„(`01`)ì´ ê²¹ì¹˜ê¸° ë•Œë¬¸ì— VLCë¡œ ì••ì¶•í•  ìˆ˜ ì—†ë‹¤.

- Try 2

    | a | b | c |
    | :---: | :---: | :---: |
    | 01 | 10 | 111 |

   $\rightarrow$ ê²¹ì¹˜ëŠ” ì ‘ë‘ì–´ê°€ ì—†ê¸° ë•Œë¬¸ì—, `01 10 111` = ì´ 7bitsë¡œ ì••ì¶•í•  ìˆ˜ ìˆë‹¤.

---

## 5.7 AND THE BIT GOES DOWN: Product Quantization

> [AND THE BIT GOES DOWN: REVISITING THE QUANTIZATION OF NEURAL NETWORKS ë…¼ë¬¸(2019)](https://arxiv.org/abs/1907.05686)

ì–´ë–¤ ë ˆì´ì–´ê°€ $(C_{in} \times K \times K)$ í¬ê¸°ì˜ 3D í…ì„œ $C_{out}$ ê°œë¥¼ ê°–ëŠ”ë‹¤ê³  í•˜ì. ìœ„ ë…¼ë¬¸ì—ì„œëŠ” í•©ì„±ê³± í•„í„°ê°€ ê°–ëŠ” spatial redundancyë¥¼ ì´ìš©í•  ìˆ˜ ìˆë„ë¡, $K \times K$ í¬ê¸°ë¥¼ ê°–ëŠ” subvector ë‹¨ìœ„ë¡œ vector quantizationì„ ì ìš©í•œë‹¤.

- ê° 3ì°¨ì› í…ì„œë¥¼, subvector $C_{in}$ ê°œë¡œ êµ¬ì„±ëœ ë‹¨ì¼ ë²¡í„°ë¡œ reshapeí•œë‹¤.

  - subvector size $d$ : $K \times K$

  - \#subvectors per vector: $C_{in}$

- (í¬ê¸° $d$ ë¥¼ ê°–ëŠ”) subvector $k$ ê°œë¡œ êµ¬ì„±ëœ codebook ê¸°ë°˜ìœ¼ë¡œ ì–‘ìí™”í•œë‹¤.

| Filters | Reshaped filters | Codebook |
| :---: | :---: | :---: |
| ![codebook](images/ATBGD_1.png) | ![codeword](images/ATBGD_2.png) | ![codebook index](images/ATBGD_3.png) |

---

### 5.7.1 Product Quantization

> [MATRIN KERSNER BLOG: Kill the bits and gain the speed?](https://martinkersner.com/2019/11/28/kill-the-bits/#product-quantization)

ì•ì„œ ì‚´í´ë³¸ Vector Quantization(VQ)ëŠ” **Product Quantization**(PQ)ì˜ íŠ¹ìˆ˜í•œ ê²½ìš°ë¡œ ë³¼ ìˆ˜ ìˆë‹¤. ë‹¤ìŒì€ Product Quantizationì˜ ë‘ ê°€ì§€ ê²½ìš°ë¥¼ ë¹„êµí•œ í‘œë‹¤.

| | Vector Quantization | Scalar K-means algorithm |
| :---: | :---: | :---: |
| subvector size $d$ | $C_{in}$ | $1$ |
| \#subvectors per vector | $1$ | $C_{in}$ |

ë˜í•œ product quantizationì—ì„œ codebook $C = \lbrace c_1, \cdots , c_k \rbrace$ ëŠ”, í¬ê¸° $d$ ë¥¼ ê°–ëŠ” centroid(**codeword**) $k$ ê°œë¡œ êµ¬ì„±ëœë‹¤.

| | Codebook | Codeword | 
| :---: | :---: | :---: |
| | ![codebook](images/pq_codebook_codeword_1.png) | ![codeword](images/pq_codebook_codeword_2.png) |
| dimension | $d \times k$ | $d$ |

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ 3: Product Quantizationì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ &nbsp;&nbsp;&nbsp;</span>

ë‹¤ìŒê³¼ ê°™ì€ ì¡°ê±´ì—ì„œ Product Quantizationìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ê³„ì‚°í•˜ë¼.

- ì…ë ¥ ë ˆì´ì–´: $(128 \times 128 \times 3 \times 3)$

- \#centroids: $k = 256$ 

  ë°ì´í„° íƒ€ì…ì€ `float16`ì„ ì‚¬ìš©í•˜ë©°, ê° subvectorëŠ” 1 byteë¥¼ ì°¨ì§€í•œë‹¤ê³  ê°€ì •í•œë‹¤.

- block size: $d = 9$

### <span style='background-color: #C2B2B2; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ” í’€ì´&nbsp;&nbsp;&nbsp;</span>

ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì€ í¬ê²Œ (1) indexing costì™€ (2) FP16 íƒ€ì…ì˜ centroidê°€ ì°¨ì§€í•˜ëŠ” ë©”ëª¨ë¦¬ë¡œ ë‚˜ë‰œë‹¤.

- indexing cost

  ì…ë ¥ ë ˆì´ì–´ì˜ \#blocks $m$ ì€ $128 \times 128 = 16,384$ ê°œë‹¤. ë”°ë¼ì„œ 16kB ë©”ëª¨ë¦¬ë¥¼ ì°¨ì§€í•œë‹¤.
  
- centroids

  FP16ì„ ì‚¬ìš©í•˜ë¯€ë¡œ, 256ê°œ centroidsê°€ ì°¨ì§€í•˜ëŠ” ë©”ëª¨ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. 
  
  $$9 \times 256 \times 2 \ \mathrm{bytes} = 4,608 \ \mathrm{bytes}$$

---

### 5.7.2 Minimize Difference between Output Activations

ìµœì ì˜ centroid(codeword)ë¥¼ ì°¾ê¸° ìœ„í•œ ë°©ë²•ì„ ì‚´í´ë³´ì. ë¨¼ì € ì–‘ìí™” ì „,í›„ ê°€ì¤‘ì¹˜ ê°’ì„ ë¹„êµí•˜ë©°, quantization errorë¥¼ ìµœì†Œí™”í•˜ëŠ” objective functionì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆë‹¤.

$$ || W - \widehat{W}|{|}_2^2 = \sum_{j} || w_j - q(w_j) |{|}_2^2 $$

- $q(w_j) = (c_{i_1}, c_{i_2}, \cdots , c_{i_m})$

í•˜ì§€ë§Œ ë…¼ë¬¸ì—ì„œëŠ” ì–‘ìí™” ì „,í›„ì˜ ì°¨ì´ë¥¼ ìµœì†Œí™”í•´ ì–»ì€ ê°€ì¤‘ì¹˜ê°€, ë°˜ë“œì‹œ ì–‘ìí™” ì „ì˜ ì¶œë ¥ê³¼ ë¹„ìŠ·í•œ ê²°ê³¼ë¥¼ ë³´ì¥í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì‚¬ì‹¤ì— ì£¼ëª©í•œë‹¤. ëŒ€ì‹  in-domain inputì„ ì¶”ë¡ ì‹œí‚¤ë©´ì„œ, activationì„ ëŒ€ìƒìœ¼ë¡œ ì–‘ìí™” ì „,í›„ ì°¨ì´ë¥¼ ìµœì†Œí™”í•˜ëŠ” objective functionì„ ì œì•ˆí•œë‹¤.

$$ || y - \widehat{y}|{|}_2^2 = \sum_{j} || x(w_j - q(w_j)) |{|}_2^2 $$

ë‹¤ìŒì€ ê°œì™€ ê³ ì–‘ì´ë¥¼ ë¶„ë¥˜í•˜ëŠ” ê°„ë‹¨í•œ binary classifier $\varphi$ ë¥¼ ëŒ€ìƒìœ¼ë¡œ, ë‘ ê°€ì§€ objective functionì„ ì‚¬ìš©í•œ ê²°ê³¼ë¥¼ ë¹„êµí•œ ê·¸ë¦¼ì´ë‹¤.

![activation based objective function](images/ATBGD_objective_function.png)

- weight-based(ë¹¨ê°„ìƒ‰), activation-based(ì´ˆë¡ìƒ‰)

- in-domain ì…ë ¥ì— ëŒ€í•´, activation-based objective functionìœ¼ë¡œ ìµœì í™”í•œ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ë” ìš°ìˆ˜í•˜ë‹¤.

---

## 5.8 Linear Quantization

ë‹¤ìŒì€ ì¼ì •í•œ step sizeë¥¼ ê°€ì§€ëŠ” ëŒ€í‘œì ì¸ ì–‘ìí™” ë°©ë²•ì¸ **Linear Quantization**ì„ ì‚´í´ë³´ì. ì•ì„œ K-means-based quantization ì˜ˆì œì— linear quantizationì„ ì ìš© ì‹œ ë‹¤ìŒê³¼ ê°™ë‹¤.

- integer $q \rightarrow$ real $r$ ì˜ affine mappingìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤.

  - $Z$ : Zero points

  - $S$ : Scaling factor

$$ r = S(q-Z) $$

| Weights<br/>(FP32 x 16) |  | Quantized Weights<br/>(INT2) |  | Reconstruction<br/>(FP32 x 16) |
| :---: | :---: | :---: | :---: | :---: |
| ![linear quantization ex 1](images/deep_compression_ex_1.png) | â†’ | ![linear quantization ex 2](images/linear_quantization_ex_2.png) | â†’ |  ![linear quantization ex 3](images/linear_quantization_ex_3.png) |

---

### 5.8.1 Zero Point, Scaling Factor

zero point $Z$ , scaling factor $S$ ë¥¼ ê³„ì‚°í•´ ë³´ì.

![linear quantization mapping](images/linear_quantization_mapping.png)

1. ë¨¼ì € ì–‘ìí™”ë¥¼ ì ìš©í•  floating-point rangeë¥¼ ì •í•œë‹¤.

    - $r_{min}$ : real number $r$ ìµœì†Œê°’

    - $r_{max}$ : real number $q$ ìµœëŒ€ê°’

2. integer clipping rangeë¥¼ ì •í•œë‹¤.

    - $q_{min}$ : integer $q$ ìµœì†Œê°’

    - $q_{max}$ : integer $q$ ìµœëŒ€ê°’

3. ë‹¤ìŒ ì‹ì„ ê³„ì‚°í•˜ì—¬ zero point, scaling factorë¥¼ êµ¬í•œë‹¤.

    - $r_{max} = S(q_{max} - Z)$

    - $r_{min} = S(q_{min} - Z)$

ì´ë–„ ë‘ ì‹ì„ ì¡°í•©í•˜ì—¬ scaling factorì— ëŒ€í•œ ì‹ìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ìˆë‹¤.

$$ S = {{r_{max} - r_{min}} \over {q_{max} - q_{min}}} $$

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ 4: linear quantization &nbsp;&nbsp;&nbsp;</span>

ë‹¤ìŒ weight matrixì—ì„œ zero point, scaling factor ê°’ì„ êµ¬í•˜ë¼.

![floating-point matrix](images/floating-point_matrix.png)

### <span style='background-color: #C2B2B2; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ” í’€ì´&nbsp;&nbsp;&nbsp;</span>

ì˜ˆì‹œ í–‰ë ¬ì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ê°’ë“¤ì„ êµ¬í•  ìˆ˜ ìˆë‹¤.

- $r_{min}, r_{max} = [-1.08, 2.12]$

- $q_{min}, q_{max} = [-2, 1]$

ìœ„ ê°’ì„ ê°€ì§€ê³  scaling factor $S$ ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.

$$ S = {{2.12 - (-1.08)} \over {1 - (-2)}} = 1.07 $$

$r_{min}$ í˜¹ì€ $r_{max}$ ë°©ì •ì‹ì— $S$ ë¥¼ ëŒ€ì…í•˜ë©´, zero point $Z$ ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤. ì´ë•Œ $Z$ ê°€ ì •ìˆ˜ê°€ ë˜ë„ë¡ round(ë°˜ì˜¬ë¦¼) ì—°ì‚°ì„ ì ìš©í•´ì•¼ í•œë‹¤.

$$ Z = \mathrm{round}{\left( q_{min} - {{r_{min}} \over S} \right)} = \mathrm{round}{\left( -2 - {{-1.08} \over {1.07}} \right)} = 1 $$

---

### 5.8.2 Sources of Quantization Error

linear quantization errorì™€, ì´ë¥¼ ë°œìƒì‹œí‚¤ëŠ” ì›ì¸ì„ ì•Œì•„ë³´ì. ë‹¤ìŒì€ linear quantizationì„ ë‚˜íƒ€ë‚´ëŠ” ê·¸ë¦¼ì´ë‹¤.

![quant error example 1](images/quant_error_ex_1.png)

- ë‹¤ìŒê³¼ ê°™ì€ ì„œë¡œ ë‹¤ë¥¸ ê°’ì´ ë™ì¼í•œ integer domain gridì— mappingëœë‹¤.

  - ê·¼ì ‘í•œ ë‘ FP32 ê°’

  - $q_{max}$ ë³´ë‹¤ í° FP32 outlier: $2^{b} - 1$ ë¡œ ë§¤í•‘

ë‹¤ìŒì€ ì–‘ìí™”ëœ ê°’ì„ ë‹¤ì‹œ ë³µì›í•œ ë’¤ error ê°’ì„ ë‚˜íƒ€ë‚¸ ê·¸ë¦¼ì´ë‹¤. 

> quantization errorëŠ” round error, clip errorë¥¼ í•©í•œ ê°’ì´ë‹¤.

![quant error example 2](images/quant_error_ex_2.png)

- outlierì— ë”°ë¥¸ clip errorê°€ í¬ë‹¤.

í•˜ì§€ë§Œ ë‹¤ìŒê³¼ ê°™ì´ $q_{max}$ ê°’ì´ í° ì˜ˆì‹œì—ì„œëŠ”, clip errorë³´ë‹¤ round errorê°€ ë” ì»¤ì§€ê²Œ ëœë‹¤. 

![quant error example 3](images/quant_error_ex_3.png)

- roundingì— ë”°ë¥¸ round errorê°€ í¬ë‹¤.

ë”°ë¼ì„œ ì´ëŸ¬í•œ **trade-off** ê´€ê³„ë¥¼ ê³ ë ¤í•˜ì—¬ quantization rangeë¥¼ ì •í•  í•„ìš”ê°€ ìˆë‹¤.

---

## 5.9 Linear Quantized Matrix Multiplication

linear quantization ì—°ì‚°ì€ **affine mapping**(ì•„í•€ë³€í™˜)ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤.

$$ r = S(q - Z) $$

> affine mapping: linear transform í›„ translationí•˜ëŠ” ë³€í™˜. non-linear transformì— í•´ë‹¹ëœë‹¤.

---

### 5.9.1 Linear Quantized Fully-Connected Layer

ë¨¼ì € Fully-Connected layer + linear quantization ìˆ˜ì‹ì„ ì‚´í´ë³´ì.

$$ Y = WX + b $$

$$ \downarrow $$

$$ S_{Y}(q_{Y} - Z_{Y}) =  S_{W}(q_{W} - Z_{W}) \cdot S_{X}(q_{X} - Z_{X}) + S_b(q_b - Z_b) $$

1. weight zero point $Z_{W} = 0$ ë¡œ ê°€ì •í•˜ë©´, ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜ì‹ì´ ë°”ë€ë‹¤.

$$ S_{Y}(q_{Y} - Z_{Y}) =  S_{W}S_{X}(q_{W}q_{X} - Z_{X}q_{W})  + S_b(q_b - Z_b) $$

2. bias zero point $Z_b = 0$ , scaling factor $S_b = S_W S_X$ ë¡œ ê°€ì •í•˜ë©´, ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜ì‹ì´ ë°”ë€ë‹¤.

   > ì´ì²˜ëŸ¼ bias, weightì˜ zero pointê°€ ëª¨ë‘ 0ì¸ ê²½ìš°ëŠ”, symmetric quantizationì— í•´ë‹¹ëœë‹¤.

$$ S_{Y}(q_{Y} - Z_{Y}) =  S_{W}S_{X}(q_{W}q_{X} - Z_{X}q_{W}+ q_b) $$

3. ìœ„ ìˆ˜ì‹ì„ integer ì¶œë ¥ $q_{Y}$ ì— ê´€í•œ ì‹ìœ¼ë¡œ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$ q_{Y} = {{S_{W}S_{X}} \over {S_{Y}}}(q_{W}q_{X} + q_b - Z_{W}q_{X}) + Z_{Y} $$

4. ì—°ì‚° ì „ì— ì•Œ ìˆ˜ ìˆëŠ” í•­ì„ biasë¡œ í•©ì³ì¤€ë‹¤.

    > $q_b - Z_X q_W = q_{bias}$

    > Note: $q_b$ , $q_{bias}$ ëª¨ë‘ 32 bits integerì´ë‹¤.

$$ q_{Y} = {{S_{W}S_{X}} \over {S_{Y}}}(q_{W}q_{X} + q_{bias}) + Z_{Y} $$

ì´ì œ ë‚˜ë¨¸ì§€ í•­ì´ ì–´ë–¤ ì—°ì‚°ì¸ì§€ ì‚´í´ë³´ì.

$(1) \quad {{S_{W}S_{X}} \over {S_{Y}}} = 2^{-n}M_{0}$

- bit shift( $2^{-n}$ )ì™€ fixed point ê³±ì…ˆ ( $M_0$ )ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤. ( $M_0 \in [0.5, 1)$ )

  - INT32ë¥¼ N-bit integerë¡œ, ë‹¤ì‹œ rescaleí•˜ëŠ” ì—­í• ì„ í•œë‹¤.

  - ì–¸ì œë‚˜ $(0, 1)$ ì‚¬ì´ì˜ ê°’ì„ ê°–ëŠ”ë‹¤.

$(2) \quad q_{W}q_{X} + q_{bias}$

- N-bit integer ê³±ì…ˆ í›„, 32-bit integer ë§ì…ˆì„ ìˆ˜í–‰í•œë‹¤.

$(3) \quad Z_{Y}$

- N-bit integer ë§ì…ˆì„ ìˆ˜í–‰í•œë‹¤.

---

### 5.9.2 Linear Quantized Convolution Layer

$$ Y = \mathrm{Conv} (W, X) + b $$

$$ \downarrow $$

$$ q_{Y} = {{S_{W}S_{X}} \over {S_{Y}}}(\mathrm{Conv}(q_{W}, q_{X}) + q_{bias}) + Z_{Y} $$

ê° í•­ì´ ì–´ë–¤ ì—°ì‚°ì¸ì§€ ì‚´í´ë³´ì.

$(1) \quad {{S_{W}S_{X}} \over {S_{Y}}}$

- bit shift( $2^{-n}$ )ì™€ fixed point ê³±ì…ˆ ( $M_0$ )ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤. ( $M_0 \in [0.5, 1)$ )

  - INT32ë¥¼ N-bit integerë¡œ, ë‹¤ì‹œ rescaleí•˜ëŠ” ì—­í• ì„ í•œë‹¤.

$(2) \quad \mathrm{Conv}(q_{W}q_{X}) + q_{bias}$

-  N-bit integer ê³±ì…ˆ. 32-bit integer ë§ì…ˆì„ ìˆ˜í–‰í•œë‹¤.

$(3) \quad Z_{Y}$

- N-bit integer ë§ì…ˆì„ ìˆ˜í–‰í•œë‹¤.

ìœ„ ì—°ì‚°ì„ ê·¸ë˜í”„ë¡œ ê·¸ë¦¬ë©´ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.

![CNN quantized computation graph](images/cnn_quantized_computational_graph.png)

---
