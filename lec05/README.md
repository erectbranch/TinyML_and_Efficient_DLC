# Lecture 05 - Quantization (Part I)

> [Lecture 05 - Quantization (Part I) | MIT 6.S965](https://youtu.be/91stHPsxwig)

> [A White Paper on Neural Network Quantization](https://arxiv.org/abs/2106.08295)

> [tinyML Talks: A Practical Guide to Neural Network Quantization](https://youtu.be/KASuxB3XoYQ)

> [quantization ì •ë¦¬](https://gaussian37.github.io/dl-concept-quantization/)

---

## 5.1 Numeric Data Types

---

### 5.1.1 Integer

ìš°ì„  **integer**(ì •ìˆ˜)ë¥¼ 8bitë¡œ í‘œí˜„í•œ ì„¸ ê°€ì§€ ì˜ˆì‹œë¥¼ ì‚´í´ë³´ì. 

![integer](images/integers.png)

- ì²« ë²ˆì§¸: unsigned integer

  range: $[0, 2^{n} - 1]$

- ë‘ ë²ˆì§¸: (signed integer) Sign-Magnitude

  range: $[-2^{n-1} - 1, 2^{n-1} - 1]$

   > 00000000ê³¼ 10000000ì€ ëª¨ë‘ 0ì„ í‘œí˜„í•œë‹¤.

- ì„¸ ë²ˆì§¸: (signed integer) 2-bit complement Representation

  range: $[-2^{n-1}, 2^{n-1} - 1]$

   > 00000000ì€ 0, 10000000ì€ $-2^{n-1}$ ì„ í‘œí˜„í•œë‹¤.

---

### 5.1.2 fixed-point number

ì†Œìˆ˜(**decimal**)ë¥¼ í‘œí˜„í•˜ëŠ” ë°©ì‹ì€ ë‘ ê°€ì§€ê°€ ìˆë‹¤.

- **fixed-point number**(ê³ ì • ì†Œìˆ˜ì  ì—°ì‚°)

- **floating-point number**(ë¶€ë™ ì†Œìˆ˜ì  ì—°ì‚°)

ì•„ë˜ëŠ” 8bit fixed-point numberë¥¼ ë‚˜íƒ€ë‚¸ ê·¸ë¦¼ì´ë‹¤.

![fixed-point](images/fixed_point.png)

- ë§¨ ì• 1bitëŠ” sign bitë¡œ ì‚¬ìš©í•œë‹¤.

- 3bitsë¡œ integer(ì •ìˆ˜)ë¥¼ í‘œí˜„í•œë‹¤.

- 4bitsë¡œ fraction(ì†Œìˆ˜)ì„ í‘œí˜„í•œë‹¤.

> ë‘ ë²ˆì§¸ì™€ ì„¸ ë²ˆì§¸ ì—°ì‚°ì˜ ì°¨ì´: ì†Œìˆ˜ì ( $2^{-4}$ ) ì˜ ìœ„ì¹˜ë¥¼ ë‚˜ì¤‘ì— ê³±í•˜ì˜€ë‹¤.

ìœ„ì™€ ê°™ì€ ì˜ˆì‹œë¥¼ `fixed<w,b>`ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤. `w`ê°€ ì´ bit width, `b`ê°€ fraction bit widthì´ë‹¤.

> 32bit ì˜ˆì‹œ: 1bit sign bit, 15bit integer, 16bit fraction

---

### 5.1.3 floating-point number

ë‹¤ìŒì€ 32bit **floating-point** numberì˜ ì˜ˆì‹œë‹¤.(ê°€ì¥ ë³´í¸ì ì¸ **IEEE 754** ë°©ë²•)

![32bit floating-point](images/32bit_floating_point.png)

$$ (-1)^{sign} \times (1 + \mathrm{Fraction}) \times 2^{\mathrm{Exponent} - 1} $$

- sign: ë¶€í˜¸ë¥¼ ë‚˜íƒ€ë‚´ëŠ” 1bit

- **exponent**: ì§€ìˆ˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ” 8bit

- fraction(mantissa): ê°€ìˆ˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ” 23bit

> 32bit(4byte)ëŠ” single precision(ë‹¨ì •ë„), 64bit(8byte)ëŠ” double precision(ë°°ì •ë„)ì´ë‹¤.

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ 1: IEEE 754 í‘œì¤€ì— ë”°ë¼ ìˆ«ì í‘œí˜„í•˜ê¸° &nbsp;&nbsp;&nbsp;</span>

ìˆ«ì -314.625ë¥¼ IEEE 754 í‘œì¤€ì— ë”°ë¼ í‘œí˜„í•˜ë¼.

### <span style='background-color: #C2B2B2; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ” í’€ì´&nbsp;&nbsp;&nbsp;</span>

1. ìŒìˆ˜ì´ë¯€ë¡œ **sign bit**ëŠ” 1ì´ë‹¤.

2. **fraction**

    -314.625ì˜ ì ˆëŒ“ê°’ $314.625$ ë¥¼ 2ì§„ìˆ˜ë¡œ ë³€í™˜í•˜ë©´ ${100111010.101}_{(2)}$ ê°€ ëœë‹¤.

    - ì†Œìˆ˜ì ì„ ì˜®ê²¨ì„œ ì¼ì˜ ìë¦¬ ìˆ˜, ì†Œìˆ˜ì  í˜•íƒœë¡œ ë§Œë“ ë‹¤. 
    
    - ì†Œìˆ˜ì  ë¶€ë¶„ë§Œì„ fraction 23bit ë¶€ë¶„ì— ë§¨ ì•ë¶€í„° ì±„ìš´ë‹¤.

      > ë‚¨ëŠ” ìë¦¬ëŠ”  0ìœ¼ë¡œ ì±„ìš´ë‹¤.

$$ 1.00111010101 \times 2^{8} $$

3. **exponent**

    biasë¥¼ ê³„ì‚°í•´ì•¼ í•œë‹¤. (bias = $2^{k-1}$ )
    
    - $k$ : exponent ë¶€ë¶„ì˜ bit ìˆ˜ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. 
    
    $$2^{8-1} = 127$$

    8 + 127 = 135ë¥¼ 2ì§„ìˆ˜ë¡œ ë³€í™˜í•˜ë©´ ${10000111}_{(2)}$ ì´ ëœë‹¤.

    - ë³€í™˜í•œ 2ì§„ìˆ˜ë¥¼ 8bit exponent ë¶€ë¶„ì— ì±„ì›Œì¤€ë‹¤.

ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

| sign bit | exponent | fraction |
| :---: | :---: | :---: | 
| 1 | 10000111 | 00111010101000000000000 | 

---

### 5.1.4 floating-point number comparison

ë‹¤ì–‘í•œ floating-point number í‘œí˜„ë²•ì„ ë¹„êµí•´ë³´ì. íŠ¹íˆ neural networkì—ì„œëŠ” <U>fractionë³´ë‹¤ë„ exponentì— ë” ë¯¼ê°</U>í•˜ê¸° ë–„ë¬¸ì—, exponent ì •ë³´ë¥¼ ìµœëŒ€í•œ ë³´ì¡´í•˜ëŠ” í‘œí˜„ë²•ì´ ë“±ì¥í–ˆë‹¤.

- underflow, overflow, NaNì„ ë” ì˜ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ì„œëŠ”, exponentì„ ìµœëŒ€í•œ ë³´ì¡´í•˜ì—¬ ì •í™•ë„ë¥¼ ìœ ì§€í•´ì•¼ í•œë‹¤.

- ë” ì‘ì€ bitë¥¼ ì‚¬ìš©í•˜ë©´ì„œ memory, latencyëŠ” ì¤„ì´ê³ , accuracyëŠ” ìµœëŒ€í•œ ë³´ì¡´í•˜ëŠ” ê²ƒì´ ëª©í‘œ.

![floating point ex](images/floating_point_ex.png)

- **Half Precision**(FP16)

    exponent 5 bit, fractionì€ 10 bit

- Brain Float(BF16)

    IEEE FP32ì™€ ë¹„êµí–ˆì„ ë•Œ, exponent 7bitë¡œ ì¤„ì˜€ì§€ë§Œ fractionì€ 8bitë¡œ ìœ ì§€í–ˆë‹¤. 

- TensorFloat(TF32)
    
    exponent 10bit, fraction 8bitì´ë‹¤. 
    
    > FP16ê³¼ ë™ì¼í•œ exponent(10bit), FP32ì™€ ë™ì¼í•œ fraction(8bit)ë¥¼ ì§€ì›í•œë‹¤.

    > BERT ëª¨ë¸ì—ì„œ TF32 V100ì„ ì´ìš©í•œ í•™ìŠµì´, FP32 A100ì„ ì´ìš©í•œ í•™ìŠµì— ë¹„í•´ 6ë°° speedupì„ ë‹¬ì„±í–ˆë‹¤.

---

## 5.2 Quantization

![quantized signal](images/quantized_signal.png)

continuous í˜¹ì€ large set of values íŠ¹ì„±ì„ ê°€ì§„ ì—°ì†ì ì¸ ì…ë ¥ì„ discrete setìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê²ƒì„ **quantization**(ì–‘ìí™”)ë¼ê³  ì§€ì¹­í•œë‹¤.

![quantized image](images/quantized_image.png)

ë‹¤ìŒì€ quantizationì„ í†µí•´ ì–»ì„ ìˆ˜ ìˆëŠ” ëª‡ ê°€ì§€ ì´ì ì´ë‹¤.

- memory usage

- power consumption

- latency

- silicon area

neural networkì— quantizationì„ ì ìš©í•˜ê¸° ì „/í›„ì˜ weight ë¶„í¬ ì°¨ì´ë¥¼ ì‚´í´ë³´ì. continuous spaceê°€ discrete spaceë¡œ ë§¤í•‘ëœë‹¤.

![continuous weight](images/continuous-weight.png)

![discrete weight](images/discrete-weight.png)

> fine-tuningì„ ì ìš©í•˜ë©´ ì—¬ê¸°ì„œ ì¡°ê¸ˆ ë” ë³€í™”ê°€ ìƒê¸´ë‹¤.

---

### 5.2.1 Matrix operations with quantized weights

ìš°ì„  $WX + b$ ê¼´ì˜ í–‰ë ¬ ì—°ì‚°ì´ ì–´ë–»ê²Œ ì»´í“¨í„°ì—ì„œ ì§„í–‰ë˜ëŠ”ì§€ ì‚´í´ë³´ì.

```math
W = \begin{bmatrix} 0.97 & 0.64 & 0.74 & 1.00 \\ 0.58 & 0.84 & 0.84 & 0.81 \\ 0.00 & 0.18 & 0.90 & 0.28 \\ 0.57 & 0.96 & 0.80 & 0.81 \end{bmatrix} \quad X = \begin{bmatrix} 0.41 & 0.25 & 0.73 & 0.66 \\ 0.00 & 0.41 & 0.41 & 0.57 \\ 0.42 & 0.24 & 0.71 & 1.00 \\ 0.39 & 0.82 & 0.17 & 0.35 \end{bmatrix} \quad b = \begin{bmatrix} 0.1 \\ 0.2 \\ 0.3 \\ 0.4 \end{bmatrix}
```

ì•„ë˜ ê·¸ë¦¼ì€ MAC ì—°ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” arrayë¥¼ í‘œí˜„í•œ ì˜ˆì‹œë‹¤.

![MAC array](images/MAC_array.png)

$$ A_{i} = \sum_{j}{C_{i,j}} + b_i $$

$$ A_{i} = W_i \cdot \mathrm{x_1} + W_i \cdot \mathrm{x_2} + W_i \cdot \mathrm{x_3} + W_i \cdot \mathrm{x_4} $$

1. $C_{i,j}$ ìë¦¬ì— ë¨¼ì € í–‰ë ¬ $W$ ê°’ì„ loadí•œë‹¤.

2. í•œ ì‚¬ì´í´ë§ˆë‹¤ í–‰ë ¬ $X$ ì—ì„œ ë‹¤ìŒ input valueë¥¼ ê°€ì ¸ì˜¨ë‹¤.

```math
\begin{bmatrix} 0.41 \\ 0.00 \\ 0.42 \\ 0.39 \end{bmatrix}
```

3. ì—°ì‚° í›„ í–‰ë ¬ $X$ ì˜ ë‹¤ìŒ ì—´ì„ ê°€ì ¸ì™€ì„œ ìˆœì°¨ì ìœ¼ë¡œ ê³„ì‚°í•œë‹¤.

ê·¸ëŸ°ë° ì´ ê³¼ì •ì— **weight, bias quantization**ì„ ì¶”ê°€í•˜ë©´ ì–´ë–»ê²Œ ë ê¹Œ?

1. floating-point tensor ëŒ€ì‹  **scaling factor** $s_{X}$ ê°€ ê³±í•´ì§„ í˜•íƒœì˜ integer tensor ë¥¼ ì‚¬ìš©í•œë‹¤.

```math
X_{fp32} \approx s_{X}X_{int} = \hat{X}
```

```math
\hat{X} = {{1} \over {255}} \begin{bmatrix} 105 & 64 & 186 & 168 \\ 0 & 105 & 105 & 145 \\ 107 & 61 & 181 & 255 \\ 99 & 209 & 43 & 89 \end{bmatrix}
```

- $\hat{X}$ : scaled quantized tensor

- **ìµœì†Œê°’ 0â†’0**, **ìµœëŒ€ê°’ 1.00â†’255**, `uint8` íƒ€ì…ìœ¼ë¡œ ë§¤í•‘ë˜ì—ˆë‹¤.

2. weight tensorë„ scaling factor $s_{W}$ ë¥¼ ê³±í•œ integer tensorë¥¼ ì‚¬ìš©í•œë‹¤.

```math
W = \begin{bmatrix} 0.97 & 0.64 & 0.74 & 1.00 \\ 0.58 & 0.84 & 0.84 & 0.81 \\ 0.00 & 0.18 & 0.90 & 0.28 \\ 0.57 & 0.96 & 0.80 & 0.81 \end{bmatrix} \approx {{1} \over {255}}\begin{bmatrix} 247 & 163 & 189 & 255 \\ 148 & 214 & 214 & 207 \\ 0 & 46 & 229 & 71 \\ 145 & 245 & 204 & 207 \end{bmatrix} = s_{W}W_{uint8}
```

3. bias tensorëŠ” `int32` íƒ€ì…ìœ¼ë¡œ ë§¤í•‘ëœë‹¤.

```math
\hat{b} = {{1} \over {255^2}}\begin{bmatrix} 650 \\ 1300 \\ 1951 \\ 650 \end{bmatrix} 
```

-  **overflow**ë¥¼ í”¼í•˜ê¸° ìœ„í•´ì„œëŠ” ì´ì²˜ëŸ¼ <U>ë” í° bit widthë¥¼ ì‚¬ìš©</U>í•´ì•¼ í•œë‹¤.

- $\hat{W}, \hat{X}$ ê°€ ê°€ì§€ê³  ìˆëŠ” ${{1} \over {255}}$ ê°€ ì„œë¡œ ê³±í•´ì§€ë©´ ${{1} \over {255^2}}$ ê°€ ë˜ë¯€ë¡œ, quantized bias $\hat{b}$ ëŠ” scaling factorë¡œ ${{1} \over {255^2}}$ ë¥¼ ì‚¬ìš©í•œë‹¤.

ì´ì œ ì‹¤ì œ ì—°ì‚° ê³¼ì •ì„ ë³´ì. ìš°ì„  $\hat{W}, \hat{X}$ ì—ì„œ scaling factorë¥¼ ì œì™¸í•œ ê°’ì„ í–‰ë ¬ ì—°ì‚° í•œ ë’¤ì—, ê²°ê³¼ê°’ì— ${{1} \over {255^2}}$ ë¥¼ ê³±í•´ì„œ scaleì„ ë‹¤ì‹œ ë§ì¶°ì¤€ë‹¤.

![quantized MAC array](images/quantized_MAC_array_ex.png)

í•˜ì§€ë§Œ ì´ë ‡ê²Œ ì–»ì€ `int32` activation ê°’ì„ ì´ë³´ë‹¤ ë” ë‚®ì€ ì •ë°€ë„ì¸ `int8`ë¡œ ë°”ê¾¸ê³  ì‹¶ë‹¤. ì´ê²ƒì´ **activation quantization**ì´ë©°, ë‹¤ìŒê³¼ ê°™ì´ ê²°ê³¼ê°’ $\hat{Out}$ ì„ `uint8`ë¡œ ì–‘ìí™”í•  ìˆ˜ ìˆë‹¤.

```math
\hat{Out} = {{1} \over {136671 \cdot 255}} \begin{bmatrix} 134 & 185 & 206 & 255 \\ 111 & 167 & 186 & 242 \\ 60 & 65 & 96 & 134 \\ 109 & 172 & 187 & 244 \end{bmatrix}
```

- ìµœëŒ€ê°’ 136671ì´ scaling factorì— ì“°ì¸ ì ì— ì£¼ëª©í•˜ì.

---

### 5.2.2 Symmetric vs Asymmetric Quantization

![symmetric, asymmetric, unsigned quantization](images/symmetric_asymmetric_signed.png)

> ìœ„ ì˜ˆì‹œì˜ ë°ì´í„° ë¶„í¬ì—ì„œëŠ” symmetric signed quantizationì´ ë” ì •ë°€í•˜ê²Œ ë°ì´í„°ë¥¼ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.

- **symmetric quantization** : zero pointê°€ 0ì¸ ê²½ìš°

    `signed int`: zero pointê°€ 0ì— ì •í™•íˆ ë§¤í•‘ëœë‹¤.

    `unsigned int`: ReLUì™€ ê°™ì´ unsigned ì¶œë ¥ì´ ë‚˜ì˜¤ëŠ” ê²½ìš° ìœ ë¦¬í•˜ë‹¤.

- **asymmetric quantization** : zero pointê°€ 0ì´ ì•„ë‹Œ ê²½ìš°

    - FP32 ë°ì´í„° ë¶„í¬ê°€ ëŒ€ì¹­ì ì´ì§€ ì•Šìœ¼ë©´, asymmetricì´ ë” ì •ë°€í•˜ê²Œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.

    - í‘œí˜„ë ¥ì€ ë›°ì–´ë‚˜ì§€ë§Œ computation overheadê°€ ë°œìƒí•œë‹¤.

    - ê³„ì‚°ì†ë„ê°€ ë” ë¹ ë¥¸ `unsigned int`ë¥¼ ì‚¬ìš©í•˜ëŠ” í¸ì´ ìœ ë¦¬í•˜ë‹¤.

---

### 5.2.3 Uniform vs Non-uniform Quantization

quantizationì˜ step sizeë¥¼ uniform(ê· ì¼)í•˜ê²Œ ì •í•˜ê±°ë‚˜, non-uniformí•˜ê²Œ ì •í•˜ëŠ”ê°€ì— ë”°ë¼ì„œë„ í‘œí˜„ë ¥ì´ ë‹¬ë¼ì§„ë‹¤.

![uniform vs non uniform](images/uniform_vs_non_uniform.png)

- **uniform quantization**: (a)

    - í‘œí˜„ë ¥ì€ non-uniformë³´ë‹¤ ë–¨ì–´ì§€ì§€ë§Œ êµ¬í˜„ì´ ë” ì‰½ë‹¤.

- **non-uniform quantization**: (b)

    - ë¶„í¬ì— ë”°ë¼ step sizeê°€ ê²°ì •ëœë‹¤. 
    
    - ì˜ˆì‹œ) **logarithmic quantization**: (c) 
    
      same storageì—ì„œ ë” ë„“ì€ ë²”ìœ„ì˜ ê°’ì˜ í‘œí˜„ì´ ê°€ëŠ¥í•˜ë‹¤.

$$ Q(x) = Sign(x)2^{round(\log_{2}|x|)} $$

---

## 5.3 Efficient Weights Quantization

ê·¸ë ‡ë‹¤ë©´ quantization bitsëŠ” ì–´ëŠ ì •ë„ê°€ íš¨ìœ¨ì ì¼ê¹Œ? CNNì˜ main operationì— í•´ë‹¹ë˜ëŠ” convolution(MAC), Fully-Connected(FC) layerë¥¼ quantizationí–ˆì„ ë•Œ ì •í™•ë„ë¥¼ ë‚˜íƒ€ë‚¸ ë„í‘œë¥¼ ì‚´í´ë³´ì.

![quantization bits](images/quantization_bits.png)

- Conv layer: 4bits ì´ìƒ

- FC layer: 2bits ì´ìƒ

ì°¸ê³ ë¡œ ëŒ€í‘œì ì¸ CNN ëª¨ë¸ì—ì„œ Conv, FC layerì´ ê°–ëŠ” ë¹„ì¤‘ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

![CNN models Conv, FC layers](images/conv_fc_cnn.png)

---

### 5.3.1 Huffman Coding

> [Huffman coding ì •ë¦¬](https://velog.io/@junhok82/%ED%97%88%ED%94%84%EB%A7%8C-%EC%BD%94%EB%94%A9Huffman-coding)

> Unixì˜ íŒŒì¼ ì••ì¶•, JPEG, MP3 ì••ì¶•ì—ì„œ ì£¼ë¡œ ì‚¬ìš©ëœë‹¤.

ì¶”ê°€ë¡œ **Huffman Coding** ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í•˜ë©´ memory usageë¥¼ ë” ì¤„ì¼ ìˆ˜ ìˆë‹¤.

- ìì£¼ ë‚˜ì˜¤ëŠ” weights: bit ìˆ˜ë¥¼ ì ê²Œ ì‚¬ìš©í•´ì„œ í‘œí˜„í•œë‹¤.

- ë“œë¬¸ weights: bit ìˆ˜ë¥¼ ë” ì‚¬ìš©í•´ì„œ í‘œí˜„í•œë‹¤.

ëŒ€í‘œì ìœ¼ë¡œ [Deep Compression ë…¼ë¬¸](https://arxiv.org/pdf/1510.00149.pdf)ì—ì„œëŠ” 'Pruning + K-Means-based quantization + Huffman Coding'ì„ ì ìš©í•˜ì—¬ LeNet-5 ëª¨ë¸ì—ì„œ ì•½ 39ë°° Compression ratioë¥¼ ë‹¬ì„±í–ˆë‹¤.

![Deep Compression](images/deep_compression.png)

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ 2: Huffman Coding &nbsp;&nbsp;&nbsp;</span>

A, B, C ì•ŒíŒŒë²³ì„ Huffman Codingì„ ì´ìš©í•´ ì••ì¶•í•˜ì—¬ í‘œí˜„í•˜ë¼.

### <span style='background-color: #C2B2B2; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ” í’€ì´&nbsp;&nbsp;&nbsp;</span>

ìˆœì „íˆ ASCII codeë¡œ í‘œí˜„í•˜ë ¤ê³  í•œë‹¤ë©´ 8bits x 3ìœ¼ë¡œ 24bitsë¥¼ ì‚¬ìš©í•´ì•¼ í•œë‹¤. í•˜ì§€ë§Œ Huffman codingì„ ì´ìš©í•´ ê°€ë³€ ê¸¸ì´ì˜ codeë¡œ ë§Œë“¤ ê²ƒì´ë‹¤.

ìš°ì„  a, b, cë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì••ì¶•í•˜ì—¬ ì •ì˜í–ˆë‹¤ê³  í•˜ì.

| a | b | c |
| :---: | :---: | :---: |
| 01 | 101 | 010 |

- aì™€ cì˜ ì ‘ë‘ì–´ ë¶€ë¶„ì´ ê²¹ì¹œë‹¤.(`01`)

ìœ„ì²˜ëŸ¼ ì‹œì‘ ë¶€ë¶„ì´ ê²¹ì¹˜ëŠ” ê²½ìš° **prefix code**(ì ‘ë‘ì–´ ì½”ë“œ) ë°©ì‹ìœ¼ë¡œ ê°€ë³€ ì½”ë“œë¥¼ ë§Œë“¤ ìˆ˜ ì—†ë‹¤. ë°˜ë©´ ì•„ë˜ ì˜ˆì‹œë¥¼ ë³´ì.

| a | b | c |
| :---: | :---: | :---: |
| 01 | 10 | 111 |

- ê²¹ì¹˜ëŠ” ì ‘ë‘ì–´ê°€ ì—†ë‹¤.

ì´ ê²½ìš° `01 10 111` ì´ 7bitsë¡œ ì••ì¶•í•  ìˆ˜ ìˆë‹¤.

ì—¬ê¸°ì„œ ìˆ«ìë¥¼ ê²°ì •ì§“ëŠ” ê²ƒì€ 'ë¬¸ìì˜ ë¹ˆë„ ìˆ˜'ì´ë‹¤. 

- ë¹ˆë„ ìˆ˜ê°€ ë†’ì€ ë¬¸ìì¼ìˆ˜ë¡ ì§§ì€ ê¸¸ì´ì˜ codeë¥¼ ë¶€ì—¬í•œë‹¤.

- ë¹ˆë„ ìˆ˜ê°€ ë‚®ì€ ë¬¸ìì¼ìˆ˜ë¡ ê¸´ ê¸¸ì´ì˜ codeë¥¼ ë¶€ì—¬í•œë‹¤.

---

## 5.4 Neural Network Quantization

ImageNet datasetìœ¼ë¡œ í›ˆë ¨í•œ AlexNetì—ì„œ pruning+quantization, pruning, quantization ë°©ë²•ë³„ 'accuracyì™€ compression ratio'ë¥¼ ë¹„êµí•´ ë³´ì.

![accuracy vs compression rate](images/acc_loss_and_model_compression.png)

- ê°€ë¡œ: Compression Ratio, ì„¸ë¡œ: Accuracy loss

- ë‘ ë°©ë²•ì„ ë™ì‹œì— ì ìš©í–ˆì„ ë•Œ accuracyì˜ ë³´ì¡´ìœ¨ì´ ë†’ë‹¤.

ì´ì œ neural network ë„ë©”ì¸ì—ì„œ ë‹¤ì–‘í•œ quantization ë°©ë²•ì„ ì‚´í´ë³´ì. ì•„ë˜ì™€ ê°™ì€ floating-point numberë¡œ êµ¬ì„±ëœ matrixë¥¼ quantizationí•œë‹¤ê³  ê°€ì •í•˜ì.

![floating-point matrix](images/floating-point_matrix.png)

- ì €ì¥: Floating-Point Weights

- ì—°ì‚°: Floating-Point Arithmetic

---

### 5.4.1 K-Means-based Quantization

> [Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding ë…¼ë¬¸(2015)](https://arxiv.org/abs/1510.00149)

> [Deep Compression ë…¼ë¬¸ ë¦¬ë·°](https://velog.io/@woojinn8/LightWeight-Deep-Learning-3.-Deep-Compression-%EB%A6%AC%EB%B7%B0)

**K-Means-based weight quantization**ì´ë€ ì—¬ëŸ¬ <U>bucketì„ ê°–ëŠ” codebook</U>(**centroids**, ë¬´ê²Œì¤‘ì‹¬)ì„ ë§Œë“¤ì–´ì„œ quantizationí•˜ëŠ” ë°©ì‹ì´ë‹¤.

> clustering ê¸°ë²• ìì²´ë¥¼ non-uniform quantizationì˜ ì¼ì¢…ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤.(quantization level ìˆ˜ = cluster ìˆ˜)

> ì˜ˆë¥¼ ë“¤ì–´ Computer Graphicsì—ì„œëŠ”, 65536ê°œì˜ ìŠ¤í™íŠ¸ëŸ¼ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ì›ë˜ ìƒ‰ìƒì„ 256ê°œì˜ bucketì„ ê°–ëŠ” codebookì„ ë§Œë“¤ì–´ì„œ quantizationì„ ìˆ˜í–‰í•œë‹¤.

![K-Means-based_Quantization](images/K-Means-based_Quantization.png)

- ì €ì¥: **Integer** Weights, Floating-Point Codebook

- ì—°ì‚°: Floating-Point Arithmetic

> ì˜ˆì œì—ì„œ codebookì˜ cluster indexëŠ” 0~3ê¹Œì§€ ìˆìœ¼ë¯€ë¡œ 2bitë¡œ í‘œí˜„ëœë‹¤.

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ 2: K-Means-based Quantizationì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ &nbsp;&nbsp;&nbsp;</span>

K-Means-based Quantization ì´ì „/ì´í›„ í•„ìš”í•œ memoryë¥¼ ê³„ì‚°í•˜ë¼.

### <span style='background-color: #C2B2B2; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ” í’€ì´&nbsp;&nbsp;&nbsp;</span>

- before

    32bits floating point type 4x4 í–‰ë ¬ì˜ weightë¥¼ ì €ì¥í•œë‹¤.
    
    $32 \times (4 \times 4) = 512$
    
    ë”°ë¼ì„œ ì´ 512bits = 64bytesì´ë‹¤.

- after

    í–‰ë ¬ ë‚´ ê°’ì€ 2bit cluster indexë¥¼ ê°–ëŠ”ë‹¤.

    $2 \times (4 \times 4) = 32$
    
    ë”°ë¼ì„œ í–‰ë ¬ì€ 32 bits = 4 bytesë¥¼ ê°–ëŠ”ë‹¤.

    ë˜í•œ codebookì€ 32bit floating pointë¡œ 1x4 í–‰ë ¬ì„ ê°–ëŠ”ë‹¤.

    $32 \times (1 \times 4) = 128$

    ë”°ë¼ì„œ codebookì€ 128bits = 16bytesë¥¼ ê°–ëŠ”ë‹¤.

    ê·¸ëŸ¬ë¯€ë¡œ quantization ì´í›„ í•„ìš”í•œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì€ 20byteì´ë‹¤.
    
ì–‘ìí™” ì „í›„ë¥¼ ë¹„êµí–ˆì„ ë•Œ, 64/20=3.2ë¡œ ì•½ 3.2ë°° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ê°ì†Œí–ˆë‹¤.

> ì˜ˆì‹œë³´ë‹¤ weightê°€ ë§ì€ í–‰ë ¬ì—ì„œ ë” í° íš¨ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆë‹¤.(ì•½ 32/Në°° ê°ì†Œí•œë‹¤.)

---

#### 5.4.1.1 K-Means-based Quantization Error

ìœ„ ì–‘ìí™” ì˜ˆì‹œì—ì„œ weightë¥¼ ë‹¤ì‹œ reconstructí•œ ë’¤, ê¸°ì¡´ê³¼ ë¹„êµí•˜ì—¬ errorë¥¼ ê³„ì‚°í•´ ë³´ì.

![K-Means error](images/K-Means_error.png)

ì´ì²˜ëŸ¼ quantization ì‹œ í•„ì—°ì ìœ¼ë¡œ errorê°€ ë°œìƒí•˜ê²Œ ëœë‹¤. í•˜ì§€ë§Œ ì¶”ê°€ë¡œ centroids(codebook)ì„ fine-tuningí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ errorë¥¼ ì¤„ì¼ ìˆ˜ ìˆë‹¤.

![Fine-tuning quantized weights(K-means)](images/K-means_fine_tune.png)

- ì´ë•Œ weightê°€ ì–´ë–¤ clusterì— ì†í•˜ëŠ”ì§€ì— ë”°ë¼ ë¶„ë¥˜í•œ ë’¤, í‰ê· ì¹˜ë¥¼ êµ¬í•˜ì—¬ centroidsë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤.

---

#### 5.4.1.2 K-Means-based Quantization Limitations

ê·¸ëŸ¬ë‚˜ K-Means-based weight quantizationì€ ë‹¤ìŒê³¼ ê°™ì€ ë‹¨ì ì„ ê°–ëŠ”ë‹¤.

1. ì‹¤ì œ ì¶”ë¡  ìƒí™©ì—ì„œëŠ” ë‹¤ì‹œ floating pointë¥¼ ì‚¬ìš©í•œë‹¤.

   ë‹¤ì‹œ ë§í•´ runtime inferenceì—ì„œ weightëŠ” lookup tableì„ ë°”íƒ•ìœ¼ë¡œ decompressedëœë‹¤.(ì˜ˆì‹œì—ì„œëŠ” 2bit integerê°€ ë‹¤ì‹œ 32bit floating pointë¡œ decompressedëë‹¤.)

    - ë”°ë¼ì„œ <U>ì˜¤ì§ storage costë§Œ ì¤„ì¼ ìˆ˜ ìˆë‹¤</U>ëŠ” í•œê³„ë¥¼ ì§€ë‹Œë‹¤.

2. codebookì„ reconstructioní•˜ê¸° ìœ„í•œ time complexity, computationì´ í¬ë‹¤.

3. clusterì— ìˆëŠ” weightê°€ memoryìƒì—ì„œ ì—°ì†ì ì´ì§€ ì•Šê¸° ë–„ë¬¸ì—, memory accessì—ì„œ ê¸´ ì§€ì—°ì´ ë°œìƒí•˜ê²Œ ëœë‹¤.

4. activationì€ ì…ë ¥ì— ë”°ë¼ ë‹¤ì–‘í•˜ê²Œ ë³€í•˜ê¸° ë•Œë¬¸ì—, actiavation quantizationì— clustering-based approachëŠ” ì í•©í•˜ì§€ ì•Šë‹¤.

---

### 5.4.2 Linear Quantization

ì´ë²ˆì—ëŠ” **Linear Quantization** ë°©ë²•ì„ ì‚´í´ë³´ì. ë§ˆì°¬ê°€ì§€ë¡œ linear quantizationë„ codebookì„ ì‚¬ìš©í•´ì„œ quantized weightsë¥¼ ë§Œë“¤ì–´ë‚¸ë‹¤.

í•˜ì§€ë§Œ ì´ë•Œ **centroids**ê°€ linearí•˜ë‹¤ëŠ” íŠ¹ì§•ì„ ê°–ëŠ”ë‹¤.(ì¼ì •í•œ step sizeë¥¼ ê°–ëŠ”ë‹¤.)

![uniform quantization](images/uniform_quantization.png)

ì˜ˆì‹œ weight í–‰ë ¬ì— linear quantizationì„ ì ìš©í•˜ëŠ” ê³¼ì •ì„ ì‚´í´ë³´ì.

![linear quantization](images/linear_quantization.png)

> zero point, scale ê³„ì‚°ë²•ì€ ë’¤ì—ì„œ ì‚´í•„ ê²ƒì´ë‹¤.

ìš°ì„  ìœ„ ê·¸ë¦¼ì˜ ë‚˜ì—´ ìˆœì„œëŒ€ë¡œ ìˆ˜ì‹ì„ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

> FP weight, quantized weight, zero point, scale

$$ r = (q - Z) \times S $$

- $r$ : (floating-point) real number

- $q$ : (**integer**) quantized number

- $Z$ : (**integer**) **zero point**

  real number $r=0$ ì— ì •í™•íˆ mappingë  ìˆ˜ ìˆë„ë¡ ì¡°ì ˆí•˜ëŠ” ì—­í• ì´ë‹¤. **offset**ìœ¼ë¡œë„ ì§€ì¹­í•œë‹¤.

- $S$ : (floating-point) **scaling factor**

ì´ë•Œ quantizationí•˜ëŠ” ë²”ìœ„ê°€ ìŒì˜ ì •ìˆ˜ë¥¼ í¬í•¨í•˜ëŠ”ê°€ì— ë”°ë¼ì„œ `unsigned int`, `signed int`ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ReLUì™€ ê°™ì´ ìŒìˆ˜ ê°’ì„ ì œê±°í•˜ëŠ” activation functionì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ì£¼ë¡œ `unsigned int`ë¥¼ ì‚¬ìš©í•œë‹¤.

---

#### 5.4.2.1 zero point, scaling factor

ì´ì œ real numberë¥¼ quantized numberì— mappingí•˜ë©´ì„œ, quantization parameterì¸ zero point, scaling factorì„ ê³„ì‚°í•´ ë³´ì.

ìˆ˜ì‹ì€ ê¸°ë³¸ì ìœ¼ë¡œ ìµœëŒ€, ìµœì†Œ ì‹¤ìˆ˜ê°’ì„ ê°€ì§€ê³  ê³„ì‚°í•œë‹¤.

> ì£¼ë¡œ outlier(ì´ìƒì¹˜)ë¥¼ ì œê±°(**clipping**)í•œ ë²”ìœ„ì˜ ìµœëŒ€, ìµœì†Œê°’ì„ ì‚¬ìš©í•œë‹¤.

![linear quantization mapping](images/linear_quantization_mapping.png)

$$ r_{max} = S(q_{max} - Z) $$

$$ r_{min} = S(q_{min} - Z) $$

ìœ„ ì‹ì„ ì •ë¦¬í•˜ë©´ scaling factorì— ëŒ€í•œ ì‹ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.

$$ S = {{r_{max} - r_{min}} \over {q_{max} - q_{min}}} $$

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ 3: linear quantization &nbsp;&nbsp;&nbsp;</span>

ì˜ˆì‹œ weight matrixì—ì„œ zero point, scaling factor ê°’ì„ êµ¬í•˜ì—¬ë¼.

![floating-point matrix](images/floating-point_matrix.png)

### <span style='background-color: #C2B2B2; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ” í’€ì´&nbsp;&nbsp;&nbsp;</span>

ìš°ì„  weight í–‰ë ¬ì—ì„œ FP ìµœëŒ€ê°’ì€ 2.12, FP ìµœì†Œê°’ì€ -1.08ì´ë‹¤. ë˜í•œ integerì€ ê°ê° 1ê³¼ -2ì— ëŒ€ì‘ëœë‹¤. ì´ë¥¼ ì‹ì— ëŒ€ì…í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ scaling factorë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤.

$$ S = {{2.12 - (-1.08)} \over {1 - (-2)}} = 1.07 $$

$S$ ë¥¼ êµ¬í–ˆìœ¼ë¯€ë¡œ ì•ì„œ $r_{min}$ í˜¹ì€ $r_{max}$ ê°’ì— ëŒ€ì…í•˜ëŠ” ê²ƒìœ¼ë¡œ $Z$ ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤. 

> ì´ë•Œ $Z$ ê°€ ì •ìˆ˜ê°€ ë˜ë„ë¡ round ì—°ì‚°ì„ ì ìš©í•´ì•¼ í•œë‹¤.

$$ Z = \mathrm{round}{\left( q_{min} - {{r_{min}} \over S} \right)} $$

ê°’ì„ ëŒ€ì…í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$ Z = \mathrm{round}{\left( -2 - {{-1.08} \over {1.07}} \right)} = 1 $$

ë”°ë¼ì„œ zero pointëŠ” 1ì´ë‹¤.

---

#### 5.4.2.2 Sources of quantization error

ì´ëŸ¬í•œ ì–‘ìí™” ê³¼ì •ì—ì„œ quantization errorë¥¼ ì¦ê°€ì‹œí‚¤ëŠ” ì£¼ë²”ì„ ì°¾ì•„ë³´ì.

![quant error example 1](images/quant_error_ex_1.png)

- round: ì •ìˆ˜ **ë°˜ì˜¬ë¦¼**

- clip: ì •í•´ë‘” **ë²”ìœ„ ì‚¬ì´ë¡œ ê°’ì„ ë§¤í•‘**(=CLAMP)

    ì˜ˆë¥¼ ë“¤ì–´ `int8`ì´ë©´ -128\~127 í˜¹ì€ 0\~255 ì‚¬ì´ë¡œ ê°’ì„ ë§¤í•‘í•œë‹¤.

ê·¸ë¦¼ì—ì„œ ì£¼ëª©í•  ì ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

- fp domainì—ì„œ <U>ë§¤ìš° ê°€ê¹Œìš´ ë‘ ê°’</U>ì€, ë™ì¼í•œ integer domainì˜ gridë¡œ ì¶•ì†Œëœë‹¤.

- $q_{max}$ ì´í›„ fp32 <U>outlier</U>ëŠ”, ëª¨ë‘ integer $2^{b} - 1$ ê°’ìœ¼ë¡œ ë§¤í•‘ëœë‹¤.

ì–‘ìí™” ê°’ì„ ë‹¤ì‹œ ë³µì›í•´ ë³´ì.

![quant error example 2](images/quant_error_ex_2.png)

- quantization errorëŠ” round, clip errorì˜ í•©ì´ë‹¤.

í•˜ì§€ë§Œ ìœ„ ì˜ˆì‹œì²˜ëŸ¼ í•­ìƒ clip errorê°€ round errorë³´ë‹¤ í° ê²ƒì€ ì•„ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ë‹¤ìŒê³¼ ê°™ì´ $q_{max}$ ê°’ì„ ëŠ˜ë ¸ë‹¤ë©´ round errorê°€ ë” ì»¤ì§€ê²Œ ëœë‹¤. ì´ëŸ¬í•œ **trade-off** ê´€ê³„ë¥¼ ê³ ë ¤í•˜ì—¬ quantization rangeë¥¼ ì •í•´ì•¼ í•˜ëŠ” ê²ƒì´ë‹¤.

![quant error example 3](images/quant_error_ex_3.png)

---

## 5.5 Linear Quantized Fully-Connected Layer

ì´ëŸ¬í•œ linear quantizationì„ í–‰ë ¬ ì—°ì‚° ê´€ì ì—ì„œ ì‹¤ìˆ˜ë¥¼ ì •ìˆ˜ë¡œ ë§¤í•‘í•˜ëŠ” **affine mapping**(ì•„í•€ë³€í™˜)ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤.

$$ r = S(q - Z) $$

> affine mappingì€ ê°„ë‹¨íˆ ë§í•´ linear transform(ì„ í˜• ë³€í™˜) í›„ translation(ì´ë™)ì„ ìˆ˜í–‰í•˜ëŠ” ë³€í™˜ì´ë‹¤. translationì´ ìˆê¸° ë•Œë¬¸ì— non-linear transformì´ë‹¤. 

fully-connected layerì—ì„œ linear quantizationì„ ì ìš©í•˜ë©´ ì•„ë˜ì™€ ê°™ì´ ìˆ˜ì‹ì´ ë°”ë€Œê²Œ ëœë‹¤.(bias $b$ ëŠ” ë¬´ì‹œí•œë‹¤.)

$$ Y = WX + b $$

$$ \downarrow $$

$$ S_{Y}(q_{Y} - Z_{Y}) =  S_{W}(q_{W} - Z_{W}) \cdot S_{X}(q_{X} - Z_{X}) + S_b(q_b - Z_b) $$

$$ \downarrow Z_w = 0 $$

$$ S_{Y}(q_{Y} - Z_{Y}) =  S_{W}S_{X}(q_{W}q_{X} - Z_{X}q_{W})  + S_b(q_b - Z_b) $$

$$ \downarrow Z_b = 0, \, S_b = S_W S_X $$

$$ S_{Y}(q_{Y} - Z_{Y}) =  S_{W}S_{X}(q_{W}q_{X} - Z_{X}q_{W}+ q_b) $$

- bias, weightì˜ zero pointë¥¼ ëª¨ë‘ 0ìœ¼ë¡œ ë‘ì–´ ë‹¨ìˆœí™”(symmetric)

- scaling factorëŠ” weight, bias ë™ì¼( $S_b = S_W S_X$ )

ì´ë¥¼ $q_{Y}$ ì— ê´€í•œ ì‹ìœ¼ë¡œ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.


$$ q_{Y} = {{S_{W}S_{X}} \over {S_{Y}}}(q_{W}q_{X} + q_b - Z_{W}q_{X}) + Z_{Y} $$

- precomputeí•­

    ì•„ë˜ precomputeí•­ì„ $q_{bias}$ ë¡œ ì¹˜í™˜í•  ìˆ˜ ìˆë‹¤.

$$ q_b - Z_X q_W $$

precomputeí•­ì„ ì¹˜í™˜ ì‹œ ì‹ì€ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ëœë‹¤.

$$ q_{Y} = {{S_{W}S_{X}} \over {S_{Y}}}(q_{W}q_{X} + q_{bias}) + Z_{Y} $$

í•­ë³„ë¡œ ì—°ì‚°ì´ ì–´ë–»ê²Œ ì§„í–‰ë˜ëŠ”ì§€ ë³´ì.

- Rescale to N-bit int

$${S_{W}S_{X}} \over {S_{Y}}$$

-  N-bit Integer multiplication. 32-bit Integer Addition

$$q_{W}q_{X} + q_{bias}$$

- N-bit Integer addition

$$Z_{Y}$$

> Note: $q_b$ , $q_{bias}$ ëª¨ë‘ 32 bitsì´ë‹¤.

---

## 5.6 Linear Quantized Convolution Layer

$$ Y = \mathrm{Conv} (W, X) + b $$

$$ \downarrow $$

$$ q_{Y} = {{S_{W}S_{X}} \over {S_{Y}}}(\mathrm{Conv}(q_{W}, q_{X}) + q_{bias}) + Z_{Y} $$

- Rescale to N-bit int

$${S_{W}S_{X}} \over {S_{Y}}$$

-  N-bit Integer multiplication. 32-bit Integer Addition

$$\mathrm{Conv}(q_{W}q_{X}) + q_{bias}$$

- N-bit Integer addition

$$Z_{Y}$$

> Note: $q_b$ , $q_{bias}$ ëª¨ë‘ 32 bitsì´ë‹¤.

ì´ëŸ¬í•œ on-device fixed-point inference ê³„ì‚°ì€ ë‹¤ìŒê³¼ ê°™ì€ ê³¼ì •ì„ ë”°ë¼ ì§„í–‰ëœë‹¤.

![CNN quantized computation graph](images/cnn_quantized_computational_graph.png)

---

## 5.7 Simulated Quantization

í•˜ì§€ë§Œ fixed-point operationì„ ë¯¸ë¦¬ general purpose hardware(ì˜ˆ: CPU, GPU)ë¡œ ì‹œë®¬ë ˆì´ì…˜í•  ìˆ˜ ìˆë‹¤ë©´ ë‹¤ì–‘í•œ quantization schemeì„ ì‹¤í—˜í•´ ë³¼ ìˆ˜ ìˆë‹¤.

> GPU ê°€ì†ì„ ì´ìš©í•´ ë‹¤ì–‘í•œ ì¡°ê±´ì˜ ì–‘ìí™”ë¥¼ ê²€ì¦í•  ìˆ˜ ìˆë‹¤.

ì´ëŸ¬í•œ ì‹œë®¬ë ˆì´ì…˜ì´ ê°€ëŠ¥í•˜ê²Œë” ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ì—ì„œ quantization operations(**quantizer**)ë¥¼ ì œê³µí•˜ê³  ìˆë‹¤.

![simulated quantization](images/simulated_quantization.png)

- ì¢Œì¸¡: fixed-point operationsì„ ì´ìš©í•œ quantized on-device inference

- ìš°ì¸¡: floating-point operationsì„ ì´ìš©í•œ **simulated quantization**

---