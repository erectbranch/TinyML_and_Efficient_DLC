# 10 Knowledge Distillation

> [Lecture 10 - Knowledge Distillation | MIT 6.S965](https://youtu.be/IIqf-oUTHe0)

**knowledge transfer**ì´ë€ ë³µì¡í•œ ëª¨ë¸(cloud model)ì„ ì´ìš©í•´ ë‹¨ìˆœí•œ ëª¨ë¸(edge model)ì„ í›ˆë ¨ì‹œí‚´ìœ¼ë¡œì¨, <U>generalizationê³¼ accuracy ì„±ëŠ¥ì´ ë³µì¡í•œ ëª¨ë¸ì— ê·¼ì ‘í•œ ë‹¨ìˆœí•œ ëª¨ë¸ì„ ì–»ì–´ë‚´ëŠ” ë°©ë²•</U>ì´ë‹¤.

![challenge](images/challenge.png)

cloud modelê³¼ tiny modelì˜ training curve ì°¨ì´ë¥¼ ë³´ì.

![ResNet50, MobileNetV2](images/ResNet50_vs_MobileNetV2.png)

> ê°€ë¡œ: epoch, ì„¸ë¡œ: accuracy

- ResNet50(cloud model)
 
  training accuracyê°€ 80%ë¥¼ ë„˜ëŠ”ë‹¤.

- edge model(MobileNetV2-Tiny)

  training accuracyê°€ 50% ì •ë„ì— ê°€ê¹ë‹¤. 
  
  edge modelì€ ì‘ì€ capacityë¥¼ ê°–ëŠ” ë§Œí¼ ë†’ì€ ì •í™•ë„ë¥¼ ì–»ê¸° í˜ë“¤ë‹¤.

ë˜í•œ ì‘ì€ capacityë¥¼ ê°–ê¸° ë•Œë¬¸ì—, edge modelì—ì„œ overfittingì„ ë°©ì§€í•˜ê¸° ìœ„í•œ ëª‡ ê°€ì§€ í•™ìŠµ ê¸°ë²•ì€ ì˜¤íˆë ¤ ì—­íš¨ê³¼ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë‹¤.

- **data augmentation**(ë°ì´í„° ì¦ê°•), **dropout**(ë“œë¡­ì•„ì›ƒ) ë“±

> data augmentationë€ ê¸°ì¡´ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ë°ì´í„°ë¥¼ ëŠ˜ë¦¬ëŠ” ë°©ë²•ì´ë‹¤. mirroring, random cropping, rotation, shearing, local wrapping ë“±

---

## 10.1 Knowledge Distillation

> [Distilling the Knowledge in a Neural Network ë…¼ë¬¸(2015)](https://arxiv.org/abs/1503.02531)

> [distiller documentation: knowledge distillation](https://intellabs.github.io/distiller/knowledge_distillation.html)

**Knowledge Distillation**(KD, ì§€ì‹ ì¦ë¥˜)ë€ ì»¤ë‹¤ë€ teacher networkë¥¼ ì´ìš©í•œ í•™ìŠµì„ í†µí•´, teacherê°€ ê°–ê³  ìˆëŠ” knowledgeë¥¼ student networkë¡œ ì „ë‹¬í•˜ëŠ” ë°©ë²•ì´ë‹¤.

![knowledge distillation](images/knowledge_distillation.png)

- Input (ì˜ˆ: ì´ë¯¸ì§€, ìì—°ì–´, ìŒì„±)

  teacher networkì™€ student network ì–‘ìª½ì— ì „ë‹¬ëœë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°ê° predication logitsì„ ì‚°ì¶œí•œë‹¤.

- Classification Loss

  ì¼ë°˜ì ìœ¼ë¡œ ì‚°ì¶œí•˜ëŠ” classification loss.
  
  ì¦‰, student's predicted class probabilitiesì™€ ground-truth labels ê°„ì˜ standard lossì´ë‹¤.
  
  > ground-truth: ì‹¤í—˜ìê°€ ì •í•œ 'ì •ë‹µ'ì´ë‹¤.(modelì´ ë‹µìœ¼ë¡œ ë‚´ë†“ê¸°ë¥¼ ì›í•˜ëŠ” class). hard labels/targetsë¡œ êµ¬ë¶„í•´ì„œ ë¶€ë¥´ê¸°ë„ í•œë‹¤.

- **Distillation Loss**

  teacher modelê³¼ student model ê°ê° ì˜ˆì¸¡í•œ logitsë¥¼ ë°”íƒ•ìœ¼ë¡œ **distillation loss**ë¥¼ ì‚°ì •í•œë‹¤.
  
  ëŒ€í‘œì ìœ¼ë¡œ cross-entropy lossë‚˜ L2 lossë¥¼ ë°”íƒ•ìœ¼ë¡œ ê³„ì‚°í•œë‹¤.

  - cross-entropy loss: $E(-p_{t}\log{p_{s}})$ 

  - L2 loss: $E({|| p_{t} - p_{s} ||}_{2}^{2})$

---

### 10.1.1 intuition of knowledge distillation

ì•„ë˜ëŠ” ê°œ ê³ ì–‘ì´ì˜ binary classification ë¬¸ì œì—ì„œ ê³ ì–‘ì´ ê·¸ë¦¼ì„ ì…ë ¥ìœ¼ë¡œ ì£¼ì—ˆì„ ë•Œ, teacher modelê³¼ student model ê°ê°ì˜ prediction ê²°ê³¼ì„ ë‚˜íƒ€ë‚¸ë‹¤.

![knowledge distillation ex 1](images/knowledge_distillation_ex_1.png)

- Teacher

  - cat: logits 5

  - dog: logits 1
  
  - softmaxë¥¼ ì ìš©í•˜ë©´ catì˜ probabilityëŠ” 0.982ì´ë‹¤.

$${{e^5} \over {e^5 + e^1}} = 0.982$$

- Student

  - cat: logits 3, 
  
  - dog: logits 2
  
  - softmaxë¥¼ ì ìš©í•˜ë©´ catì˜ probabilityëŠ” 0.731ì´ë‹¤.

$${{e^3} \over {e^3 + e^2}} = 0.731$$

ê²°ê³¼ë¥¼ ë³´ë©´ student modelì€ ì˜ˆì¸¡ì€ ë§ì•˜ì§€ë§Œ, teacherì— ë¹„í•˜ë©´ input imageê°€ catì´ë¼ëŠ” confidenceê°€ ë¶€ì¡±í•˜ë‹¤. ì´ë¥¼ teacher modelì˜ informationì„ ë°›ì•„ í•´ê²°í•  ê²ƒì´ë‹¤.

> teacher modelì´ ê°–ëŠ” informationì„ ë…¼ë¬¸ì—ì„œëŠ” **dark knowledge**ë¼ê³  ì§€ì¹­í•œë‹¤.

---

### 10.1.2 Softmax Temperatrue

í•˜ì§€ë§Œ teacher modelì˜ confidenceê°€ ë†’ì„ìˆ˜ë¡, ì •ë‹µì´ ì•„ë‹Œ ë‹¤ë¥¸ classì— í•´ë‹¹ë  í™•ë¥ ì— ëŒ€í•œ informationê°€ 0ì— ë§¤ìš° ê°€ê¹ê²Œ ëœë‹¤. ë”°ë¼ì„œ ë‹¤ë¥¸ classì— í•´ë‹¹ë  informationì„ ë³´ì¡´í•˜ê¸° ìœ„í•´ **temperature**ë¼ëŠ” ê°œë…ì´ ë“±ì¥í•œë‹¤.

$$ p(z_i, T) = {{\exp({z_{i} \over T})} \over {\sum_{j}{\exp({z_{j} \over T})}}} $$

- $z_{i}$: logits

- $i, j = 0, 1, 2, ..., C - 1$

  - $C$ : \#classes

- $T$ : temperature

  - $T$ ê°€ í´ìˆ˜ë¡ distributionì´ softí•˜ê²Œ ë³€í•œë‹¤.

  - $T=1$ ì¼ ë•Œ standard softmaxì´ë‹¤.

teacher modelê³¼ student modelì˜ ëª¨ë¸ ì‚¬ì´ì¦ˆ ì°¨ì´ê°€ í¬ë‹¤ë©´, ëŒ€ì²´ë¡œ ë‚®ì€ temperatureê°€ ë” íš¨ìœ¨ì ì´ë‹¤. ë§¤ìš° ì‘ì€ modelì´ teacher modelì˜ ëª¨ë“  informationì„ captureí•˜ì§€ ëª»í•˜ê¸° ë•Œë¬¸ì´ë‹¤.


### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ 1: Softmax Temperature &nbsp;&nbsp;&nbsp;</span>

10.1.1ì ˆ ì˜ˆì‹œì—ì„œ Teacherê°€ ì…ë ¥ì„ catìœ¼ë¡œ ì˜ˆì¸¡í•œ í™•ë¥ ì„, softmax temperatureë¥¼ ì´ìš©í•˜ì—¬ $T=1 , T=10$ ì¼ ë•Œë¥¼ ê°ê° êµ¬í•˜ë¼.

### <span style='background-color: #C2B2B2; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ” í’€ì´&nbsp;&nbsp;&nbsp;</span>

![knowledge distillation ex 2](images/knowledge_distillation_ex_2.png)

- $T = 1$

  **standard softmax**ì´ë‹¤. ê³ ì–‘ì´ì¼ í™•ë¥ ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

$${{e^{5 \over 1}} \over {e^{5 \over 1} + e^{1 \over 1}}} = 0.982$$

- $T = 10$

  ê³ ì–‘ì´ì¼ í™•ë¥ ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

$${{e^{5 \over 10}} \over {e^{5 \over 10} + e^{1 \over 10}}} = 0.599$$

---

## 10.2 What to match?

10.1ì ˆì˜ KDëŠ” teacher, student ì‚¬ì´ì˜ output logitsë¥¼ matchí•˜ëŠ” ê²ƒìœ¼ë¡œ, studentì˜ less confidence ë¬¸ì œë¥¼ í•´ê²°í–ˆë‹¤. í•˜ì§€ë§Œ logitsì´ ì•„ë‹Œ ë‹¤ë¥¸ ì¢…ë¥˜ì˜ ì •ë³´ë„ teacherì™€ student ì‚¬ì´ì—ì„œ matchì‹œí‚¬ ìˆ˜ ìˆë‹¤.

---

### 10.2.1 Matching intermediate weights

> [FitNets: Hints for Thin Deep Nets ë…¼ë¬¸(2015)](https://arxiv.org/abs/1412.6550)

intermediate weightsë¥¼ matchí•˜ëŠ” ë°©ë²•ì„ ì‚´í´ë³´ì. teacher, studentì˜ ê° ë ˆì´ì–´ê°€ ì„œë¡œ matchëœë‹¤.

![matching intermediate weights](images/matching_intermediate_weights_1.png)

ëŒ€í‘œì ìœ¼ë¡œ FitNets ë…¼ë¬¸ì—ì„œëŠ” teacher modelì™€ ì´ë³´ë‹¤ ë” ê¹Šê³  í­ì´ ì¢ì€ student model(FitNet) ì‚¬ì´ì—ì„œ intermediate weightsë¥¼ matchì‹œí‚¨ë‹¤.

![matching intermediate weights 2](images/matching_intermediate_weights_2.png)

> ë…¼ë¬¸ì—ì„œëŠ” intermediate hidden layersë¥¼ ì¤„ì—¬ hintsë¡œ ì§€ì¹­í•œë‹¤.

- teacher modelì´ ë” ë„“ì€ ëª¨ë¸ì´ê¸° ë•Œë¬¸ì—, studentë³´ë‹¤ ë” ë§ì€ outputì„ ê°–ëŠ”ë‹¤.

  ì´ëŸ¬í•œ teacher, student shape ì°¨ì´ë¥¼ ë³´ì •í•˜ê¸° ìœ„í•œ regressorë¥¼ ì¶”ê°€í•œë‹¤.(FC layerë¡œ êµ¬í˜„. layer weight $W_r$ ë„ í•¨ê»˜ í•™ìŠµëœë‹¤.)

  ![matching intermediate weights 3](images/matching_intermediate_weights_3.png)

- ì´í›„ teacher, student weights ì‚¬ì´ì—ì„œ L2 lossë¥¼ ì‚°ì¶œí•œë‹¤.

---

### 10.2.2 Matching intermediate features

> [Like What You Like: Knowledge Distill via Neuron Selectivity Transfer ë…¼ë¬¸(2017)](https://arxiv.org/abs/1707.01219)

teacher modelê³¼ student modelì€ ì„œë¡œ ë¹„ìŠ·í•œ feature distributionì„ ê°€ì ¸ì•¼ í•  ê²ƒì´ë¼ëŠ” ì§ê´€ì— ë”°ë¥¸ ë°©ë²•ì´ë‹¤.

---

#### 10.2.2.1 Minimizing Maximum Mean Discrepancy

Like What You Like ë…¼ë¬¸ì—ì„œëŠ” loss functionìœ¼ë¡œ **MMD**(Maximum Mean Discrepancy. ìµœëŒ€ í‰ê·  ë¶ˆì¼ì¹˜)ë¥¼ ì‚¬ìš©í•˜ì—¬ teacher, student feature map ì‚¬ì´ì˜ discrepancyë¥¼ ì‚¬ìš©í•œë‹¤.

![matching intermediate features](images/matching_intermediate_features.png)

- after matching: teacher modelê³¼ student modelì˜ feature distributionì´ ë¹„ìŠ·í•´ì§„ë‹¤.

ì´ë•Œ MMDë€, teacherì™€ studentì˜ feature map distributionì„ **Reproducing Kernel Hilbert Space**(RKHS)ë¡œ mappingí•œ ë’¤, ë‘˜ì˜ distanceë¥¼ ë°”íƒ•ìœ¼ë¡œ discrepancyë¥¼ ì¸¡ì •í•˜ëŠ” ë°©ë²•ì´ë‹¤.

![MMD](images/MMD.png)

---

#### 10.2.2.2 Minimizing the L2 distance

> [Paraphrasing Complex Network: Network Compression via Factor Transfer ë…¼ë¬¸(2018)](https://arxiv.org/abs/1802.04977)

> [NAVER Engineeraing: paraphrasing complex network ì„¸ë¯¸ë‚˜](https://tv.naver.com/v/5440966)

í˜¹ì€ feature maps ì‚¬ì´ì˜ L2 distanceë¥¼ ê³„ì‚°í•˜ëŠ” ì ‘ê·¼ë²•ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

![paraphrasing complex network](images/paraphrasing_complex_network.png)

- **Paraphraser**

  teacher network ë§ˆì§€ë§‰ feature mapì— ì¶”ê°€ë˜ëŠ” convolution node.

  ê¸°ì¡´ output dimension(channel) $m$ ì„, $k$ (ì¼ë°˜ì ìœ¼ë¡œ 0.5)ë¥¼ ê³±í•œ $m \times k$ ë¡œ ì¤„ì¸ë‹¤. ì´ ê³¼ì •ì—ì„œ ì¢‹ì€ feature mapì„ ì¶”ì¶œí•œë‹¤.

  > ParaphraserëŠ” ì„ ìƒë‹˜ì˜ ì…ì¥ì—ì„œ í•™ìƒì—ê²Œ ì‰½ê²Œ ì„¤ëª…í•˜ëŠ” ê²ƒìœ¼ë¡œ ë¹„ìœ í•œë‹¤.

  - ì›ë˜ $m$ ì°¨ì› outputê³¼ì˜ reconstruction lossë¥¼ ë°”íƒ•ìœ¼ë¡œ superviseëœë‹¤.

  - Translatorì™€ ì°¨ì´ë¥¼ êµ¬í•œ ë’¤ ë‹¤ì‹œ $m$ ì°¨ì›ìœ¼ë¡œ ë³µì›í•œë‹¤.

- **Translator**

  student network ë§ˆì§€ë§‰ feature mapì— ì¶”ê°€ë˜ëŠ” 1 layer MLP node.

  ë§ˆì°¬ê°€ì§€ë¡œ $m \times k$ ì°¨ì›ìœ¼ë¡œ ì¤„ì–´ë“ ë‹¤.(factorë¥¼ ì–»ê¸° ìœ„í•¨)

  > TranslatorëŠ” í•™ìƒ ì…ì¥ì—ì„œ ì„ ìƒë‹˜ì˜ ë§ì„ ì´í•´í•˜ëŠ” ê²ƒìœ¼ë¡œ ë¹„ìœ í•œë‹¤.

Paraphraserì™€ Translator ì‚¬ì´ì˜ factor(L1 loss)ë¥¼ ì¤„ì—¬ë‚˜ê°€ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµì„ ì§„í–‰í•œë‹¤.

> ë…¼ë¬¸ì—ì„œëŠ” Factor Transfer(FT)ë¼ëŠ” ëª…ì¹­ì„ ì“´ë‹¤. autoencoderì™€ ë¹„ìŠ·í•˜ê²Œ ë³¼ ìˆ˜ ìˆë‹¤.

---

### 10.2.3 Matching intermediate attention maps

> [Paying More Attention to Attention ë…¼ë¬¸(2017)](https://arxiv.org/abs/1612.03928)

feature mapsì˜ gradientëŠ” **attention map**ìœ¼ë¡œ í•´ì„í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ attention mapì„ ì´ìš©í•˜ë©´ teacherì™€ studentì˜ gradientë¥¼ matchí•  ìˆ˜ ìˆë‹¤.

![attention transfer](images/attention_transfer.png)

- 3D grid activation tensor

$$A \in R^{C \times H \times W}$$

- 2D attention map

  ![attention mapping](images/attention_mapping.png)

  3D grid activation tensorë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„, mapping functionì„ ê±°ì³ spatial activation mapì„ ì¶œë ¥í•œë‹¤.

$$ \mathcal{F} : R^{C \times H \times W} \rightarrow R^{H \times W} $$

- CNN feature map $x$ ì˜ attentionì„ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

  - $L$ : learning objective

$$ {{\partial L} \over {\partial x_{i,j}}} $$

- ë§Œì•½ position $i, j$ ì—ì„œì˜ attentionì´ í¬ë‹¤ë©´, ìê·¸ë§Œí•œ $x_{i, j}$ ë³€í™”ë„ final outputì— í° ì˜í–¥ì„ ë¯¸ì¹  ê²ƒì´ë‹¤. 

attentionì„ ë°”íƒ•ìœ¼ë¡œ í•œ transfer objectiveëŠ” ë‹¤ìŒê³¼ ê°™ì€ ìˆ˜ì‹ìœ¼ë¡œ ì •ì˜ëœë‹¤.

![matching intermediate attention map](images/matching_intermediate_attention_map.png)

```math
{{\beta} \over {2}} || J_S - J_T||_{2}^{2}
```

- $J_S$ : student attention map

- $J_T$ : teacher attention map

---

#### 10.2.3.1 Spatial Attention Mapping Function

ë…¼ë¬¸ì—ì„œëŠ” spatial attention mapping functionìœ¼ë¡œ ì•„ë˜ 3ê°€ì§€ ë°©ë²•ì„ ì†Œê°œí•˜ê³  ìˆë‹¤.

- ì±„ë„ë³„ ì ˆëŒ“ê°’ì˜ í•©

```math
F_{sum}(A) = {\sum}_{i=1}^{C}{|A_{i}|}
```

- ì±„ë„ë³„ ì ˆëŒ“ê°’ì˜ p ê±°ë“­ì œê³± í•©( ì´ë•Œ $p > 1$ )

```math
F_{sum}^{p}(A) = {\sum}_{i=1}^{C}{|A_{i}|}^{p}
```

- ì±„ë„ë³„ ì ˆëŒ“ê°’ì˜ p ê±°ë“­ì œê³± ê°’ ì¤‘ ìµœëŒ“ê°’:( ì´ë•Œ $p > 1$ )

```math
F_{max}^{p}(A) = \max_{i=1,c}{|A_{i}|}^{p}
```

ì°¸ê³ ë¡œ ë™ì¼í•œ ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í–ˆì„ ë•Œ, high accuracy model(perfomant model)ë“¤ì˜ activation mapì€ ë¹„ìŠ·í•œ ê²½í–¥ì„ ë³´ì¸ë‹¤.

![attention map comparison](images/attention_maps_compare.png)

- accuracyê°€ ë†’ì€ ResNet34ì™€ ResNet101ì€ ë¹„ìŠ·í•œ activation mapì„ ê°–ëŠ”ë‹¤.

---

### 10.2.4 Matching sparsity pattern

> [Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons ë…¼ë¬¸(2019)](https://arxiv.org/abs/1811.03233)

í•œí¸, **sparsity** patternì„ matchì‹œí‚¬ ìˆ˜ë„ ìˆë‹¤. teacher modelê³¼ student model ëª¨ë‘ ReLU activationì„ ê±°ì¹˜ë©´ì„œ ë¹„ìŠ·í•œ sparsity patternì„ ê°€ì ¸ì•¼ í•œë‹¤ëŠ” ì§ê´€ì—ì„œ ì¶œë°œí•œë‹¤.

- sparsity patternì€ **indicator function**ì„ ì‚¬ìš©í•´ì„œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.

$$ \rho(x) = 1[x > 0] $$

- ë”°ë¼ì„œ loss functionì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆë‹¤.

$$ \mathcal{L}(I) = {|| \rho({{T}(I)}) - \rho({{S}(I)}) ||}_{1} $$

ë‹¤ìŒì€ teacher modelì˜ **activation boundary**ë¥¼ ì‹œê°í™”í•œ ê·¸ë¦¼ì´ë‹¤.

![matching sparsity pattern](images/sparsity_pattern.png)

- íŒŒë€ìƒ‰ì¼ìˆ˜ë¡ response strengthê°€ ë†’ë‹¤.

- classification ì„±ëŠ¥ì„ ì¢Œìš°í•˜ëŠ” **decision boundary**ëŠ”, activation boundaryì™€ í° ì—°ê´€ì„±ì„ ê°–ëŠ”ë‹¤.

---

### 10.2.5 Matching relational information

---

#### 10.2.5.1 Relations between different layers

> [A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning ë…¼ë¬¸(2017)](https://openaccess.thecvf.com/content_cvpr_2017/papers/Yim_A_Gift_From_CVPR_2017_paper.pdf)

![relational information](images/relational_information.png)

- teacher, student ì‚¬ì´ \#layersëŠ” ë‹¤ë¥´ì§€ë§Œ \#channelsëŠ” ê°™ë‹¤.

- relational informationìœ¼ë¡œëŠ” $C_{in}$ , $C_{out}$ ì˜ ë‚´ì ì„ ì‚¬ìš©í•œë‹¤.

  > ì¦‰, spatial dimensions ì •ë³´ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.

$$ C_{in} \times C_{out} $$

---

#### 10.2.5.2 Relations between different samples

> [Relational Knowledge Distillation ë…¼ë¬¸(2019)](https://arxiv.org/abs/1904.05068)

ê¸°ì¡´ KDëŠ” ì˜¤ì§ 1ê°œì˜ inputì—ì„œ features, logits ë“±ì„ matchingí–ˆìœ¼ë‚˜, **Relational Knowledge Distillation**(RKD)ëŠ” multiple inputsì—ì„œì˜ intermediate features ì‚¬ì´ ê´€ê³„ë¥¼ ë¶„ì„í•œë‹¤.

![conventional vs relational KD](images/Conventional_vs_Relational_KD.png)

ê°€ë ¹ $n$ ê°œì˜ sampleì´ ìˆë‹¤ê³  í•  ë•Œ, relationì€ ë‹¤ìŒê³¼ ê°™ì€ ìˆ˜ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.

![relational knowledge distillation](images/Relational_KD.png)

$$ \Psi (s_1, s_2, ..., s_n) = (||s_1 - s_2||_{2}^{2}, ||s_1 - s_3||_{2}^{2}, ..., ||s_1 - s_n||_{2}^{2}, ..., ||s_{n-1} - s_n||_{2}^{2} )  $$

---