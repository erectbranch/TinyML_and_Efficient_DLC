# Lecture 02 - Basics of Neural Networks

> [Lecture 02 - Basics of Neural Networks | MIT 6.S965](https://youtu.be/5HpLyZd1h0Q)

---

## 2.5 Efficiency Metrics

ë³´í†µ networkë¥¼ ì„¤ê³„í•  ë•Œ, í¬ê²Œ ì„¸ ê°€ì§€ ìš”ì†Œë¥¼ ê³ ë ¤í•œë‹¤.

- latency

- storage

- energy

ê·¸ë ‡ë‹¤ë©´ ë‹¤ë¥¸ network ì‚¬ì´ì—ì„œ **efficiency**(íš¨ìœ¨ì„±)ì„ ë¹„êµí•  ë•Œ ì–´ë–¤ ì§€í‘œë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„êµí•´ì•¼ í• ê¹Œ?

![metrics](images/metrics.png)

> **storage**ê°€ weightë§Œ ì˜ë¯¸í•œë‹¤ë©´, **memory**ëŠ” ì¶”ê°€ë¡œ activationê¹Œì§€ ê³ ë ¤í•œë‹¤.

- **Memory-Related**

  - \#parameters

  - model size

  - total/peak \#activations

- **computation**

  - MACs

  - FLOP

---

### 2.5.1 Latency

Neural Network ì¶”ë¡ ì— ìˆì–´ì„œ, **latency**(ì§€ì—°ì‹œê°„)ëŠ” NN ìì²´ì˜ íŠ¹ì„±ê³¼ hardware íŠ¹ì„±ì— ëª¨ë‘ ì˜í–¥ì„ ë°›ëŠ”ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ pipeliningì„ ì´ìš©í•˜ë©´ computationê³¼ data movementëŠ” ë™ì‹œì— ì´ë£¨ì–´ì§ˆ ìˆ˜ ìˆë‹¤.

![latency](images/latency.png)

ë³‘ë ¬í™”ê°€ ê°€ëŠ¥í•œ ìì›ë§Œ ì¶©ë¶„í•˜ë‹¤ë©´ latencyëŠ” ë‹¤ìŒ ìˆ˜ì‹ìœ¼ë¡œ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.

$$ Latency \approx \max(T_{computation}, T_{memory}) $$

- $T_{computation}$

  ë¶„ëª¨ëŠ” hardware íŠ¹ì„±, ë¶„ìëŠ” NN íŠ¹ì„±ì´ë‹¤.

$$ \approx {{Number \ of \ Operations \ of \ NN} \over {Number \ of \ Opeartions \ that \ Processor \ can \ Process \ Per \ Second}} $$

- $T_{memory}$

$$ \approx T_{data \ movement \ of \ activations} + T_{data \ movement \ of \ weights} $$

- $T_{data \ movement \ of \ weights}$ 

  ë¶„ëª¨ëŠ” hardware íŠ¹ì„±, ë¶„ìëŠ” NN íŠ¹ì„±ì´ë‹¤.

  > weightë¥¼ SRAMì— ëª¨ë‘ ì €ì¥í•˜ë©´, main memoryì— ì ‘ê·¼í•˜ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤.

$$ \approx {{Model \ size} \over {Memory \ Bandwidth \ of \ Processor}} $$

- $T_{data \ movement \ of \ activations}$ 

$$ \approx {{Input \ activation \ size + Output \ activation \ size} \over {Memory \ Bandwidth \ of \ Processor}} $$

---

### 2.5.2 Energy Consumption

![Energy Consumption](images/energy_consumption.png)

(ìƒëµ)

---

### 2.5.3 Number of Parameters (\#Parameters)

\#ParametersëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì´ weight ê°œìˆ˜ë¥¼ ì˜ë¯¸í•œë‹¤. ì—¬ëŸ¬ ë ˆì´ì–´ ì¢…ë¥˜ì— ë”°ë¥¸ \#Parametersë¥¼ êµ¬í•´ë³´ì.(biasëŠ” ë¬´ì‹œ)

- Linear Layer

  $W^{T}$ : ì…ë ¥ ì±„ë„ ìˆ˜( $c_i$ )ì™€ ì¶œë ¥ ì±„ë„ ìˆ˜( $c_o$ )ë¥¼ ê³±í•˜ë©´ ëœë‹¤.

  ![linear layer](https://github.com/erectbranch/TinyML_and_Efficient_DLC/blob/master/lec02/summary02/images/linear.png)

$$ c_o \cdot c_i $$

- Convolution

  ![filters](images/filters.png)

$$ c_o \cdot c_i \cdot k_h \cdot k_w $$

- Grouped Convolution

  ![grouped convolution](images/grouped_conv.png)

$$ c_o/g \cdot c_i/g \cdot k_h \cdot k_w \cdot g  $$

$$ = c_o \cdot c_i \cdot k_h \cdot k_w / g $$

- Depthwise Convolution

  ![depthwise convolution](images/depthwise_conv.png)

$$ = c_o \cdot k_h \cdot k_w $$

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ 1: AlexNet \#Parameters &nbsp;&nbsp;&nbsp;</span>

AlexNetì˜ \#Parametersë¥¼ êµ¬í•˜ë¼. ë‹¨, biasëŠ” ë¬´ì‹œí•œë‹¤.

![AlexNet example](images/AlexNet_ex.png)

### <span style='background-color: #C2B2B2; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ” í’€ì´&nbsp;&nbsp;&nbsp;</span>

ë ˆì´ì–´ë³„ \#parametersë¥¼ êµ¬í•´ë³´ì.

- Conv Layer 1

$$ 96 \times 3 \times 11 \times 11 = 24,848 $$

- Conv Layer 2(grouped convolution 1)

$$ {{256 \times 96 \times 5 \times 5} \over {2}} = 307,200 $$

- Conv Layer 3

$$ 384 \times 256 \times 3 \times 3 = 884,736 $$

- Conv Layer 4(grouped convolution 2)

$$ 384 \times 384 \times 3 \times 3 / 2 = 663,552 $$

- Conv Layer 5(grouped convolution 3)

$$ 256 \times 384 \times 3 \times 3 / 2 = 442,368 $$

- Linear Layer 1

$$ 4096 \times (256 \times 6 \times 6) = 37,738,736 $$

- Linear Layer 2

$$ 4096 \times 4096 = 16,777,216 $$

- Linear Layer 3

$$ 1000 \times 4096 = 4,096,000 $$

ëª¨ë“  ë ˆì´ì–´ì˜ \#parametersë¥¼ í•©ì¹˜ë©´ ì´ 61Mì´ë‹¤.

---

### 2.5.4 Model Size

model sizeëŠ” weightê°€ ë™ì¼í•œ bit widthë¥¼ ê°€ì§„ë‹¤ë©´ ê°„ë‹¨íˆ êµ¬í•  ìˆ˜ ìˆë‹¤.

> bit widthê°€ ë‹¤ë¥¸ mixed precision modelì€ ê³„ì‚°ì´ ë‹¬ë¼ì§„ë‹¤.

- Model Size = \#Parameters $\cdot$ Bit Width

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ 2: AlexNet model size &nbsp;&nbsp;&nbsp;</span>

AlexNetì´ \#Parametersë¥¼ 61Më§Œí¼ ê°–ëŠ”ë‹¤ê³  í•  ë•Œ, ê° ì¡°ê±´ì—ì„œì˜ model sizeë¥¼ êµ¬í•˜ë¼.

- weight: fp32 type

- weight: int8 type

### <span style='background-color: #C2B2B2; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ” í’€ì´&nbsp;&nbsp;&nbsp;</span>

- fp32 type(=4 bytes)

$$ 61M \times 4Byte = 224MB $$

- int8 type(=1 byte)

$$ 61M \times 1Byte = 61MB $$

> $1MB = 1 \times 10^6 Bytes$

---

### 2.5.5 Number of Activations (\#Activations)

ResNetê³¼ MobileNetV2ë¥¼ ë¹„êµí•´ ë³´ì.

![ResNet vs MobileNet](images/ResNet_vs_MBV2.png)

- MobileNetV2: ResNetë³´ë‹¤ \#ParameterëŠ” ì ì§€ë§Œ, ë°˜ëŒ€ë¡œ Peak Activationì€ ëŠ˜ì—ˆë‹¤.

ì´ëŠ” MobileNetV2ì˜ íŠ¹ì • ë ˆì´ì–´ì—ì„œ \#activationsì´ memory bottleneckì„ ì¼ìœ¼í‚¤ëŠ” êµ¬ì¡°ì´ê¸° ë•Œë¬¸ì´ë‹¤.

> [MCUNetV2: Memory-Efficient Patch-based Inference for Tiny Deep Learning ë…¼ë¬¸](https://arxiv.org/abs/2110.15352)

![MBV2 activation](images/MBV2_activation.png)

- ì…ë ¥ë¶€ì™€ ê°€ê¹Œìš´ ë ˆì´ì–´ì˜ activation memoryê°€ êµ‰ì¥íˆ í¬ë‹¤.

- ë°˜ëŒ€ë¡œ resolutionê°€ ì¤„ì–´ë“¤ë©° channel ìˆ˜ê°€ ë§ì•„ì§€ëŠ” í›„ë°˜ë¶€ëŠ” activation memoryê°€ ì¤„ì–´ë“¤ì§€ë§Œ, ê·¸ë§Œí¼ ë§ì€ filterë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ weight memoryê°€ êµ‰ì¥íˆ ì»¤ì§„ë‹¤. 

ë˜í•œ í›ˆë ¨ ì¤‘ì—ë„ memory bottleneckì˜ ì£¼ëœ ì›ì¸ì€ \#Parameterê°€ ì•„ë‹Œ \#activationsì´ë‹¤.

![memory bottleneck in training](images/memory_bottleneck.png)

- \#Parameter, \#activations: FP32

- MobileNetV2ì´ ResNetë³´ë‹¤ \#Parameterê°€ 4ë°° ë” ì ì§€ë§Œ, \#activationsì€ 1.1ë°° ì ë‹¤.

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ 3: AlexNet \#Activations &nbsp;&nbsp;&nbsp;</span>

AlexNetì˜ (1) Total \#Activations, (2) Peak \#Activationsë¥¼ êµ¬í•˜ë¼.

![AlexNet example](images/AlexNet_ex.png)

### <span style='background-color: #C2B2B2; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ” í’€ì´&nbsp;&nbsp;&nbsp;</span>

$$ 3 \times 224 \times 224 = 150,528 $$

$$ 96 \times 55 \times 55 = 290,400 $$

$$ 96 \times 27 \times 27 = 69,984 $$

...

$$ 256 \times 6 \times 6 = 9,216 $$

$$ 4096 = 4096 $$

$$ 4096 = 4096 $$

$$ 1000 = 1000 $$

ë”°ë¼ì„œ Total \#Activations, Peak \#Activationsì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

- Total \#Activations

$$ = 932,264 $$

- Peak \#Activations 

$$\approx input \ activations + output \ activations $$

$$= 150,528 + 290,400 = 440,928$$

---

### 2.5.6 MACs

computation efficiencyë¥¼ í‘œí˜„í•˜ëŠ” ëŒ€í‘œì ì¸ ì§€í‘œì¸ **MAC**(Multiply-Accumulate) operationsë¥¼ ì‚´í´ë³´ì.(MAC ì—°ì‚°ì€ CNN ì—°ì‚°ì˜ ëŒ€ë¶€ë¶„ì„ ì°¨ì§€í•œë‹¤.)

ìš°ì„  Multiply-Accumulate operation(MAC)ì€ ë‹¤ìŒê³¼ ê°™ì´ ê³±ì…ˆê³¼ ë§ì…ˆìœ¼ë¡œ ì´ë£¨ì–´ì§„ ì—°ì‚°ì„ ì˜ë¯¸í•œë‹¤.

$$ a \leftarrow a + b \cdot c $$

ë‘ ê°€ì§€ ëŒ€í‘œì ì¸ ì—°ì‚°ì—ì„œ MACsë¥¼ êµ¬í•´ë³´ì.

- Matrix-Vector Multiplication(MV)

  Matrix-Vector Multiplication ì—°ì‚°ì—ì„œ MACsëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.

    ![Matrix-Vector](images/Matrix-Vector.png)

$$ MACs = m \cdot n $$

- General Matrix-Matrix Multiplication(GEMM)

  Matrix-Matrix ì—°ì‚°ì—ì„œ MACsëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.

    ![General Matrix-Matrix](images/General_Matrix-Matrix.png)

ì´ë²ˆì—ëŠ” ì—¬ëŸ¬ ë ˆì´ì–´ ì¢…ë¥˜ë³„ë¡œ ê³„ì‚°í•´ ë³´ì.(batch size n=1ë¡œ ê°€ì •)

- Linear Layer

  ![linear layer](images/linear.png)

$$ c_o \cdot c_i $$

- Convolution

  ![2D convolution](images/2D_conv.png)

$$ c_i \cdot k_h \cdot k_w \cdot c_o \cdot w_o \cdot c_o $$

> $w_o, c_o$ ê³„ì‚°ì— ì£¼ì˜. 

> ë‹¤ì‹œ ë§í•´ output activation ê° í”½ì…€ë§ˆë‹¤ $c_i \cdot k_h \cdot k_w \cdot c_o$ ë§Œí¼ì˜ MACsë¥¼ ê°–ëŠ”ë‹¤ëŠ” ëœ»ì´ê¸°ë„ í•˜ë‹¤.

- Grouped Convolution

  ![grouped convolution](images/grouped_conv.png)

$$ c_i/g \cdot k_h \cdot k_w \cdot c_o \cdot w_o \cdot c_o $$

- Depthwise Convolution

  ![depthwise convolution](images/depthwise_conv.png)

$$ k_h \cdot k_w \cdot c_o \cdot w_o \cdot c_o $$

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ 4: AlexNet \#MACs &nbsp;&nbsp;&nbsp;</span>

AlexNetì˜ \#MACsë¥¼ êµ¬í•˜ë¼.

![AlexNet example](images/AlexNet_ex.png)

### <span style='background-color: #C2B2B2; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ” í’€ì´&nbsp;&nbsp;&nbsp;</span>

- Conv Layer 1

$$ 96 \times 3 \times 11 \times 11 \times 55 \times 55 = 105,415,200 $$

- Conv Layer 2

$$ 256 \times 96 \times 5 \times 5 \times 27 \times 27 / 2 = 223,948,800 $$

...

- Linear Layer 1

$$ 4096 \times (256 \times 6 \times 6) = 37,748,736 $$

- Linear Layer 2

$$ 4096 \times 4096 = 16,777,216 $$

- Linear Layer 3

$$ 1000 \times 4096 = 4,096,000 $$

ë”°ë¼ì„œ ì´ MACsëŠ” 724Mì´ë‹¤.

---

### 2.5.7 FLOP

MACê³¼ ë§ˆì°¬ê°€ì§€ë¡œ computationê³¼ ê´€ë ¨ëœ ëŒ€í‘œì ì¸ ì§€í‘œë¡œ **FLOP**(Floating Point Operations)ì´ ìˆë‹¤.

> processorì˜ ì„±ëŠ¥ ì§€í‘œì¸ FLOPS(Floating Point Operation Per Second)ì™€ êµ¬ë¶„í•  ê²ƒ

ë§Œì•½ operationsì´ ë‹¤ìŒê³¼ ê°™ì€ data typeì´ë¼ë©´, 1 MAC = 2 FLOPì´ë‹¤.

- multiply: FP32

- add: FP32

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ 5: AlexNet \#FLOP &nbsp;&nbsp;&nbsp;</span>

AlexNetì˜ FLOPsë¥¼ êµ¬í•˜ë¼. AlexNetì€ ì´ MACsë¥¼ 724Mê°œë¥¼ ê°–ëŠ”ë‹¤ê³  í•œë‹¤.

- multiply: FP32

- add: FP32

### <span style='background-color: #C2B2B2; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ” í’€ì´&nbsp;&nbsp;&nbsp;</span>

$$ 724M \times 2 = 1.4G $$

---