# 7 Neural Architecture Search (Part I)

> [Lecture 07 - Neural Architecture Search (Part I) | MIT 6.S965](https://www.youtube.com/watch?v=NQj5TkqX48Q)

AutoMLì€ í¬ê²Œ ì„¸ ê°€ì§€ processë¥¼ í†µì¹­í•˜ëŠ” ìš©ì–´ë‹¤. ì´ë²ˆ ì±•í„°ëŠ” ê·¸ ì¤‘ **Neural Architecture Search**(NAS)ì— ëŒ€í•´ ë‹¤ë£¬ë‹¤.

| feature engineering | Hyper-Parameter Optimization(HPO) | Neural Architecture Search |
| :---: | :---: | :---: |
| ë„ë©”ì¸ ì§€ì‹ì— ê¸°ë°˜í•œ feature engineering | hyperparameterì˜ meta-optimization | ìµœì  model architectureë¥¼ íƒìƒ‰ |

---

## 7.1 Basic Concepts

NASëŠ” ëª¨ë¸ êµ¬ì¡°ë¥¼ ë‚˜ëˆ„ì–´ ë¶€ë¥¼ ë•Œ, ë‹¤ìŒê³¼ ê°™ì´ input stem, stage, head ì„¸ ê°€ì§€ ìš©ì–´ë¥¼ ì£¼ë¡œ ì‚¬ìš©í•œë‹¤.

![input stem, head, stages](images/stage.png)

- **Input Stem** 

  - ì£¼ë¡œ í° ì»¤ë„ í¬ê¸°( $7 \times 7$ )ë¥¼ ì‚¬ìš©í•œë‹¤.

    > channel ìˆ˜ê°€ 3ê°œ(RGB)ë¡œ ì ê¸° ë•Œë¬¸ì—, ê³„ì‚°ëŸ‰ ìì²´ëŠ” ì ë‹¤.

- **Stage**

  - ì¶œë ¥ í•´ìƒë„ê°€ ë™ì¼í•œ blockë“¤ì˜ ì§‘í•©ì´ë‹¤. (first blockì—ì„œ downsampling ìˆ˜í–‰)

  - downsamplingì´ ìˆëŠ” ë¸”ë¡ì„ ì œì™¸í•œ, stageì˜ ë‚˜ë¨¸ì§€ ë¸”ë¡ì—ì„œëŠ” **residual connection**ì„ ì¶”ê°€í•  ìˆ˜ ìˆë‹¤.

- **Head**

  - application-specificí•˜ë‹¤. (detection head, segmentation head ë“±)

ì´ë•Œ, early stageì™€ late stageì˜ íŠ¹ì§•ì„ ë¹„êµí•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

|| early stage | late stage |
| :---: | :---: | :---: |
| activation size | í¬ë‹¤ | ì‘ë‹¤ |
| \#parameters | ì ë‹¤ | ë§ë‹¤ |

---

## 7.2 manually-designed neural network

ë‹¤ìŒì€ ImageNet ë°ì´í„°ì…‹ì—ì„œ í•™ìŠµí•œ ì—¬ëŸ¬ CNN ëª¨ë¸ë¥¼, ì—°ì‚°ëŸ‰(MACs)-ì •í™•ë„ ê·¸ë˜í”„ì— ë‚˜íƒ€ë‚¸ ê²ƒì´ë‹¤.

![accuracy-efficiency tradeoff on ImageNet](images/accuracy-efficiency_tradeoff.png)

> NASëŠ” ì´ëŸ¬í•œ CNN ëª¨ë¸ì— ê¸°ë°˜í•˜ì—¬, ë” íš¨ìœ¨ì ì¸ ëª¨ë¸ì„ ì°¾ëŠ” ê²½ìš°ê°€ ë§ë‹¤. ë”°ë¼ì„œ, ë¨¼ì € ì´ëŸ¬í•œ CNN ëª¨ë¸ì„ ì‚´í´ë³´ì.

---

### 7.2.1 AlexNet, VGGNet

> [ImageNet Classification with Deep Convolutional Neural Networks ë…¼ë¬¸(2012)](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf): AlexNet

> [Very Deep Convolutional Networks for Large-Scale Image Recognition ë…¼ë¬¸(2014)](https://arxiv.org/abs/1409.1556): VGGNet

ë‹¤ìŒì€ CNNì˜ ì´ˆê¸° ëª¨ë¸ì¸ AlexNetê³¼ VGGNetì˜ êµ¬ì¡°ë¥¼ ë¹„êµí•œ ë„í‘œë‹¤.

|| AlexNet | VGGNet |
| :---: | :---: | :---: |
| êµ¬ì¡° | ![AlexNet](https://github.com/erectbranch/TinyML_and_Efficient_DLC/blob/master/2022/lec07/summary01/images/AlexNet_arch.png) | ![VGGNet](https://github.com/erectbranch/TinyML_and_Efficient_DLC/blob/master/2022/lec07/summary01/images/VGGNet_arch.png) |
| íŠ¹ì§• | ealry stageì—ì„œ í° kernel sizeë¥¼ ì‚¬ìš© ( $11 \times 11$ ) | early stageì—ì„œ ì‘ì€ kernelì„ ì—¬ëŸ¬ ê°œ ì‚¬ìš© ( $3 \times 3$ ) |

VGGNetì—ì„œëŠ” $3 \times 3$ ë ˆì´ì–´ë¥¼ ë‘ ê°œ ìŒ“ëŠ” ê²ƒì´, AlexNetë³´ë‹¤ computational costê°€ ì ê²Œ ë“¤ë©´ì„œë„ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì„ì„ ì…ì¦í–ˆë‹¤.

- (-) í•˜ì§€ë§Œ \#layers, kernel call, memory accessê°€ ëŠ˜ì–´ë‚˜ë©´ì„œ, ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ì¸¡ë©´ì—ì„œëŠ” ë‹¨ì ì„ ê°–ëŠ”ë‹¤.
  
  ![VGGNet bottleneck](https://github.com/erectbranch/TinyML_and_Efficient_DLC/blob/master/2022/lec07/summary01/images/VGGNet_FLOP_bottleneck.png)

---

### 7.2.2 SqueezeNet: File Module

> [SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size ë…¼ë¬¸(2016)](https://arxiv.org/pdf/1602.07360)

**SqueezeNet**ì€ $3 \times 3$ convolutionì„ **fire module**ë¡œ ëŒ€ì²´í•˜ì—¬, íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ì¤„ì´ë©´ì„œ ì„±ëŠ¥ì€ ìœ ì§€í•˜ì—¬ íš¨ìœ¨ì ìœ¼ë¡œ ì—°ì‚°ì„ ìˆ˜í–‰í•œë‹¤.

| Architecture | Fire Module |
| :---: | :---: | 
| ![SqueezeNet](https://github.com/erectbranch/TinyML_and_Efficient_DLC/blob/master/2022/lec07/summary01/images/SqueezeNet.png) | ![Fire Module](https://github.com/erectbranch/TinyML_and_Efficient_DLC/blob/master/2022/lec07/summary01/images/fire_module_2.png) |
| headì—ì„œ GAP(Global Average Pooling)ì„ ì‚¬ìš©í•œë‹¤. | $1 \times 1$ convolution(**squeeze**), $3 \times 3$ convolution(**expand**)ì„ ì‚¬ìš©í•œë‹¤. |

fire moduleì€ ë‹¤ìŒê³¼ ê°™ì€ ë‹¨ê³„ë¡œ ì—°ì‚°ì´ ì§„í–‰ëœë‹¤.

| Squeeze | $\rightarrow$ | Expand | $\rightarrow$ | Concatenate |
| :---: | :---: | :---: | :---: |:---: |
| ì±„ë„ ìˆ˜ë¥¼ ì¤„ì¸ë‹¤. ( $1 \times 1$ ) || ì±„ë„ ìˆ˜ë¥¼ í™•ì¥í•œë‹¤. ( $1 \times 1$ , $3 \times 3$ ) || ì¶œë ¥ì„ í•©ì¹œë‹¤. |

---

### 7.2.3 ResNet50: Bottleneck Block

> [Deep Residual Learning for Image Recognition ë…¼ë¬¸(2015)](https://arxiv.org/abs/1512.03385)

ResNetì—ì„œëŠ” **bottleneck block**ì„ ë„ì…í•˜ì—¬, ì—°ì‚°ëŸ‰ì€ ì¤„ì´ë©´ì„œ residual connectionì„ í†µí•´ gradient vanishing ë¬¸ì œë¥¼ í•´ê²°í•˜ë©°, ë” ê¹Šì€ CNN êµ¬ì¡°ë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤.

![ResNet bottleneck block](images/ResNet_bottleneck.png)


### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ 1: bottleneck block MACs &nbsp;&nbsp;&nbsp;</span>

ìœ„ bottleneck block ê·¸ë¦¼ì—ì„œ \#MACs ì—°ì‚°ì´ ì–¼ë§ˆë‚˜ ì¤„ì—ˆëŠ”ì§€ ê³„ì‚°í•˜ë¼.

### <span style='background-color: #C2B2B2; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ” í’€ì´&nbsp;&nbsp;&nbsp;</span>

- full convolution(\#channels 2048, \#kernels 9)

$$ 2048 \times 2048 \times H \times W \times 9 = 512 \times 512 \times H \times W \times 144 $$

- bottleneck block

$$ 2048 \times 512 \times H \times W \times 1 $$

$$ + 512 \times 512 \times H \times W \times 9 $$

$$ + 2048 \times 512 \times H \times W \times 1 $$

$$ = 512 \times 512 \times H \times W \times 17 $$

> ì´ **8.5ë°°** \#MACs ì—°ì‚°ì´ ì¤„ì–´ë“¤ì—ˆë‹¤.

---

### 7.2.4 ResNeXt: Grouped Convolution

> [Aggregated Residual Transformations for Deep Neural Networks ë…¼ë¬¸(2017)](https://arxiv.org/abs/1611.05431)

**ResNeXt**(2017) ë…¼ë¬¸ì€, **grouped convolution**ì„ ë„ì…í•˜ì—¬ íŒŒë¼ë¯¸í„° ìˆ˜ëŠ” ì¤„ì´ë©´ì„œ ì •í™•ë„ëŠ” ëŠ˜ë ¸ë‹¤.

| ResNet(2015) | ResNeXt(2017) |
| :---: | :---: |
| ![ResNet](images/ResNet_block.png) | ![ResNeXt](images/ResNeXt_block.png) |

> í‘œê¸°: \#ì…ë ¥ ì±„ë„ ìˆ˜, í•„í„° í¬ê¸°, \# ì¶œë ¥ ì±„ë„ ìˆ˜

> ë…¼ë¬¸ì—ì„œëŠ” group ìˆ˜ë¥¼ cardinalityë¼ëŠ” ìš©ì–´ë¡œ ì •ì˜í•œë‹¤.

---

### 7.2.5 ShuffleNet: 1x1 group convolution & channel shuffle

> [ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices ë…¼ë¬¸(2018)](https://arxiv.org/abs/1707.01083)
 
ShuffleNetì—ì„œëŠ”, group convolutionì—ì„œ channel informationì´ ì†ì‹¤ë˜ëŠ” ê²ƒì„ ë³´ì™„í•˜ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œ **channel shuffle**ì„ ì œì•ˆí–ˆë‹¤.

| ShuffleNet block | Channel Shuffle |
| :---: | :---: |
| ![ShuffleNet](https://github.com/erectbranch/TinyML_and_Efficient_DLC/blob/master/2022/lec07/summary01/images/ShuffleNet.png) | ![channel shuffle](https://github.com/erectbranch/TinyML_and_Efficient_DLC/blob/master/2022/lec07/summary01/images/channel_shuffle.png) |

---

### 7.2.6 SENet: Squeeze-and-Excitation Block

> [Squeeze-and-Excitation Networks ë…¼ë¬¸(2017)](https://arxiv.org/pdf/1709.01507.pdf)

SENetì€ **Squeeze-and-Excitation**(**SE**) blockì„ ë„ì…í•˜ì—¬, feature mapì˜ <U>ê° channelì˜ ì •ë³´ê°€ ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œì§€ë¥¼ íŒë‹¨</U>í•œ ë’¤, í•´ë‹¹ ì±„ë„ì„ ê°•ì¡°í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¨ë‹¤.

> Transformerì˜ attentionê³¼ ë¹„ìŠ·í•œ ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤. 

> SE blockì€ ë‹¤ë¥¸ CNN model(VGG, ResNet ë“±)ì˜ ì–´ë””ë“  ë¶€ì°©í•  ìˆ˜ ìˆë‹¤.

![SE block](images/SE_block.png)

SE blockì€ Squeeze-Ecxcitation ë‘ ë‹¨ê³„ë¡œ ì´ë£¨ì–´ì§„ë‹¤.

- **Squeeze**(ì••ì¶•)

  global average pooling ì—°ì‚°ì„ ì´ìš©í•˜ì—¬, spatial informationì„ $1 \times 1$ ë¡œ ì••ì¶•í•œë‹¤.(\#channels $C$ ëŠ” ìœ ì§€)

  - $u_{c}$ : feature map( $H \times W$ )

$$ z = F_{sq}(u_{c}) = {{1} \over {H \times W}} {\sum_{i=1}^{H}}{\sum_{j=1}^{W}}{u_{c}(i, j)} $$

- **Excitation**(ì¬ì¡°ì •)

  squeezeëœ ë²¡í„°ë¥¼ normalizeí•œ ë’¤, ì›ë˜ feature mapì— ê³±í•´ì¤€ë‹¤. ì´ë•Œ FC1 - ReLU - FC2 - Sigmoid ìˆœì„œë¡œ normalizeëœë‹¤.

  - $W_{1}, W_{2}$ : FC layer weight matrix

  - $\delta$ : ReLU ì—°ì‚°, $\sigma$ : Sigmoid ì—°ì‚°

  ![Excitation](https://github.com/erectbranch/TinyML_and_Efficient_DLC/blob/master/2022/lec07/summary01/images/Excitation.png)

$$ s = F_{ex}(z, W) = {\sigma}(W_{2} {\delta}(W_{1} z)) $$

ì´ë ‡ê²Œ êµ¬í•œ ê°€ì¤‘ì¹˜ ë²¡í„° $s$ ë¥¼ feature map $u$ ì— ê³±í•˜ëŠ” ê²ƒìœ¼ë¡œ, ì¤‘ìš”í•œ channel ê°’ì„ ê°•ì¡°í•œë‹¤.

$$ F_{scale}(u_{c}, s_{c}) = s_{c} \cdot u_{c} $$

---

## 7.3 MobileNet Family

---

### 7.3.1 MobileNet: Depthwise-Separable Block

> [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications ë…¼ë¬¸(2017)](https://arxiv.org/abs/1704.04861)

**MobileNetV1**(2017)ì€ depthwise, pointwise convolution ë‘ ë ˆì´ì–´ê°€ ì—°ê²°ëœ, **depthwise-separable block**ì„ ì œì•ˆí–ˆë‹¤.

> input stemì˜ í•˜ë‚˜ì˜ full convì„ ì œì™¸í•˜ê³ , ë‚˜ë¨¸ì§€ëŠ” ëª¨ë‘ depthwise-separable conv ì—°ì‚°ìœ¼ë¡œ êµ¬ì„±ëœë‹¤.

![depthwise-separable convolution](images/depthwise-separable_3.png)

| depthwise conv | pointwise conv |
| :---: | :---: |
| ë‹¨ì¼ ì±„ë„ë§ˆë‹¤ spatial informationì„ ìº¡ì²˜ | spatial informationì„ mixing |

ì´ë•Œ, conv ë‹¤ìŒìœ¼ë¡œ ì´ì–´ì§€ëŠ” activation function(non-linearity)ìœ¼ë¡œ ReLU6ë¥¼ ì‚¬ìš©í•œë‹¤.

![ReLU6](images/ReLU6.png)

depthwise-separable blockê³¼ full conv blockì„ ë¹„êµ ì‹œ, í›¨ì”¬ ì ì€ íŒŒë¼ë¯¸í„° ìˆ˜ë¡œ ê·¼ì†Œí•œ ì •í™•ë„ë¥¼ í™•ë³´í•œë‹¤.

![depthwise-separable vs full convolution](images/depthwise-separable_vs_conv.png)

> depthwise-separable block: \#channels = \#groupsì¸ ê·¹ë‹¨ì ì¸ í˜•íƒœì˜ group convolutionë¡œë„ ë³¼ ìˆ˜ ìˆë‹¤.

---

#### 7.3.1.1  Width Multiplier, Resolution Multiplier

> [Width Transfer: On the (In)variance of Width Optimization ë…¼ë¬¸(2021)](https://arxiv.org/abs/2104.13255)

MobileNetì—ì„œëŠ” ë‹¤ì–‘í•œ ì¡°ê±´ì— ë§ëŠ” ëª¨ë¸ì„ íšë“í•  ìˆ˜ ìˆë„ë¡, ë‘ ê°€ì§€ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ê°€ë¡œ ì‚¬ìš©í•œë‹¤. 

![width mult, res](images/width_res.png)

| Width Multiplier $\alpha$ | Resolution Multiplier $\rho$ |
| :---: | :---: |
| ì¶œë ¥ ì±„ë„ ìˆ˜ì— ì ìš© ( $N \rightarrow {\alpha}N$ ) | ì…ë ¥ ë°ì´í„°ì˜ í•´ìƒë„ì— ì ìš© |

> $\alpha \in (0, 1]$ , $\rho \in (0, 1]$

---

### 7.3.2 MobileNetV2: inverted bottleneck block

> [MobileNetV2: Inverted Residuals and Linear Bottlenecks ë…¼ë¬¸(2018)](https://arxiv.org/pdf/1801.04381.pdf)

ê¸°ì¡´ bottleneck blockì˜ ë¬¸ì œì ì€, ReLUë¥¼ ì‚¬ìš©í•˜ë©´ì„œ ìƒëŠ” ì •ë³´ëŸ‰ì´ ë§ë‹¤ëŠ” ì ì´ë‹¤. **MobileNetV2**(2018)ì€ ì…ë ¥, ì¶œë ¥ ì±„ë„ ìˆ˜ë¥¼ ëŠ˜ë¦¬ë©´, ì •ë³´ ì†ì‹¤ì„ compensateí•  ìˆ˜ ìˆë‹¤ëŠ” ì•„ì´ë””ì–´ë¡œë¶€í„°, **inverted bottleneck block**ì„ ì œì•ˆí–ˆë‹¤.

| inverted bottleneck block | stride=1 | stride=2 | 
| :---: | :---: | :---: |
| ![inverted residual block](images/inverted_residual_block.png) | ![mbv2 block stride](images/Mb_vs_MbV2_block_2.png) | ![mbv2 block stride](images/Mb_vs_MbV2_block_3.png) |
| | block + residual connection | block + downsampling |

MobileNetV1ê³¼ ë¹„êµí–ˆì„ ë•Œ, ë” ì ì€ íŒŒë¼ë¯¸í„°ë¡œ ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ íšë“í–ˆë‹¤.

![MbV2 vs MbV1](images/mobilenetv1_vs_shufflenet_vs_mobilenetv2.png)

---

### 7.3.3 MobileNetV3

> [Searching for MobileNetV3 ë…¼ë¬¸(2019)](https://arxiv.org/pdf/1905.02244.pdf)

**MobileNetV3**(2019)ëŠ” MobileNetV2ì˜ í›„ì† ë…¼ë¬¸ìœ¼ë¡œ, NetAdapt algorithm + hardware-aware NASë¥¼ ì´ìš©í•´ ì°¾ì€ ê°œì„ ëœ architectureì´ë‹¤. 

| MobileNetV3-Large | MobileNetV3-Small |
| :---: | :---: |
| ![MbV3-Large](images/MbV3-Large.png) | ![MbV3-Small](images/MbV3-Small.png) |

- SE: Squeeze-and-Excitation block

- NL: non-linearity

  > HS: h-swish, RE: ReLU

- NBN: no batch normalization

---

#### 7.3.3.1 MobileNetV2 vs MobileNetV3

MbV3ì€ MbV2ì—ì„œ ë¹„ìš©ì´ í° ë ˆì´ì–´(last stage)ë¥¼ êµ¬ì¡°ì ìœ¼ë¡œ ê°œì„ í•˜ê³ , ê°œì„ ëœ non-linearity functionì„ ì‚¬ìš©í•œë‹¤.


- redesign expensive layers

  ![MbV3 last stage](images/MbV3_last_stage.png)

  | MobileNetV2 | MobileNetV3 |
  | :---: | :---: |
  | 1x1 conv + avgpool (expensive) | avgpool + 1x1 conv (effective) |

- nonlinearity(activation function)

  ReLUì™€ **h-swish**ë¥¼ í•¨ê»˜ ì‚¬ìš©í•œë‹¤. ë¯¸ë¶„í•´ì„œ ìƒìˆ˜ ê°’ì´ ì•„ë‹ˆë©°, ì¼ë¶€ ìŒìˆ˜ ê°’ì„ í—ˆìš©í•œë‹¤.

  > ë‹¨ë…ìœ¼ë¡œ h-swishë§Œì„ ì‚¬ìš©í•˜ê¸°ë³´ë‹¤, ReLUì™€ í•¨ê»˜ ì‚¬ìš©í•˜ë©´ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.

---

#### 7.3.3.2 swish vs h-swish

**swish**ëŠ” sigmoidë¥¼ í¬í•¨í•˜ê¸° ë•Œë¬¸ì— ë³µì¡í•˜ê³ , í•˜ë“œì›¨ì–´ ì§€ì›ì´ ì œí•œì ì´ë‹¤. ì´ë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•´ ë“±ì¥í•œ non-linearityê°€ ë°”ë¡œ **h-swish**ì´ë‹¤.

| ReLU vs swish | swish vs h-swish |
| :---: | :---: |
| ![swish vs ReLU](images/swish_vs_ReLU.png) | ![swish vs h-swish](images/swish_vs_h-swish.png) |


$$ \mathrm{swish} \ x = x \cdot {\sigma}(x) $$

$$ \mathrm{h} \ \mathrm{swish} \ = x{{\mathrm{ReLU}6(x+3)} \over {6}} $$

---