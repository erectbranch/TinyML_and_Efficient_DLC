# 10 Knowledge Distillation

> [Lecture 10 - Knowledge Distillation | MIT 6.S965](https://youtu.be/IIqf-oUTHe0)

> [EfficientML.ai Lecture 9 - Knowledge Distillation (MIT 6.5940, Fall 2023, Zoom)](https://youtu.be/dSDW_c789zI)

**Knowledge Transfer**ë€, ë³µì¡í•œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë‹¨ìˆœí•œ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚´ìœ¼ë¡œì¨, ë³µì¡í•œ ëª¨ë¸ ì„±ëŠ¥ì— ê·¼ì ‘í•œ ë‹¨ìˆœí•œ ëª¨ë¸ì„ íšë“í•˜ëŠ” ë°©ë²•ì´ë‹¤. 

---

## 10.1 Cloud Model vs Edge Model

cloud model(ResNet50)ê³¼ edge model(MobileNetV2-Tiny)ì˜ training curveë¥¼ ë¹„êµí•´ ë³´ì.

![ResNet50, MobileNetV2](images/ResNet50_vs_MobileNetV2.png)

> ê°€ë¡œ: epoch, ì„¸ë¡œ: accuracy

ìœ„ ê·¸ë˜í”„ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, edge model(MobileNetV2-Tiny)ì€ ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì§•ì„ ê°–ëŠ”ë‹¤.

- (-) ì‘ì€ capacityë¥¼ ê°–ëŠ” ë§Œí¼, ë†’ì€ ì •í™•ë„ë¥¼ ì–»ê¸° í˜ë“¤ë‹¤.

- (-) overfittingì„ ë°©ì§€í•˜ê¸° ìœ„í•œ í•™ìŠµ ê¸°ë²•ì´, ì˜¤íˆë ¤ ì •í™•ë„ í•˜ë½ì„ ìœ ë°œí•  ìˆ˜ ìˆë‹¤.

  > **data augmentation**(ë°ì´í„° ì¦ê°•), **dropout**(ë“œë¡­ì•„ì›ƒ) ë“±

  > data augmentation: mirroring, random cropping, rotation, shearing, local wrapping ë“±

---

## 10.2 Knowledge Distillation

> [Distilling the Knowledge in a Neural Network ë…¼ë¬¸(2015)](https://arxiv.org/abs/1503.02531)

> [Neural Network Intelligence: Knowledge Distillation on NNI](https://nni.readthedocs.io/en/v2.3/TrialExample/KDExample.html#knowledgedistill)

**Knowledge Distillation**(KD, ì§€ì‹ ì¦ë¥˜)ë€, teacherê°€ ê°–ê³  ìˆëŠ” knowledgeë¥¼ student networkë¡œ ì „ë‹¬í•˜ëŠ” ë°©ë²•ì´ë‹¤.

![knowledge distillation](images/distill.png)

> ground-truth: ì‹¤í—˜ìê°€ ì •í•œ 'ì •ë‹µ'(modelì´ ë‹µìœ¼ë¡œ ë‚´ë†“ê¸°ë¥¼ ì›í•˜ëŠ” class)

- Input: Teacher, Student ëª¨ë‘ì—ê²Œ ì „ë‹¬ëœë‹¤.

- **Distillation Loss**

  ë‹¤ìŒ ë‘ ì •ë³´ì˜ ì°¨ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚°ì¶œí•œë‹¤.

  - soft labels = Teacher Logits + Softmax with Temperature

  - soft predictions = Student Logits + Softmax with Temperature

- **Classification Loss** = student's standard loss

- **Total loss** = classification loss $\times \alpha$ + distillation loss $\times \beta$

---

### 10.2.1 Intuition of Knowledge Distillation

ê°œì™€ ê³ ì–‘ì´ì˜ ì´ì§„ ë¶„ë¥˜ ë¬¸ì œ ì˜ˆì‹œë¥¼ ë°”íƒ•ìœ¼ë¡œ KDë¥¼ ì´í•´í•´ ë³´ì.

![knowledge distillation ex 1](images/kd_ex_1.png)

ìœ„ ê²°ê³¼ì—ì„œ, teacherì™€ studentê°€ ì…ë ¥ì„ ê³ ì–‘ì´ë¡œ ì˜ˆì¸¡í•œ í™•ë¥ ì€, softmaxë¥¼ í†µí•´ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.

- Teacher

$${ {e^5} \over {e^5 + e^1} } = 0.982$$

- Student

$${ {e^3} \over {e^3 + e^2} } = 0.731$$

studentëŠ”, teacherì— ë¹„í•´ ì…ë ¥ì´ ê³ ì–‘ì´ë¼ëŠ” confidenceê°€ ë¶€ì¡±í•˜ë‹¤. ì´ë¥¼ teacher modelì˜ informationìœ¼ë¡œ ë³´ì™„í•œë‹¤.

---

### 10.2.2 Softmax Temperatrue

í•˜ì§€ë§Œ teacherì˜ confidenceê°€ ë†’ì„ìˆ˜ë¡, ì •ë‹µì´ ì•„ë‹Œ ë‹¤ë¥¸ classì˜ informationì´ 0ì— ê°€ê¹ê²Œ ëœë‹¤. teacherì˜ informationì„ ë³´ì¡´í•˜ì—¬ ì „ë‹¬í•˜ê¸° ìœ„í•´, **temperature**ë¼ëŠ” ê°œë…ì´ ë“±ì¥í•œë‹¤.

$$ p(z_i, T) = { {\exp({z_{i} \over T})} \over {\sum_{j}{\exp({z_{j} \over T})} } } $$

- $T$ : temperature

  - $T$ ê°€ í´ìˆ˜ë¡, softí•œ distributionì´ ëœë‹¤.

  - $T=1$ : standard softmax

- teacherì™€ student í¬ê¸° ì°¨ì´ê°€ í´ ë•ŒëŠ”, ëŒ€ì²´ë¡œ ì‘ì€ temperatureê°€ ë” íš¨ìœ¨ì ì´ë‹¤. 

  > studentê°€ teacherì˜ informationì„, ì œëŒ€ë¡œ captureí•˜ì§€ ëª»í•˜ê¸° ë•Œë¬¸.


### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ 1: Softmax Temperature &nbsp;&nbsp;&nbsp;</span>

ë‹¤ìŒ ì˜ˆì‹œì—ì„œ, $T=1 , T=10$ ì¼ ë•Œ ì…ë ¥ì„ ê³ ì–‘ì´ë¡œ ì˜ˆì¸¡í•œ í™•ë¥ ì„ êµ¬í•˜ë¼.

|  | Logits |
| :---: | :---: |
| Cat | 5 |
| Dog | 1 | 

### <span style='background-color: #C2B2B2; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ” í’€ì´&nbsp;&nbsp;&nbsp;</span>

- $T = 1$ (**standard softmax**)

$${ {e^{5 \over 1} } \over {e^{5 \over 1} + e^{1 \over 1} } } = 0.982$$

- $T = 10$

$${ {e^{5 \over 10} } \over {e^{5 \over 10} + e^{1 \over 10} } } = 0.599$$

---

## 10.3 What to match?

ì´í›„ ë‹¤ì–‘í•œ ì—°êµ¬ì—ì„œ logits(response)ë§Œ ì•„ë‹ˆë¼, êµì‚¬ì˜ ë‹¤ë¥¸ ì •ë³´ë¥¼ í•™ìƒê³¼ matchí•˜ëŠ” ë°©ë²•ë“¤ì´ ì œì•ˆë˜ì—ˆë‹¤.

---

### 10.3.1 Matching intermediate weights

> [FitNets: Hints for Thin Deep Nets ë…¼ë¬¸(2015)](https://arxiv.org/abs/1412.6550)

FitNets ë…¼ë¬¸ì—ì„œëŠ”, êµì‚¬ì™€ í•™ìƒì˜ ê° ë ˆì´ì–´ì˜ intermediate weightsë¥¼ ë¹„êµí•˜ì—¬ Distillation Lossë¥¼ ì‚°ì¶œí•œë‹¤.

![matching intermediate weights](images/matching_intermediate_weights_1.png)

- êµì‚¬ ê°€ì¤‘ì¹˜ì™€ í•™ìƒ ê°€ì¤‘ì¹˜ì˜ L2 distanceë¥¼ ì¸¡ì •í•œë‹¤.

- ì´ë•Œ ì±„ë„ ìˆ˜ê°€ ì ì€ í•™ìƒì— 1x1 convolutionì„ ì ìš©í•˜ì—¬, êµì‚¬ì™€ ë™ì¼í•œ ì±„ë„ì„ ê°–ë„ë¡ projectioní•œë‹¤. (linear transformation)

    ![matching intermediate weights 3](images/matching_intermediate_weights_3.png)

---

### 10.3.2 Matching intermediate features

êµì‚¬ì™€ í•™ìƒ ì‚¬ì´ì˜ feature mapì„ ë§¤ì¹­í•˜ëŠ” ê¸°ë²•ë„ ì œì•ˆë˜ì—ˆë‹¤.

![matching intermediate features](images/matching_feature_map.png)

---

#### 10.3.2.1 Minimizing MMD

> [Like What You Like: Knowledge Distill via Neuron Selectivity Transfer ë…¼ë¬¸(2017)](https://arxiv.org/abs/1707.01219)

Like What You Like ë…¼ë¬¸ì—ì„œëŠ” **MMD**(Maximum Mean Discrepancy. ìµœëŒ€ í‰ê·  ë¶ˆì¼ì¹˜)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, êµì‚¬ì™€ í•™ìƒì˜ feature map discrepancyë¥¼ ê³„ì‚°í•œë‹¤.

![matching intermediate features](images/matching_intermediate_features.png)

---

#### 10.3.2.2 Matching intermediate attention maps

> [Paying More Attention to Attention ë…¼ë¬¸(2017)](https://arxiv.org/abs/1612.03928)

ìœ„ ë…¼ë¬¸ì—ì„œëŠ” êµì‚¬ì™€ í•™ìƒì˜ attention mapì„ ë§¤ì¹­í•œë‹¤.

![attention transfer](images/attention_transfer.png)

ì´ë•Œ attentionì˜ ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì§•ì— ì£¼ëª©í•œë‹¤.

- CNN feature map $x$ ì˜ attention

$$\mathrm{attention} = { {\partial L} \over {\partial x} }$$

- intuition: position $i, j$ ì˜ attentionì´ í¬ë‹¤ë©´, í•´ë‹¹ ì§€ì ì— ì‘ì€ ë³€í™”(perturbation)ë¥¼ ì£¼ì–´ë„ ìµœì¢… ì¶œë ¥ì— í° ì˜í–¥ì„ ë¯¸ì¹  ê²ƒì´ë‹¤.

- ë†’ì€ ì •í™•ë„ë¥¼ ê°–ëŠ” ëª¨ë¸ì€, ë¹„ìŠ·í•œ attention mapì„ ê°–ëŠ”ë‹¤.

  ![attention map ex](https://github.com/erectbranch/MIT-Efficient-AI/blob/master/2022/lec10/summary01/images/attentions_ex.png)

---

#### 10.3.2.3 Minimizing the L2 distance

> [Paraphrasing Complex Network: Network Compression via Factor Transfer ë…¼ë¬¸(2018)](https://arxiv.org/abs/1802.04977)

> [NAVER Engineering: paraphrasing complex network seminar](https://tv.naver.com/v/5440966)

ìœ„ ë…¼ë¬¸ì—ì„œëŠ” Paraphraser, Translatorë¥¼ ë‘ì–´, êµì‚¬ì˜ output feature mapsì—ì„œ í•œ ì°¨ë¡€ ë” feature extractionì„ ê±°ì¹œ ì •ë³´ë¥¼ ì „ë‹¬í•œë‹¤.(factor transfer)

![paraphrasing complex network](images/paraphrasing_complex_network.png)

| | Paraphraser | Translator |
| :---: | :---: | :---: |
| Implementation | convolution | 1 layer MLP |

---

### 10.3.3 Matching sparsity pattern

> [Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons ë…¼ë¬¸(2019)](https://arxiv.org/abs/1811.03233)

> [Martin Trimmel: Linear Regions of Deep Neural Networks](https://www.youtube.com/watch?v=RM0wOvYkXDA)

ReLUë¥¼ ê±°ì¹˜ë©° ìƒê¸°ëŠ” activated, deactivated ì˜ì—­ì€, ì…ë ¥ì„ ë°”ë¥´ê²Œ êµ¬ë¶„í•˜ê¸° ìœ„í•œ linear decision boundaryë¥¼ í˜•ì„±í•œë‹¤. 

![matching sparsity pattern](images/sparsity_pattern.png)

- **decision boundary**ëŠ” activation boundaryì™€ í° ì—°ê´€ì„±ì„ ê°–ëŠ”ë‹¤.

- ë”°ë¼ì„œ sparsity patternìœ¼ë¡œë„, êµì‚¬ê°€ ê°€ì§€ëŠ” ì •ë³´ë¥¼ ì „ë‹¬í•  ìˆ˜ ìˆë‹¤.

ìœ„ì™€ ê°™ì€ ì•„ì´ë””ì–´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ë‹¤ìŒê³¼ ê°™ì´ êµì‚¬ì˜ sparsity pattern ì •ë³´ë¥¼ í•™ìƒì—ê²Œ ì „ë‹¬í•œë‹¤.

![matching sparsity pattern](images/matching_sparsity.png)

- Sparsity Pattern

  **indicator function**ì„ ì‚¬ìš©í•´ì„œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.

$$ \rho(x) = 1[x > 0] $$

- Loss Function

$$ \mathcal{L}(I) = {|| \rho({ {T}(I)}) - \rho({ {S}(I)}) ||}_{1} $$

---

### 10.3.4 Matching relational information

ì„œë¡œ ë‹¤ë¥¸ ë ˆì´ì–´ë‚˜ ë°ì´í„° ì‚¬ì´ì˜ ê´€ê³„, ì¦‰ **relational information**ì— ì£¼ëª©í•œ Knowledge Distillation ê¸°ë²•ë„ ì œì•ˆë˜ì—ˆë‹¤.

---

#### 10.3.4.1 Relations between different layers

> [A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning ë…¼ë¬¸(2017)](https://openaccess.thecvf.com/content_cvpr_2017/papers/Yim_A_Gift_From_CVPR_2017_paper.pdf)

ë‹¤ìŒ ê·¸ë¦¼ì€ 32ê°œ ë ˆì´ì–´ë¥¼ ê°–ëŠ” êµì‚¬ì™€, 14ê°œ ë ˆì´ì–´ë¥¼ ê°–ëŠ” í•™ìƒ ëª¨ë¸ ì‚¬ì´ì—ì„œ relational informationì„ ì „ë‹¬í•˜ëŠ” ê³¼ì •ì´ë‹¤.

> ì´ë•Œ êµì‚¬ì™€ í•™ìƒì˜ ë ˆì´ì–´ ìˆ˜ëŠ” ë‹¤ë¥´ë‚˜, ì±„ë„ ìˆ˜ëŠ” ëª¨ë‘ ë™ì¼í•˜ë¯€ë¡œ ì£¼ì˜.

![relational information](images/relational_information.png)

- **module** ë¬¶ìŒ ë‚´ feature mapì„ ë‚´ì í•˜ì—¬, FSP matrixë¥¼ ì‚°ì¶œí•œë‹¤.

  > Flow of Solution Procedure(FSP) matrix

$$ G_{i,j}(x; W) = \sum_{s=1}^h \sum_{t=1}^w { {F^1_{s,t,i}(x;W) \times F^2_{s,t,j}(x; W)} \over {h \times w} } $$

- êµì‚¬ì™€ í•™ìƒ FSP matrixì˜, L2 lossë¥¼ ê³„ì‚°í•œë‹¤.

---

#### 10.3.4.2 Relations between different samples

> [Relational Knowledge Distillation ë…¼ë¬¸(2019)](https://arxiv.org/abs/1904.05068)

Relational Knowledge Distillation(RKD) ë…¼ë¬¸ì—ì„œëŠ”, ì—¬ëŸ¬ ì…ë ¥ ìƒ˜í”Œì„ ì£¼ì—ˆì„ ë•Œ, ê° ìƒ˜í”Œì˜ intermediate featuresê°€ ê°–ëŠ” ê´€ê³„ë¥¼ ì „ë‹¬í•œë‹¤.

![conventional vs relational KD](images/Conventional_vs_Relational_KD.png)

- ì´ë•Œ $n$ ê°œ sampleì˜ relationì€, ë‹¤ìŒê³¼ ê°™ì€ ìˆ˜ì‹ìœ¼ë¡œ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.

```math
\Psi (s_1, s_2, \cdots , s_n) = (||s_1 - s_2||_{2}^{2}, ||s_1 - s_3||_{2}^{2}, \cdots , ||s_1 - s_n||_{2}^{2}, \cdots , ||s_{n-1} - s_n||_{2}^{2} )
```

---

## 10.4 Distillation Schemes

> [Knowledge Distillation: A Survey ë…¼ë¬¸(2020)](https://arxiv.org/abs/2006.05525)

êµì‚¬ì™€ í•™ìƒì´ ë™ì‹œì— í•™ìŠµë˜ëŠ”ì§€ ì—¬ë¶€ì— ë”°ë¼ì„œ, KDë¥¼ ì„¸ ê°€ì§€ ë²”ì£¼ë¡œ ë¶„ë¥˜í•  ìˆ˜ ìˆë‹¤.

| Offline Distillation | Online Distillation | Self-Distillation |
| :---: | :---: | :---: |
| ![offine](images/kd_schemes_1.png) | ![offine](images/kd_schemes_2.png) | ![offine](images/kd_schemes_3.png) |

> ë¹¨ê°„ìƒ‰: Pre-trained, ë…¸ë€ìƒ‰: To be trained

---

### 10.4.1 Self Distillation

> [Born Again Neural Networks ë…¼ë¬¸(2018)](https://arxiv.org/abs/1805.04770)

ìœ„ ë…¼ë¬¸ì—ì„œëŠ” $k$ stagesë¡œ í•™ìŠµ ê³¼ì •ì„ ë‚˜ëˆ„ê³ , ë§¤ stageì—ì„œ ë™ì¼í•œ êµ¬ì¡°ì˜ ëª¨ë¸ì„ í•™ìŠµí•œë‹¤.

![self-distillation](images/self-distillation.png)

- ì´ì „ stageì—ì„œ íšë“í•œ ëª¨ë¸ì„ êµì‚¬ë¡œ í•˜ì—¬, iterativeí•˜ê²Œ í•™ìŠµì„ ì§„í–‰í•œë‹¤.

- Network Architecture

$$ T = S_1 = S_2 = ... = S_k $$

- Accuracy
  
$$ T < S_1 < S_2 < ... < S_k $$

- ê° stage modelì˜ ì˜ˆì¸¡ì„ **ensemble**í•˜ì—¬, ì˜ˆì¸¡ ì •í™•ë„ë¥¼ ì¶”ê°€ë¡œ ë†’ì¼ ìˆ˜ ìˆë‹¤.

---

### 10.4.2 Online Distillation

> [Deep Mutual Learning ë…¼ë¬¸(2018)](https://arxiv.org/abs/1706.00384)

Online Distillationì—ì„œëŠ”, êµì‚¬ì™€ í•™ìƒì„ ë™ì‹œì— **from scratch**ë¡œ í•™ìŠµí•œë‹¤.

![Deep Mutual Learning](images/deep_mutual_learning.png)

- êµì‚¬ì™€ í•™ìƒì˜ output distribution ì°¨ì´ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµì´ ì§„í–‰ëœë‹¤.

$$ L(S) = \mathrm{CrossEntropy}(S(I), y) + KL(S(I), T(I)) $$

$$ L(T) = \mathrm{CrossEntropy}(T(I), y) + KL(T(I), S(I)) $$

- (+) pretrained teacher $T$ ê°€ í•„ìš”í•˜ì§€ ì•Šë‹¤.

- (+) ë‘ ëª¨ë¸ì´ ê°™ì€ êµ¬ì¡°ë¥¼ ê°€ì ¸ë„ ì ìš©í•  ìˆ˜ ìˆë‹¤. ( $S = T$ )

- (-) ë™ì‹œì— í•™ìŠµí•˜ëŠ” í•™ìƒì´ ë§ì„ìˆ˜ë¡, í•™ìŠµ ìì›ë„ ë§ì´ ì†Œëª¨ëœë‹¤.

---

### 10.4.3 Combining Online and Self-Distillation

---

#### 10.4.3.1 ONE: Multiple Branches + Ensemble

> [Knowledge Distillation by On-the-Fly Native Ensemble ë…¼ë¬¸(2018)](https://arxiv.org/abs/1806.04606)

Online Distillationì€ ì—¬ëŸ¬ í•™ìƒì„ í•™ìŠµí•˜ë©´ì„œ í•™ìŠµ ìì›ì„ ë§ì´ ì†Œëª¨í•˜ê³ , ë™ì‹œë‹¤ë°œì ì´ê³  ë³µì¡í•œ backpropagation ì•Œê³ ë¦¬ì¦˜ì„ ê°–ëŠ”ë‹¤. ONE ë…¼ë¬¸ì—ì„œëŠ” í•´ë‹¹ ë¬¸ì œë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´, ë‹¤ì–‘í•œ branchë¥¼ ê°–ëŠ” ë‹¨ì¼ ëª¨ë¸ì—ì„œ Distillationì„ êµ¬í˜„í–ˆë‹¤.

![ONE](images/ONE.png)

- **Gate**: ê° branchì˜ ì¶œë ¥ logitsì„ ensembleí•œë‹¤.

---

#### 10.4.3.2 Be Your Own Teacher: Deep Supervision + Distillation

> [Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation ë…¼ë¬¸(2019)](https://arxiv.org/abs/1905.08094)

ìœ„ ë…¼ë¬¸ì—ì„œëŠ”, ì¶œë ¥ë¶€ì— ê°€ê¹Œìš´ ë ˆì´ì–´(deeper layer)ì˜ ì§€ì‹ì„, ì…ë ¥ë¶€ì™€ ê°€ê¹Œìš´ ë ˆì´ì–´(shallower layer)ë¡œ ì „ë‹¬í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ Distillationì„ êµ¬í˜„í–ˆë‹¤.


![BYOT](images/BYOT.png)

- ì˜ˆë¥¼ ë“¤ì–´, ê¹Šì€ ë ˆì´ì–´(ì˜ˆ: ResBlock4)ì—ì„œ, ë” ì–•ì€ ë ˆì´ì–´(ResBlock 3)ì—ê²Œ ì§€ì‹ì´ ì „ë‹¬ëœë‹¤.

  > ì„œë¡œ ë‹¤ë¥¸ ì±„ë„ ìˆ˜: 1x1 convë¥¼ ì´ìš©í•´ ë§ì¶˜ë‹¤.

---