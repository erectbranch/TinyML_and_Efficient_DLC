# Lecture 06 - Quantization (Part II)

> [Lecture 06 - Quantization (Part II) | MIT 6.S965](https://youtu.be/3nqUFSSJYKQ)

> [Neural Network Quantization Technique - Post Training Quantization](https://medium.com/mbeddedwithai/neural-network-quantization-technique-post-training-quantization-ff747ed9aa95)

---

## 6.6 Quantization-Aware Training(QAT)

**Quantization-Aware Training**(QAT)ì´ë€ training í˜¹ì€ re-training(fine-tuning)ì„ í†µí•´, ìµœì ì˜ quantization schemeì„ ì°¾ëŠ” ë°©ë²•ì´ë‹¤.

| | PTQ | QAT |
| :---: | :---: | :---: |
| ì†ë„ | ëŒ€ì²´ë¡œ ë¹ ë¥´ë‹¤ | ëŠë¦¬ë‹¤ |
| re-training | ë¶ˆí•„ìš” | í•„ìš” |
| Plug and Play | ê°€ëŠ¥ | re-training í•„ìš” |
| accuracy drop | ì¡°ì ˆí•˜ê¸° í˜ë“¤ë‹¤. | ì–´ëŠ ì •ë„ ì¡°ì ˆí•  ìˆ˜ ìˆë‹¤. |

> Plug and Play(PnP): ë³„ë‹¤ë¥¸ ì„¤ì •ì—†ì´ ì ìš© ê°€ëŠ¥

---

### 6.6.1 Simulated/Fake Quantization

ëª¨ë¸ì˜ í•™ìŠµ ê³¼ì •ì—ì„œ, ì–‘ìí™” í›„ ì •í™•ë„ë¥¼ ë¯¸ë¦¬ í™•ì¸í•˜ëŠ” simulationì„ í†µí•´ ìµœì ì˜ ì–‘ìí™” ì •ì±…ì„ ê²°ì •í•  ìˆ˜ ìˆë‹¤. (**simulated quantization** = **fake quantization**)

- í›ˆë ¨ ì¤‘ weight Wì˜ full precision copyëŠ” ìœ ì§€ëœë‹¤.

- ì •ìˆ˜ ê°’ì„ ì‚¬ìš©í•˜ë©´ í¬ì°©í•˜ì§€ ëª»í•  small gradientë¥¼ ë¬¸ì œ ì—†ì´ ë°˜ì˜í•  ìˆ˜ ìˆë‹¤.

- í›ˆë ¨ í›„ ì‹¤ì œ ì¶”ë¡ ì—ì„œëŠ” quantized weightë¥¼ ì‚¬ìš©í•œë‹¤.

ë‹¤ìŒì€ simulated quantization ë°©ë²•ì„ í†µí•´, linear quantization ê¸°ë°˜ weight, activation quantizationì„ ìˆ˜í–‰í•˜ëŠ” ì—°ì‚° ê·¸ë˜í”„ë‹¤.

![linear quantization forward, backward](images/linear_forward_backward.png)

- Weight quantization

$$ W \rightarrow S_{W}(q_{W} - Z_{W}) = Q(W) $$

- Activation quantization

$$ Y \rightarrow S_{Y}(q_{Y} - Z_{Y}) = Q(Y) $$

---

### 6.6.2 Straight-Through Estimator(STE)

> [Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation ë…¼ë¬¸(2013)](https://arxiv.org/abs/1308.3432)

> [UNDERSTANDING STRAIGHT-THROUGH ESTIMATOR IN TRAINING ACTIVATION QUANTIZED NEURAL NETS ë…¼ë¬¸(2019)](https://arxiv.org/abs/1903.05662)

í•˜ì§€ë§Œ ì–‘ìí™”ëœ ê°’ì€ discreteí•˜ë¯€ë¡œ, ê±°ì˜ ëŒ€ë¶€ë¶„ì˜ ê²½ìš°ì—ì„œ ë¯¸ë¶„ ê°’ì€ 0ì´ ëœë‹¤.

$$ {{\partial Q(W)} \over {\partial W}} = 0 $$

ë”°ë¼ì„œ ë‹¤ìŒê³¼ ê°™ì´ gradient update ê³¼ì • ì—­ì‹œ ë¶ˆê°€ëŠ¥í•˜ê²Œ ëœë‹¤.

$$ g_{W} = {{\partial L} \over {\partial W}} = {{\partial L} \over {\partial Q(W)}} \cdot {{\partial Q(W)} \over {\partial W}} = 0 $$

ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë“±ì¥í•œ ë°©ë²•ì´ **Straight-Through Estimator**(STE)ì´ë‹¤.

- ${{\partial Q(W)}/{\partial W}}$ deriativeë¥¼ ë¬´ì‹œí•˜ê³ , identity functionì²˜ëŸ¼ ê³ ë ¤í•œë‹¤. 

- ë”°ë¼ì„œ quantized weightë§Œìœ¼ë¡œ ë°”ë¡œ gradient updateë¥¼ ìˆ˜í–‰í•œë‹¤.

$$ g_{W} = {{\partial L} \over {\partial W}} = {{\partial L} \over {\partial Q(W)}} $$

---

### 6.6.3 INT8 Linear Quantization-Aware Training

> [Quantizing deep convolutional networks for efficient inference: A whitepaper ë…¼ë¬¸(2017)](https://arxiv.org/abs/1806.08342)

ì‘ì€ ëª¨ë¸ì—ì„œëŠ” INT8 linear QAT ë°©ë²•ì´, PTQë³´ë‹¤ í›¨ì”¬ ë” ì¢‹ì€ ì •í™•ë„ë¥¼ ê°–ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.

![QAT mobilenet](images/QAT_mobilenet.png)

---

## 6.7 Binary Quantization

**Binary Quantization**ì€ ë” ë‚˜ì•„ê°€ 1 bitë§Œì„ ì‚¬ìš©í•˜ëŠ” ì–‘ìí™” ë°©ì‹ì´ë‹¤. 

- storage: binary weights

- computation: bit operation

real number weight ì—°ì‚°ê³¼, binary quantizated weight ì—°ì‚°ì„ ë¹„êµí•´ ë³´ì.

| baseline | Real Number Weights | Binary Quantized Weights |
| :---: | :---: | :---: |
| ![binary quantization ex 1](https://github.com/erectbranch/TinyML_and_Efficient_DLC/blob/master/2022/lec06/summary02/images/binary_ex_1.png) | ![binary quantization ex 2](https://github.com/erectbranch/TinyML_and_Efficient_DLC/blob/master/2022/lec06/summary02/images/binary_ex_2.png) | ![binary quantization ex 3](https://github.com/erectbranch/TinyML_and_Efficient_DLC/blob/master/2022/lec06/summary02/images/binary_ex_3.png) |
| $y_i = \sum_{j}{W_{ij} \cdot x_{j}}$ | $8 \times 5 + (-3) \times 2 + 5 \times 0 + (-1) \times 1$ | $5 - 2 + 0 - 1$ |
| operation | + x | + - |
| memory | 1x | ~32x less |
| computation | 1x | ~2x less |

**Binarization**ì€ í¬ê²Œ deterministic binarization, stochastic binarizaion ë°©ë²•ìœ¼ë¡œ ë‚˜ë‰œë‹¤.

- **Deterministic Binarization**

    ì •í•´ë‘” threshold ì´ìƒì¸ ê°’ì€ 1, ë¯¸ë§Œì¸ ê°’ì€ -1ë¡œ ì–‘ìí™”í•œë‹¤.(sign function)

```math
q = sign(r) = \begin{cases} +1, & r \ge 0 \\ -1, & r < 0 \end{cases}
```

- **Stochastic Binarization**

    global statistics í˜¹ì€ input dataë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì–‘ìí™” ì‹œ -1, +1 probabilityë¥¼ ê²°ì •í•œë‹¤.

---

### 6.7.1 Binarize the weights

> [BinaryConnect: Training Deep Neural Networks with binary weights during propagations ë…¼ë¬¸(2015)](https://arxiv.org/abs/1511.00363)

ëŒ€í‘œì ìœ¼ë¡œ BinaryConnection ë…¼ë¬¸ì€ ë‹¤ìŒê³¼ ê°™ì´ ì–‘ìí™”í•œë‹¤.
 
- discreteí•œ ì •ë„ë¥¼ averagingí•  ìˆ˜ ìˆë„ë¡, **hard sigmoid**ë¥¼ ì´ìš©í•´ì„œ probabilityë¥¼ ê²°ì •í•œë‹¤.( $\sigma (r)$ ) 

  ![hard sigmoid](https://github.com/erectbranch/TinyML_and_Efficient_DLC/blob/master/2022/lec06/summary02/images/hard_sigmoid.png)

```math
q = \begin{cases} +1, & with \, probability \, p = \sigma(r) \\ -1, & with \, probability \, 1 - p \end{cases}
```

$$ \sigma (r) = \min (\max ({{r+1} \over {2}}), 1) $$

- (-) í•˜ì§€ë§Œ ì •í™•ë„ ì†ì‹¤ì´ í¬ë‹¤. (ImageNet ëŒ€ìƒ AlexNet Top-1 accuracy: -21.2%p)

- (-) í•˜ë“œì›¨ì–´ì—ì„œ probability ê³„ì‚°ì„ ìœ„í•œ í•¨ìˆ˜ë¥¼ ì§€ì›í•˜ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤.

---

### 6.7.2 Binarize the weights with scaling factor

> [XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks ë…¼ë¬¸(2016)](https://arxiv.org/abs/1603.05279)

XNOR-Net ë…¼ë¬¸ì—ì„œëŠ” weight binarizationì— ë”°ë¥¸ ì •í™•ë„ë¥¼ íšŒë³µí•˜ê¸° ìœ„í•´, binarized weight tensor(`fp32`)ì— FP32 scaling factorë¥¼ ì¶”ê°€í•œë‹¤.

$$ W \approx \alpha W^{\mathbb{B}} $$

| weights<br/>(32bit float) | BinaryConnection | XNOR-Net |
| :---: | :---: | :---: |
| ![fp32 weight ex](images/binarize_ex_1.png) | ![binary weight ex](images/binarize_ex_2.png) | ![XNOR-Net weight ex](images/binarize_ex_3.png) |

ì´ëŸ¬í•œ scaling factor $\alpha$ ëŠ”, ì–‘ìí™” ì „ ê°€ì¤‘ì¹˜ í–‰ë ¬ ì›ì†Œ(FP32)ì˜ ì ˆëŒ€ê°’ í‰ê· ì„ ì‚¬ìš©í•œë‹¤.

$$ \alpha = {1 \over n}||W||_1 $$

---

### 6.7.3 Binarize the weights and activations

í˜¹ì€ weightê³¼ activationì— ëª¨ë‘ binary quantizationì„ ì ìš©í•  ìˆ˜ ìˆë‹¤. ì´ ê²½ìš° ëª¨ë“  ì—°ì‚°ì„ XNOR ì—°ì‚°ìœ¼ë¡œ ëŒ€ì²´í•  ìˆ˜ ìˆë‹¤. ê°€ë ¹ ë‹¤ìŒê³¼ ê°™ì€ ì—°ì‚°ì„ ìƒê°í•´ ë³´ì.

$$ y_i = \sum_{j}{W_{ij} \cdot x_{j}} $$

- ì—°ì‚°ì˜ ê°€ëŠ¥í•œ ê²½ìš°ì˜ ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

    | W | X | Y=WX |
    | :---: | :---: | :---: |
    | 1 | 1 | 1 |
    | 1 | -1 | -1 |
    | -1 | -1 | 1 |
    | -1 | 1 | -1 |

- -1 ëŒ€ì‹  0ìœ¼ë¡œ ë°”ê¾¸ë©´, XNOR ì§„ë¦¬í‘œì™€ ì™„ì „íˆ ì¼ì¹˜í•œë‹¤.

    | $b_w$ | $b_x$ | XNOR( $b_w, b_x$ ) |
    | :---: | :---: | :---: |
    | 1 | 1 | 1 |
    | 1 | 0 | 0 |
    | 0 | 0 | 1 |
    | 0 | 1 | 0 |

ì•ì„  ì˜ˆì‹œì™€ ë¹„êµ ì‹œ, ë©”ëª¨ë¦¬ëŠ” 32ë°° ì´ìƒ, ì—°ì‚°ì€ 58ë°° ì´ìƒ ì¤„ì–´ë“ ë‹¤.

| input | weight | operations | memory | computation |
| :---: | :---: | :---: | :---: | :---: |
| $\mathbb{R}$ | $\mathbb{R}$ | + x | 1x | 1x |
| $\mathbb{R}$ | $\mathbb{B}$ | + - | ~32x less | ~2x less |
| $\mathbb{B}$ | $\mathbb{B}$ | xnor, popcount | ~32x less | ~58x less |

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ 1: Binary Weight Quantization &nbsp;&nbsp;&nbsp;</span>

ë‹¤ìŒ í–‰ë ¬ ì—°ì‚°ì„ xnor ì—°ì‚°ìœ¼ë¡œ ê³„ì‚°í•˜ë¼.

![binary quantization ex 4](images/binary_ex_4.png)

### <span style='background-color: #C2B2B2; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ” í’€ì´&nbsp;&nbsp;&nbsp;</span>

$$ = 1 \times 1 + (-1) \times 1 + 1 \times (-1) + (-1) \times 1 = -2 $$

$$ = 1 \, \mathrm{xnor} \, 1 + 0 \, \mathrm{xnor} \, 1 + 1  \, \mathrm{xnor} \, 0 + 0 \, \mathrm{xnor} \, 1 = 1 $$

í•˜ì§€ë§Œ ë‹¨ìˆœíˆ -1ì„ 0ìœ¼ë¡œ ì¹˜í™˜í•´ì„œëŠ” ë‹¤ë¥¸ ê²°ê³¼ê°€ ë‚˜ì˜¤ê²Œ ëœë‹¤. ë”°ë¼ì„œ ë‹¤ìŒê³¼ ê°™ì€ ë³´ì •ì„ ê±°ì³ì•¼ í•œë‹¤.

- 1: ë³¸ë˜ -1, 1ì˜ ì°¨ì´ì¸ 2ë¥¼, 1ì˜ ê°œìˆ˜ë§Œí¼ ë”í•´ì¤€ë‹¤.

- 0: -1ì„ ì¹˜í™˜í•œ ê°’ì´ë¯€ë¡œ, 0ì˜ ê°œìˆ˜ë§Œí¼ -1ì„ ë”í•´ì¤€ë‹¤.

$$ -4 + 2 \times (1 + 0 + 0 + 0) = -2 $$

ìœ„ ì‹ì„ ë‹¤ìŒê³¼ ê°™ì´ ì¼ë°˜í™”í•  ìˆ˜ ìˆë‹¤.

$$ y_i = -n + 2 \cdot \sum_{j} W_{ij} \, \mathrm{xnor} \, x_j $$

ì´ëŠ” ì›ì†Œ ì¤‘ 1ì˜ ê°œìˆ˜ê°€ ì–¼ë§ˆë‚˜ ìˆëŠ”ì§€ë¥¼ íŒŒì•…í•˜ëŠ”, **popcount** ì—°ì‚°ìœ¼ë¡œ ëŒ€ì‹ í•  ìˆ˜ ìˆë‹¤.

$$ y_i = -n + \mathrm{popcount}(W_i \, \mathrm{xnor} \, x) \ll 1$$

---

### 6.7.3 Accuracy Degradation of Binarization

ë‹¤ìŒì€ ë‹¤ì–‘í•œ binary quantizationì— ë”°ë¥¸ ì •í™•ë„ ë³€í™”ë¥¼ ì •ë¦¬í•œ ë„í‘œì´ë‹¤.

- BWN: scaling factorë¥¼ ì‚¬ìš©í•˜ëŠ” **Binary Weight Network**

- BNN: scale factorsë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” **Binarized Neural Network** 

![binary accuracy degradation](images/binary_network_acc.png)

---

## 6.8 Ternary Quantization

> [Ternary Weight Networks ë…¼ë¬¸(2016)](https://arxiv.org/abs/1605.04711)

1bitë§Œì„ ì‚¬ìš©í•˜ëŠ” binary quantizationë³´ë‹¤ ì •í™•ë„ë¥¼ ë³´ì¡´í•˜ê¸° ìœ„í•´, 2bit(-1, 0, 1)ë¥¼ ì‚¬ìš©í•œ quantization **Ternary Weight Networks**(TWN)ê°€ ë“±ì¥í–ˆë‹¤.

```math
q = \begin{cases} r_t, & r > \triangle \\ 0, & |r| \le \triangle \\ -r_t, & r < - \triangle \end{cases}
```

- $\triangle = 0.7 \times \mathbb{E}(|r|)$

- $r_t = \mathbb{E_{|r| > \triangle}}(r)$

ì•ì„  ì˜ˆì‹œì— ternary quantizationì„ ì ìš©í•´ ë³´ì.

- threshold $\triangle$

$$\triangle = 0.7 \times {{1}\over{16}}||W||_1 = 0.73$$

- scaling factor $r_t$

  non-zero ê°’ì„ ë°”íƒ•ìœ¼ë¡œ l1 normì„ ê³„ì‚°í•œë‹¤.

$$ {{1} \over {11}}||W_{W^T \neq 0} ||_1 = 1.5 $$

| weights $W$ <br/>(32bit float) | ternary weights $W^T$ <br/>(2bit) |
| :---: | :---: |
| ![fp32 weight ex](https://github.com/erectbranch/TinyML_and_Efficient_DLC/blob/master/2022/lec06/summary02/images/binarize_ex_1.png) | ![ternary weight ex](https://github.com/erectbranch/TinyML_and_Efficient_DLC/blob/master/2022/lec06/summary02/images/ternary_ex_2.png) |

---

### 6.8.1 Trained Ternary Quantization(TTQ)

> [Trained Ternary Quantization ë…¼ë¬¸(2016)](https://arxiv.org/abs/1612.01064)

Trained Ternary Quantization(TTQ) ë…¼ë¬¸ì—ì„œëŠ”, ê¸°ì¡´ì˜ -1, 0, 1 ëŒ€ì‹  í›ˆë ¨ ê°€ëŠ¥í•œ $w_{p}$ , $w_{n}$ íŒ¨ëŸ¬ë¯¸í„°ë¥¼ ë„ì…í•œ ternary quantizationì„ ì ìš©í•œë‹¤.

![TTQ](images/TTQ.png)

1. full precision ê°€ì¤‘ì¹˜ë¥¼ [-1, 1] ì‚¬ì´ë¡œ normalizeí•œë‹¤.

2. thresholdì— ë”°ë¼ -1, 0, 1ë¡œ ternary quantizationì„ ìˆ˜í–‰í•œë‹¤.

3. scale parameter $w_n$ , $w_p$ ë¥¼ í›ˆë ¨í•œë‹¤.

```math
q = \begin{cases} w_p, & r > \triangle \\ 0, & |r| \le \triangle \\ -w_p, & r < - \triangle \end{cases}
```

ë‹¤ìŒì€ ImageNet ëŒ€ìƒ ResNet-18 ëª¨ë¸ì—ì„œ, full precision, BWN, TWN ê°ê°ì˜ Top-1 accuracyë¥¼ ë¹„êµí•œ ë„í‘œë‹¤.

| ImageNet Top-1</br>Accuracy | Full Precision | BWN(1 bit) | TWN(2 bit) | TTQ |
| :---: | :---: | :---: | :---: | :---: |
| ResNet-18 | 69.6 | 60.8 | 65.3 | 66.7 |

---