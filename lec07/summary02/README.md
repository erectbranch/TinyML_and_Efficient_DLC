# 7 Neural Architecture Search (Part I)

> [Lecture 07 - Neural Architecture Search (Part I) | MIT 6.S965](https://www.youtube.com/watch?v=NQj5TkqX48Q)

> [Exploring the Implementation of Network Architecture Search(NAS) for TinyML Application](http://essay.utwente.nl/89778/1/nieuwenhuis_MA_EEMCS.pdf)

ë‹¤ìŒì€ 7.2ì ˆ benchmarkì— NASë¥¼ ì´ìš©í•´ ì°¾ì•„ë‚¸ neural architectureë¥¼ ì¶”ê°€í•œ ê²ƒì´ë‹¤. 

- NASë¥¼ ì´ìš©í•´ ì°¾ì•„ë‚¸ neural architectureëŠ” í›¨ì”¬ ì ì€ ì—°ì‚°ëŸ‰(MACs)ìœ¼ë¡œë„ manually-designed modelë³´ë‹¤ ë” ë†’ì€ accuracyë¥¼ ê°–ëŠ”ë‹¤.

![automatic design](images/automatic_design.png)

---

## 7.3 Neural Architecture Search

**Neural Architecture Search**(NAS)ì˜ ëª©í‘œëŠ” **Search Space**(íƒìƒ‰ ê³µê°„)ì—ì„œ íŠ¹ì • ì „ëµì„ í†µí•´ ìµœì ì˜ neural network architectureë¥¼ ì°¾ëŠ” ê²ƒì´ë‹¤.

> ëª©í‘œë¡œ í•˜ëŠ” ì„±ëŠ¥ì€ accuracy, efficiency, latency ë“±ì´ ë  ìˆ˜ ìˆë‹¤.

![NAS](images/NAS.png)

- **Search Space**(íƒìƒ‰ ê³µê°„) 

  íƒìƒ‰í•  neural network architectureì´ ì •ì˜ëœ ê³µê°„ì´ë‹¤.

  > ì ì ˆí•œ domain ì§€ì‹ì„ ì ‘ëª©í•˜ë©´ search space í¬ê¸°ë¥¼ ì¤„ì¼ ìˆ˜ ìˆë‹¤. í•™ìŠµ ì‹œê°„ì˜ ë‹¨ì¶•ê³¼ accuracy í–¥ìƒì— ë„ì›€ì´ ëœë‹¤.

- **Search Strategy**(íƒìƒ‰ ì „ëµ)

  search spaceë¥¼ ì–´ë–»ê²Œ íƒìƒ‰í• ì§€ ê²°ì •í•œë‹¤. 

- **Performance Estimation Strategy**(ì„±ê³¼ í‰ê°€ ì „ëµ)

  ì„±ëŠ¥ì„ ì¸¡ì •í•  ë°©ë²•ì„ ê²°ì •í•œë‹¤. 

---

## 7.4 Search Space

ëª¨ë¸ êµ¬ì¡°ë¥¼ ì°¾ê¸° ìœ„í•´ì„œëŠ” search spaceë¥¼ ë¨¼ì € ì •ì˜í•´ì•¼ í•œë‹¤. search space ë‚´ ëª¨ë¸ì€ ë‹¤ì–‘í•œ ì˜µì…˜ì„ ê°€ì§€ê²Œ ëœë‹¤. ì•„ë˜ëŠ” ëŒ€í‘œì ì¸ ì˜ˆì‹œë‹¤.

- ë‹¤ì–‘í•œ ë ˆì´ì–´

  convolution layer, fully connected layer, pooling layer ë“±

- ë ˆì´ì–´ê°€ ê°–ëŠ” íŠ¹ì§•

  \#neurons, kernel size, activation function ë“± 

ë˜í•œ search spaceë¥¼ ì–´ë–¤ ë‹¨ìœ„ë¡œ êµ¬ì„±í•˜ëŠ”ê°€ì— ë”°ë¼, í¬ê²Œ **cell-level search space**ì™€ **network-level search space**ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤.

---

### 7.4.1 Cell-level Search Space

> [Learning Transferable Architectures for Scalable Image Recognition ë…¼ë¬¸(2017)](https://arxiv.org/abs/1707.07012)

ImageNet ë°ì´í„°ì…‹ì„ ì…ë ¥ìœ¼ë¡œ ë°›ëŠ” CNN êµ¬ì¡° ì˜ˆì‹œë¥¼ ë³´ì.

![ImageNet dataset arch ex](images/classifier_architecture_ex.png)

- Reduction Cell: í•´ìƒë„ë¥¼ ì¤„ì¸ë‹¤.(stride > 2)

- Normal Cell: í•´ìƒë„ê°€ ìœ ì§€ëœë‹¤.(stride = 1)

ë…¼ë¬¸(NASNet)ì—ì„œëŠ” RNN controllerë¥¼ ì´ìš©í•˜ì—¬ candidate cellë¥¼ ìƒì„±í•œë‹¤. ì´ ë‹¤ì„¯ ë‹¨ê³„ë¡œ êµ¬ì„±ëœ ë‹¨ê³„ë¥¼ ì„¤ëª…í•˜ëŠ” ì•„ë˜ ê·¸ë¦¼ì„ ì‚´í´ë³´ì.

![cell-level search space](images/cell-level_search_space.png)

- $h_{i+1}$ ì„ ë§Œë“¤ê¸° ìœ„í•œ input candidate: $h_i$ hidden layer í˜¹ì€ $h_{i-1}$ hidden layerë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

1. hidden layer A: ì²« ë²ˆì§¸ hidden stateë¥¼ ìƒì„±í•œë‹¤.

2. hidden layer B: ë‘ ë²ˆì§¸ hidden stateë¥¼ ìƒì„±í•œë‹¤.

3. hidden layer Aê°€ ê°€ì§ˆ operationì„ ê³ ë¥¸ë‹¤.(`3x3 conv`)

   > ì˜ˆë¥¼ ë“¤ë©´ convolution/pooling/identityì™€ ê°™ì€ ë ˆì´ì–´ê°€ ë  ìˆ˜ ìˆë‹¤.

4. hidden layer Bê°€ ê°€ì§ˆ operationì„ ê³ ë¥¸ë‹¤.(`2x2 maxpool`)

5. Aì™€ Bë¥¼ í•©ì¹  ë°©ë²•ì„ ê³ ë¥¸ë‹¤.(`add`)

ë‹¤ìŒì€ NASNetì„ ì„¤ëª…í•˜ëŠ” ë„ì‹ì´ë‹¤.

![NASNet ex](images/NASNet_ex.png)

í•˜ì§€ë§Œ ë„ˆë¬´ í° search spaceë¡œ ì¸í•´, **search cost**ì™€ **hardware efficiency**ë©´ì—ì„œ ë” íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ í•„ìš”í•´ì¡Œë‹¤.

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ 1: Cell-level Search Space size &nbsp;&nbsp;&nbsp;</span>

ë‹¤ìŒ ì¡°ê±´ì—ì„œ NASNetì˜ search space sizeë¥¼ êµ¬í•˜ë¼.

- 2 candidate input

- M input transform operations

- N combine hidden states operations

- B \#layers

### <span style='background-color: #C2B2B2; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ” í’€ì´&nbsp;&nbsp;&nbsp;</span>

search space sizeëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

$$(2 \times 2 \times M \times M \times N)^{B} = 4^{B}M^{2B}N^{B}$$

> $M=5, N=2, B=5$ ë¼ê³  í•˜ë©´ search space í¬ê¸°ëŠ” $3.2 \times 10^{11}$ ì´ ëœë‹¤.

---

### 7.4.2 Network-Level Search Space

> [MnasNet: Platform-Aware Neural Architecture Search for Mobile ë…¼ë¬¸(2018)](https://arxiv.org/abs/1807.11626)

> [Once-for-All: Train One Network and Specialize it for Efficient Deployment ë…¼ë¬¸(2019)](https://arxiv.org/abs/1908.09791)

**network-level search space**ì€ í”íˆ ì“°ì´ëŠ” patternì€ ê³ ì •í•˜ê³ , ì˜¤ì§ ê° stageê°€ ê°–ëŠ” blocksì„ ë³€í™”ì‹œí‚¤ëŠ” ë°©ë²•ì´ë‹¤.

---

#### 7.4.2.1 Network-Level Search Space for Image Segmantation

> [Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation ë…¼ë¬¸(2019)](https://arxiv.org/abs/1901.02985)

**Image Segmentation** domainì—ì„œ network-level search spaceë¥¼ êµ¬ì„±í•œ Auto-DeepLab ë…¼ë¬¸ì—ì„œëŠ”, layerë³„ upsampling/downsamplingì„ search spaceë¡œ ì‚¬ìš©í•œë‹¤. 

> Image Segmentation: imageë¥¼ ì—¬ëŸ¬ í”½ì…€ ì§‘í•©ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” task

![network-level search space](images/network-level_search_space_ex.png)

- ê°€ë¡œ: \#layers, ì„¸ë¡œ Downsample(í•´ìƒë„ê°€ ì¤„ì–´ë“ ë‹¤.)

- íŒŒë€ìƒ‰ nodesë¥¼ ì—°ê²°í•˜ëŠ” pathê°€ candidate architectureê°€ ëœë‹¤.

---

#### 7.4.2.2 Network-Level Search Space for Object Detection

> [NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection](https://arxiv.org/abs/1904.07392)

NAS-FPN ë…¼ë¬¸ì—ì„œëŠ” **Object Detection** domainì—ì„œ í”íˆ ì“°ì´ëŠ” FPN(Feature Pyramid Networks for Object Detection) ëª¨ë¸ì„ Network-Level Search Spaceë¡œ êµ¬ì„±í•œë‹¤.

![NAS-FPN](images/NAS-FPN.png)

> AP: average precision(í‰ê·  ì •ë°€ë„)

ì´ì²˜ëŸ¼ NASë¥¼ í†µí•´ ì–»ì€ ìµœì  network architectureëŠ” ì‚¬ëŒì´ ë””ìì¸í•œ êµ¬ì¡°ì™€ ìƒë‹¹íˆ ë‹¤ë¥¸ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. 

> í•˜ì§€ë§Œ accuracyì™€ irregularity ì‚¬ì´ì—ì„œ ê· í˜•ì„ ë§ì¶°ì•¼ í•œë‹¤. irregularity topologyëŠ” hardwareìƒìœ¼ë¡œ êµ¬í˜„í•˜ê¸° í˜ë“¤ê³ , ë˜í•œ parallelismì˜ ì´ì ì„ ì‚´ë¦¬ê¸° ì–´ë µê¸° ë•Œë¬¸ì´ë‹¤.

---

## 7.5 Design the Search Space

ê·¸ë ‡ë‹¤ë©´ íš¨ìœ¨ì ì¸ search spaceëŠ” ì–´ë–¤ ê¸°ì¤€ìœ¼ë¡œ ì„ íƒí•´ì•¼ í• ê¹Œ?

---

### 7.5.1 Cumulative Error Distribution

> [On Network Design Spaces for Visual Recognition ë…¼ë¬¸(2019)](https://arxiv.org/abs/1905.13214)

ë…¼ë¬¸(RegNet)ì—ì„œëŠ” **cumulative error distribution**ì„ ì§€í‘œë¡œ ì‚¬ìš©í•œë‹¤.

![ResNet cumulative error distribution](images/ResNet_cumulative_error_distribution.png)

- íŒŒë€ìƒ‰ ê³¡ì„ : 38.9% modelì´ 49.4%ê°€ ë„˜ëŠ” errorë¥¼ ê°€ì§„ë‹¤.

- ì£¼í™©ìƒ‰ ê³¡ì„ : 38.7% modelì´ 43.2%ê°€ ë„˜ëŠ” errorë¥¼ ê°€ì§„ë‹¤.

  íŒŒë€ìƒ‰ ê³¡ì„ ë³´ë‹¤ errorê°€ ì ìœ¼ë¯€ë¡œ ë” ìš°ìˆ˜í•œ search spaceì´ë‹¤.

í•˜ì§€ë§Œ cumulative error distributionì„ ì¸¡ì •í•˜ê¸° ìœ„í•´ì„œ êµ‰ì¥íˆ ê¸´ ì‹œê°„ë™ì•ˆ trainingì„ ìˆ˜í–‰í•´ì•¼ í•˜ëŠ” ë‹¨ì ì´ ìˆë‹¤.

---

### 7.5.2 FLOPs distribution

> [MCUNet: Tiny Deep Learning on IoT Devices ë…¼ë¬¸(2020)](https://arxiv.org/abs/2007.10319)

MCUNet ë…¼ë¬¸ì˜ TinyNASëŠ” MCU ì œì•½ì¡°ê±´ì— ì•Œë§ì€ search spaceë¥¼ ì°¾ê¸° ìœ„í•´, ë‹¤ìŒê³¼ ê°™ì€ ìµœì í™” ê³¼ì •ì„ ê±°ì¹œë‹¤.

1. Automated search space optimization

2. Resource-constrained model specialization

![TinyNAS](images/TinyNAS.png)

ì´ëŸ¬í•œ ìµœì í™” ê³¼ì •ì—ì„œ ì œì¼ í•µì‹¬ì´ ë˜ëŠ” heuristicì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

- ë™ì¼í•œ memory constraintì—ì„œëŠ” <U>FLOPsê°€ í´ìˆ˜ë¡ í° model capacityë¥¼ ê°–ëŠ”ë‹¤.</U>

- í° model capacityëŠ” ë†’ì€ accuracyì™€ ì§ê²°ëœë‹¤. 

![FLOPs distribution](images/FLOPs_and_probability.png)

---

## 7.6 Search Strategy

---

### 7.6.1 Grid Search

ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ìœ¼ë¡œ **grid search**ê°€ ìˆë‹¤. ê°„ë‹¨í•œ ì˜ˆì‹œë¡œ ë‹¤ìŒê³¼ ê°™ì´ Widthë‚˜ Resolutionì—ì„œ ëª‡ ê°€ì§€ pointë¥¼ ì§€ì •í•œë‹¤.(width 3ê°œ, resolution 3ê°œë¡œ ì´ 9ê°œì˜ ì¡°í•©ì´ ë‚˜ì˜¨ ì˜ˆë‹¤.)

![grid search ex](images/grid_search_ex.png)

- latency constraintë¥¼ ë§Œì¡±í•˜ë©´ íŒŒë€ìƒ‰, ë§Œì¡±í•˜ì§€ ëª»í•˜ë©´ ë¹¨ê°„ìƒ‰.

í•˜ì§€ë§Œ ì´ëŸ° ê°„ë‹¨í•œ ì˜ˆì‹œì™€ëŠ” ë‹¤ë¥´ê²Œ ì‹¤ì œ ì‘ìš©ì—ì„œëŠ” ì„ íƒì§€ì™€ dimensionì´ í›¨ì”¬ ì»¤ì§€ê²Œ ëœë‹¤. ë²”ìœ„ë¥¼ ë„“ê²Œ, stepì„ ì‘ê²Œ ì„¤ì •í• ìˆ˜ë¡ ìµœì í•´ë¥¼ ì°¾ì„ ê°€ëŠ¥ì„±ì€ ì»¤ì§€ì§€ë§Œ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ê²Œ ëœë‹¤.

> ëŒ€ì²´ë¡œ ë„“ì€ ë²”ìœ„ì™€ í° stepìœ¼ë¡œ ì„¤ì •í•œ ë’¤, ë²”ìœ„ë¥¼ ì¢íˆëŠ” ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤.

---

#### 7.6.1.1 EfficientNet

> [EfficientNet ë…¼ë¬¸](https://arxiv.org/pdf/1905.11946.pdf)

ì´ëŸ° ë°©ë²•ì„ ì‚¬ìš©í•˜ëŠ” modelë¡œ **EfficientNet**ê°€ ìˆë‹¤. ì‚´í´ë³´ê¸° ì•ì„œ modelì—ì„œ **depth**, **width**, **resolution**ì´ ê°ê° ì–´ë–¤ ì—­í• ì„ í•˜ëŠ”ì§€ ì•Œì•„ë³´ì. ë‹¤ìŒì€ ResNetì—ì„œ ì´ ì„¸ ê°€ì§€ ìš”ì¸ì„ ì¡°ì ˆí–ˆì„ ë•Œ ImageNet datasetì˜ accuracyë¥¼ ê¸°ë¡í•œ ë„í‘œë‹¤.

![EfficientNet graph](images/efficientnet_graph.png)

- **depth**( $d$ )

  - $d$ ê°€ ì»¤ì§ˆìˆ˜ë¡ model capacityê°€ ì»¤ì§„ë‹¤.(ë” complexí•œ featureë¥¼ ê°€ì§ˆ ìˆ˜ ìˆë‹¤.)

  - $d$ ê°€ ì»¤ì§ˆìˆ˜ë¡ modelì˜ parameter ìˆ˜ê°€ ë§ì•„ì§„ë‹¤. ë”°ë¼ì„œ memory footprint(ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰)ê°€ ì»¤ì§€ëŠ” ë‹¨ì ì´ ìˆë‹¤.

  - $d$ ê°€ ì»¤ì§ˆìˆ˜ë¡ modelì˜ FLOPsê°€ ë§ì•„ì§€ë©° accuracyë„ ëŠ˜ì–´ë‚˜ì§€ë§Œ, modelì˜ latencyë„ ì»¤ì§„ë‹¤.

  - training ê³¼ì •ì—ì„œ vanishing gradient ë¬¸ì œë¥¼ ê²ªì„ ê°€ëŠ¥ì„±ì´ í¬ë‹¤.(skip connectionì´ë‚˜ batch normalization ë“±ì˜ ë°©ë²•ìœ¼ë¡œ ë°©ì§€)

- **width**( $w$ )

  > width scalingì€ ì£¼ë¡œ small size modelì—ì„œ ì‚¬ìš©í•œë‹¤. ë„“ê¸°ë§Œ í•˜ê³  ì–•ì€ networkë¡œëŠ” high level featureë¥¼ ì–»ê¸° í˜ë“¤ê¸° ë•Œë¬¸ì´ë‹¤.

  - wider networkê°€ ë” fine-grained featureë¥¼ ê°€ì§€ë©° trainingì´ ìš©ì´í•˜ë‹¤.

- **resolution**( $r$ )

  - high resolution input imageë¥¼ ì‚¬ìš©í• ìˆ˜ë¡ ë” fine-grained featureë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.

  > ì˜ˆë¥¼ ë“¤ì–´ 224x224, 299x299ë¥¼ ì‚¬ìš©í•œ ì˜ˆì „ modelê³¼ ë‹¬ë¦¬, 480x480 resolutionì„ ì‚¬ìš©í•œ GPipe modelì´ SOTA accuracyë¥¼ ì–»ì€ ì  ìˆë‹¤. í•˜ì§€ë§Œ ë„ˆë¬´ í° resolutionì€ ë°˜ëŒ€ë¡œ accuracy gainì´ ì¤„ì–´ë“¤ ìˆ˜ ìˆë‹¤.

EfficientNetì€ depth, width, resolutionê°€ ì¼ì • ê°’ ì´ìƒì´ ë˜ë©´ accuracyê°€ ë¹ ë¥´ê²Œ saturateëœë‹¤ëŠ” ì‚¬ì‹¤ì„ ë°”íƒ•ìœ¼ë¡œ, ì´ë“¤ì„ í•¨ê»˜ ê³ ë ¤í•˜ëŠ” **compound scaling** ë°©ë²•ì„ ì œì•ˆí•œë‹¤. 

![compound scaling](images/compound_scaling.png)

EfficientNetì€ ê° layerê°€ ìˆ˜í–‰í•˜ëŠ” ì—°ì‚°(F)ë¥¼ ê³ ì •í•˜ê³ , width, depth, resolutionë§Œì„ ë³€ìˆ˜ë¡œ search spaceë¥¼ íƒìƒ‰í•œë‹¤.

$$ depth = d = {\alpha}^{\phi} $$

$$ width = w = {\beta}^{\phi} $$

$$ resolution = r = {\gamma}^{\phi} $$

- $\phi$ : compound scaling parameter

$$ \underset{d,w,r}{\max} \quad Accuracy(N(d,w,r)) $$

---

### 7.6.2 random search

Grid Searchì˜ ë¬¸ì œë¥¼ ê°œì„ í•œ ë°©ë²•ìœ¼ë¡œ **random search**ê°€ ì œì•ˆë˜ì—ˆë‹¤. random searchëŠ” ì •í•´ì§„ ë²”ìœ„ ë‚´ì—ì„œ ë§ ê·¸ëŒ€ë¡œ ì„ì˜ë¡œ ì„ íƒí•˜ë©° ìˆ˜í–‰í•˜ë©°, grid searchë³´ë‹¤ ìƒëŒ€ì ìœ¼ë¡œ ë” ë¹ ë¥´ê³  íš¨ìœ¨ì ì´ë‹¤.

> random searchëŠ” ì°¨ì›ì´ ì ì„ ë•Œ ìµœì„ ì˜ parameter search strategyì¼ ê°€ëŠ¥ì„±ì´ í¬ë‹¤. 

![grid search, random search](images/grid_random_search.png)

grid searchë³´ë‹¤ ë” íš¨ìœ¨ì ì¸ ì´ìœ ëŠ” ì§ê´€ì ìœ¼ë¡œë„ ì´í•´í•  ìˆ˜ ìˆë‹¤. ì¢…ì¢… ì¼ë¶€ parameterëŠ” ë‹¤ë¥¸ parameterë³´ë‹¤ performanceì— í° ì˜í–¥ì„ ë¯¸ì¹œë‹¤. ê°€ë ¹ modelì´ hyperparameter 1ì— ë§¤ìš° ë¯¼ê°í•˜ê³ , hyperparameter 2ì—ëŠ” ë¯¼ê°í•˜ì§€ ì•Šë‹¤ê³  í•˜ì.

grid searchëŠ” {hyperparameter 1 3ê°œ} * {hyperparameter 2 3ê°œ}ë¥¼ ì‹œë„í•œë‹¤. ë°˜ë©´ random searchì˜ ê²½ìš°ì—ëŠ” hyperparameter 1 9ê°œì˜ ë‹¤ë¥¸ ê°’(í˜¹ì€ hyperparameter 2 9ê°œì˜ ë‹¤ë¥¸ ê°’)ì„ ì‹œë„í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.

![evolution vs random](images/evolution_vs_random.png)

ë˜í•œ Single-Path-One-Shot(SPOS)ì—ì„œëŠ” random searchê°€ ë‹¤ë¥¸ advanceëœ ë°©ë²•ë“¤(ì˜ˆë¥¼ ë“¤ë©´ evolutionary architecture search)ë³´ë‹¤ ì¢‹ì€ baselineì„ ì œê³µí•  ìˆ˜ ìˆë‹¤.

> SPOSë€ ë§ ê·¸ëŒ€ë¡œ single pathì™€ one-shot ì ‘ê·¼ë²•ì„ ì‚¬ìš©í•˜ëŠ” NASì´ë‹¤. 

> one-shot NASëŠ” ëª¨ë“  candidate architectureë¥¼ í¬í•¨í•˜ë©° weightë¥¼ ê³µìœ í•˜ëŠ” **supernet**ì—ì„œ search spaceë¥¼ íƒìƒ‰í•œë‹¤. ë•ë¶„ì— resourceê°€ ëœ í•„ìš”í•˜ë‹¤ëŠ” ë¹„ìš© ì ˆê°ì  ì¥ì ì„ ì§€ë‹Œë‹¤. í•˜ì§€ë§Œ ê° architectureë¥¼ ê°œë³„ì ìœ¼ë¡œ trainí•˜ê³  evaluateí•˜ëŠ” ê¸°ì¡´ NASë³´ë‹¤ëŠ” performanceê°€ ë‚®ë‹¤.

---
 
### 7.6.3 reinforcement learning

> [Introduction to Neural Architecture Search (Reinforcement Learning approach)](https://smartlabai.medium.com/introduction-to-neural-architecture-search-reinforcement-learning-approach-55604772f173)

![RL-based NAS](images/RL-based_NAS.png)

- controller(RNN)ì´ Sample architecture(**child network**)ë¥¼ ìƒì„±í•œë‹¤.

- ë¬¸ìì—´ë¡œ ë‚˜ì˜¨ ì´ child networkë¥¼ trainingí•˜ë©´, validation setì— ëŒ€í•œ **accuracy**ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.

- accuracyë¥¼ ë°”íƒ•ìœ¼ë¡œ controllerì˜ policyë¥¼ updateí•œë‹¤.

> RNN controllerëŠ” tokenì˜ list í˜•íƒœë¡œ child CNN descriptionì˜ hyperparameterë“¤ì„ ìƒì„±í•´ ì¤€ë‹¤. filterì˜ ê°œìˆ˜, ê·¸ filterì˜ height, width, strideì˜ heightë‚˜ layerë‹¹ width ë“±ì´ í¬í•¨ëœë‹¤. ê·¸ë¦¬ê³  ì´ descriptionì´ **softmax classifier**ë¥¼ ê±°ì¹œ ë’¤, child CNNì´ built ë° trainëœë‹¤.(ë”°ë¼ì„œ sample architectureëŠ” ê°ì probability pë¥¼ ê°–ëŠ”ë‹¤.)

> ì´ trainëœ modelì„ validationí•˜ì—¬ ì–»ì€ accuracy( $\theta$ )ë¥¼ ë°”íƒ•ìœ¼ë¡œ controllerë¥¼ updateí•œë‹¤. ë‹¤ìŒ ë²ˆ rewardê°€ ë” ë†’ì€ í–‰ë™(architecture)ë¥¼ ì„ íƒí•˜ê²Œ í•˜ë„ë¡ reward $R$ ë¡œ accuracyë¥¼ ì‚¬ìš©í•œë‹¤.

$$ J({\theta}_{c}) = E_{P(a_{1:T};{\theta}_{c})}[R] $$

- a : **action**. controllerê°€ child networkì˜ hyperparameterë¥¼ í•˜ë‚˜ ì˜ˆì¸¡í•˜ëŠ” ê³¼ì •ì„ actionì´ë¼ê³  ì§€ì¹­í•œë‹¤. ì¦‰, ì—¬ëŸ¬ actionì„ ê±°ì³ í•˜ë‚˜ì˜ child network architectureê°€ ìƒì„±ë˜ëŠ” ê²ƒì´ë‹¤.

  - $a_{1}:T$ : child networkë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ ê±°ì¹œ actionë“¤ì˜ listë¥¼ ì˜ë¯¸í•œë‹¤. 

í•˜ì§€ë§Œ RNN controllerë¡œ ì–»ëŠ” <U>accuracyëŠ” **non-differentiable**</U>í•˜ê¸° ë•Œë¬¸ì—, ë‹¤ìŒê³¼ ê°™ì€ policy gradient methodë¥¼ ì´ìš©í•œë‹¤.

$$ {\nabla}_{{\theta}_{c}}J({\theta}_{c}) = \sum_{t=1}^{T}{E_{P(a_{1:T};{\theta}_{c})}[{\nabla}_{{\theta}_{c}} {\log}P({\alpha}_{t}|{\alpha}_{(t-1):1};{\theta}_{c})R]} $$

> ì‹¤ì œë¡œëŠ” ì´ë¥¼ ë” approximateí•˜ê³  baselineì„ ì¶”ê°€í•œ ì‹ì„ ì‚¬ìš©í•œë‹¤. baselineì€ ì´ì „ architectureë“¤ì˜ í‰ê·  accuracyë¥¼ ì´ìš©í•´ì„œ ê²°ì •í•œë‹¤.

> [Policy Gradient Algorithms](https://talkingaboutme.tistory.com/entry/RL-Policy-Gradient-Algorithms)

---

#### 7.6.3.1 ProxylessNAS

> ProxyëŠ” Differentiable NASê°€ êµ‰ì¥íˆ í° GPU cost(GPU hours, memory)ë¥¼ í•„ìš”ë¡œ í•´ì„œ, ì´ë¥¼ ì¤„ì´ê¸° ìœ„í•´ proxyë¼ëŠ” ì‘ì€ ë‹¨ìœ„ì˜ taskë“¤ë¡œ ë‚˜ëˆ„ì–´ì„œ ìˆ˜í–‰í•˜ë©´ì„œ ìƒê¸´ ê°œë…ì´ë‹¤.

**ProxylessNAS**ì—ì„œëŠ” architecture parameterë“¤ì„ pathê°€ activatedë˜ì—ˆëŠ”ì§€ ì—¬ë¶€ì— ë”°ë¼ 0ê³¼ 1ë¡œ ì´ë£¨ì–´ì§„ binary vectorë¡œ í‘œí˜„í•œë‹¤. ì´ë ‡ê²Œ í•˜ì—¬ architectureë¥¼ ìƒì„±í•˜ëŠ” ê³¼ì •ì´ ë” ê°„ë‹¨í•´ì§€ê³ , ë” ë¹ ë¥´ê²Œ ìˆ˜ë ´í•  ìˆ˜ ìˆë‹¤.

ì–´ë–»ê²Œ ì´ëŸ° í‘œí˜„ì´ ê°€ëŠ¥í•œì§€ ì‚´í´ë³´ì. neural network $\mathcal{N}$ ì´ nê°œì˜ edgeë¥¼ ê°–ëŠ”ë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.

$$ \mathcal{N}(e, \cdots e_{n}) $$

- $e$ : ì¼ë°©í–¥ ê·¸ë˜í”„ì¸ **DAG**(Directed Acyclic Graph)ì—ì„œ edgeë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. 

edgeê°€ ê°–ëŠ” operationì˜ ì§‘í•©ì¸ $O$ ì€ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤. 

- $O = \lbrace {o}_{i} \rbrace$ : $N$ ê°œì˜ ê°€ëŠ¥í•œ operationì˜ ì§‘í•©ì´ë‹¤. operationì˜ ì˜ˆë¡œëŠ” convolution, pooling, fully-connected ë“±ì´ ìˆë‹¤. 

ê·¸ëŸ°ë° ê° edgeë§ˆë‹¤ primitive operationì„ ì„¤ì •í•˜ëŠ” ë°©ë²•ì´ ì•„ë‹ˆë¼, ëª¨ë“  architectureì„ í¬í•¨í•˜ëŠ” over-parameterized networkë¥¼ ìƒì„±í•œë‹¤.

![update parameters](images/update_parameters.png)


ë”°ë¼ì„œ over-parameterized networkëŠ” ê° edgeë§ˆë‹¤ $N$ ê°œì˜ ê°€ëŠ¥í•œ operationì„ ê°€ì ¸ì•¼ í•œë‹¤. ì´ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ mixed operation function $m_{O}$ ë¥¼ ë°˜ì˜í•´ì„œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.

$$ \mathcal{N}(e = {m_{O}^{1}}, \cdots, e_{n} = {m_{O}^{n}}) $$

- One-Shotì—ì„œ $m_{O}$ ëŠ” input $x$ ê°€ ì£¼ì–´ì¡Œì„ ë•Œ $o_{i}(x)$ ë“¤ì˜ ì´í•©ì´ë‹¤.

- DARTSì—ì„œ $m_{O}$ ëŠ” input $x$ ê°€ ì£¼ì–´ì¡Œì„ ë•Œ $o_{i}(x)$ ë“¤ì˜ weighted sum(softmax)ì´ë‹¤.

$$ m_{O}^{One-Shot}(x) = {\sum}_{i=1}^{N}{o_{i}(x)} $$

$$ m_{O}^{DARTS}(x) = {\sum}_{i=1}^{N}{p_{i}o_{i}(x)} = {\sum}_{i=1}^{N}{{\exp({\alpha}_{i})} \over {\sum_{j}{\exp({\alpha}_{j})}}}{o_{i}(x)} $$

> $\lbrace{\alpha}_{i}\rbrace$ : Nê°œì˜ real-valued architecture parameters

í•˜ì§€ë§Œ ì´ì²˜ëŸ¼ ëª¨ë“  operation output ê°’ì„ ë°˜ì˜í•˜ë©´ì„œ trainingí•˜ëŠ” ê²ƒì€ memory usageë¥¼ êµ‰ì¥íˆ ì¡ì•„ë¨¹ê²Œ ëˆë‹¤. ë”°ë¼ì„œ ProxylessNASì—ì„œëŠ” **path binarization** ë°©ë²•ì„ ë„ì…í•´ì„œ memory ë¬¸ì œë¥¼ í•´ê²°í•œë‹¤.

- **binarized path**

ìœ„ ê·¸ë¦¼ì²˜ëŸ¼ ProxylessNASëŠ” **Binary Gate** $g$ ë¥¼ ë„ì…í•˜ì—¬ pathë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. 

![binary gate](images/binary_gate.png)

ì´ binary gatesë¥¼ ë„ì…í•œ mixed operationì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

![mixed operation](images/binary_mixed_operation.png)

ë”°ë¼ì„œ runtimeì— path í•˜ë‚˜ë§Œ ìœ ì§€í•˜ë©´ ë˜ê¸° ë•Œë¬¸ì— memory usageë¥¼ ëŒ€í­ ì¤„ì¼ ìˆ˜ ìˆë‹¤. trainingì€ weight parameterê³¼ architecture parameterë¥¼ ë‚˜ëˆ ì„œ ë”°ë¡œ ì§„í–‰ëœë‹¤.

1. ìš°ì„  weight parameterë¥¼ trainingí•œë‹¤.

    - architecture parameter(path ì„ íƒí™•ë¥ )ì„ freezeì‹œí‚¤ê³ , samplingì„ ë°”íƒ•ìœ¼ë¡œ activeëœ pathì˜ weightë¥¼ updateí•œë‹¤.

    - samplingì€ binary gateì˜ probability( $p_1, \cdots, p_{N}$ )ë¥¼ ë°”íƒ•ìœ¼ë¡œ stochasticalí•˜ê²Œ ìˆ˜í–‰ëœë‹¤.

2. ê·¸ ë‹¤ìŒ architecture parameterë¥¼ trainingí•œë‹¤.

    - weight parameterë¥¼ freezeì‹œí‚¤ê³ , architecture parameterë¥¼ updateí•œë‹¤.

    - ë‚®ì€ í™•ë¥ ì˜ pathë“¤ì„ pruningí•˜ë©° ìµœì¢… pathë¥¼ ì°¾ì•„ë‚¸ë‹¤.

> ProxylessNAS ì—­ì‹œ 7.7.3ì ˆì˜ ë¯¸ë¶„ ê°€ëŠ¥í•œ ê·¼ì‚¬ì‹ì„ ì‚¬ìš©í•´ì„œ updateë¥¼ ìˆ˜í–‰í•œë‹¤.

---

### 7.6.4 Bayesian optimization

> [3Blue1Brown youtube: Bayes theorem](https://youtu.be/HZGCoVF3YvM)

Bayes' theoremì„ ìƒê¸°í•´ ë³´ì. ì–´ë–¤ ì‚¬ê±´ì´ ì„œë¡œ ë°°ë°˜(mutally exclusive events)ì¸ event ë‘˜ì— ì˜í•´ ì¼ì–´ë‚œë‹¤ê³  í•  ë•Œ, ì´ê²ƒì´ ë‘ ì›ì¸ ì¤‘ í•˜ë‚˜ì¼ í™•ë¥ ì„ êµ¬í•˜ëŠ” ì •ë¦¬ë‹¤.(ì‚¬í›„ í™•ë¥ )

$$ P(B|A) = {{P(A|B)P(B)} \over {P(A)}} $$

ë‹¤ì‹œ ë§í•´ ê¸°ì¡´ ì‚¬ê±´ë“¤ì˜ í™•ë¥ (ì‚¬ì „ í™•ë¥ )ì„ ë°”íƒ•ìœ¼ë¡œ, ì–´ë–¤ ì‚¬ê±´ì´ ì¼ì–´ë‚¬ì„ ë•Œì˜ í™•ë¥ (ì‚¬í›„ í™•ë¥ )ì„ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤.
 
NASì—ì„œ **Bayesian optimization**ì„ ì ìš©í•˜ë©´, exploitationê³¼ exploration ì¤‘ ì–´ë–¤ ê²ƒì„ ìˆ˜í–‰í• ì§€ ì œì•ˆì„ ë°›ì„ ìˆ˜ ìˆë‹¤.

> [exploration(íƒìƒ‰)ê³¼ exploitation(í™œìš©)](https://github.com/erectbranch/Neural_Networks_and_Deep_Learning/tree/master/ch09)

![Bayesian optimization](images/bayesian_optimization.png)

- explorationì´ ìš°ì„ ì ì¼ ë•Œ: a to c

- exploitationì´ ìš°ì„ ì¼ ë•Œ: a to b

> í˜„ì¬ ë„ë¦¬ ì“°ì´ì§€ ì•ŠëŠ”ë‹¤. ë” ë„ë¦¬ ì“°ì´ëŠ” ê±´ ì•„ë˜ gradient-based searchì´ë‹¤.

---

### 7.6.5 gradient-based search

> [DARTS](https://arxiv.org/pdf/1806.09055.pdf)

ëŒ€í‘œì ì¸ ì˜ˆì‹œê°€ ë°”ë¡œ DARTS(Differentiable Architecture Search)ì´ë‹¤. ProxylessNAS(7.7.3.1ì ˆ)ì—ì„œ ì ì‹œ ë´¤ë˜ ê²ƒì²˜ëŸ¼, **DARTS**ëŠ” (weighted sum ê¸°ë°˜) mixed operation function $m_{O}$ ì„ ì‚¬ìš©í•´ì„œ ë¯¸ë¶„ ê°€ëŠ¥í•œ ê·¼ì‚¬ì‹ìœ¼ë¡œ gradient descentë¥¼ ì ìš©í•œë‹¤. ì´ë ‡ê²Œ ê° connectionë§ˆë‹¤ ì–´ë–¤ operationì´ ìµœì ì˜ operationì— í•´ë‹¹ë˜ëŠ”ì§€ íŒŒì•…í•œë‹¤.

![DARTS](images/DARTS.png)

ì•„ë˜ëŠ” learnable blockë§ˆë‹¤ architecture parameterì™€, loss functionì— latency penalty termì„ ì¶”ê°€í•œ ê²ƒì„ ë‚˜íƒ€ë‚¸ ê·¸ë¦¼ì´ë‹¤.

![DARTS architecture parameter](images/gradient-based_search.png)

- $F$ : latency prediction model

ê° block $i$ ë§ˆë‹¤ latencyì˜ í‰ê· ì€ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.

$$ \mathbb{E}[\mathrm{latency}_i] = \sum_{j}{p_{j}}^{i} \times F(o_{j}^{i}) $$

- ${p_{j}}^{i}$ : opreationì˜ probability

- $F(o_{j}^{i})$ : operation $o_{j}^{i}$ ì˜ latency prediction model

ê·¸ ë‹¤ìŒ ëª¨ë“  learnable blockë“¤ì˜ latency í•©ì‚°ì„ êµ¬í•œ ë’¤ ì´ë¥¼ loss functionì— ì¶”ê°€í•˜ë©´ latency penalty termì„ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤.

$$ \mathbb{E}[\mathrm{latency}] = \sum_{i}{\mathbb{E}[\mathrm{latency}_i]} $$

$$ Loss = {Loss}_{CE} + {\lambda}_{1}{||w||}_{2}^{2} + {\lambda}_{2}\mathbb{E}[\mathrm{latency}] $$

- CE: Cross Entropyë¥¼ ì˜ë¯¸í•œë‹¤.

ë”°ë¼ì„œ accuracyë§Œì´ ì•„ë‹ˆë¼ latencyê¹Œì§€ ê³ ë ¤í•˜ëŠ” NASë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤.

---

### 7.6.6 Evolutionary search

**Evolutionary search**ëŠ” ì£¼ì–´ì§„ networkë¥¼ ë°”íƒ•ìœ¼ë¡œ ì´ë¥¼ **mutate**í•˜ëŠ” ë°©ì‹ì´ë‹¤. depthë‚˜ layer, channel ìˆ˜ ë“±ì„ ë°”ê¿”ê°€ë©° ì´ë“¤ì„ cross-overí•œë‹¤.

ë‹¤ìŒ ê·¸ë¦¼ì€ depthë¥¼ mutationí•œ ê²½ìš°ì´ë‹¤.

![mutation on depth](images/mutation_depth.png)

> MBëŠ” MobileNetì˜ inverted bottleneck convolution layerì„ ì˜ë¯¸í•œë‹¤. MB3ì€ expansion ratioê°€ 3, MB6ì€ expansion ratioê°€ 6ì¸ ê²ƒì„ ì˜ë¯¸í•œë‹¤.

- stage 1ì˜ depthê°€ 3ì—ì„œ 2ë¡œ mutateë˜ì—ˆë‹¤.

- stage 2ì˜ depthê°€ 3ì—ì„œ 4ë¡œ mutateë˜ì—ˆë‹¤.

ë‹¤ìŒì€ operatorë¥¼ mutationí•œ ê²½ìš°ì´ë‹¤. 3x3 convolutionì„ 5x5 convolutionìœ¼ë¡œ ë°”ê¾¸ëŠ” ë“±ì˜ mutationì´ ì¼ì–´ë‚œë‹¤.

![mutation on opeartor](images/mutation_operator.png)

> CNNì´ ë°œì „í•˜ë©´ì„œ parameter ìˆ˜ê°€ ë” ë§ì€ ë” í° kernelì„ ì‚¬ìš©í•œ ê²ƒì²˜ëŸ¼, mutationì—ì„œ ë” í° kernelì„ ì“°ë„ë¡ mutationì´ ì¼ì–´ë‚¬ë‹¤.

> ë˜í•œ GPUì˜ parallelism ê´€ì ì—ì„œ ë” íš¨ìœ¨ì ì´ë‹¤.

ë‹¤ìŒì€ cross-overê°€ ì¼ì–´ë‚œ child networkë¥¼ ë‚˜íƒ€ë‚¸ ê·¸ë¦¼ì´ë‹¤. ë‘ parentì—ì„œ randomí•˜ê²Œ operatorë¥¼ ì„ íƒí•´ì„œ child networkì— ì ìš©í•œë‹¤.

![Evolutionary search crossover](images/crossover.png)

---