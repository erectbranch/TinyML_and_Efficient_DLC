# 7 Neural Architecture Search (Part I)

> [Lecture 07 - Neural Architecture Search (Part I) | MIT 6.S965](https://www.youtube.com/watch?v=NQj5TkqX48Q)

> [Exploring the Implementation of Network Architecture Search(NAS) for TinyML Application](http://essay.utwente.nl/89778/1/nieuwenhuis_MA_EEMCS.pdf)

ë‹¤ìŒì€ 7.2ì ˆ benchmarkì— NASë¥¼ ì´ìš©í•´ ì°¾ì•„ë‚¸ neural architectureë¥¼ ì¶”ê°€í•œ ê²ƒì´ë‹¤. 

- NASë¥¼ ì´ìš©í•´ ì°¾ì•„ë‚¸ neural architectureëŠ” í›¨ì”¬ ì ì€ ì—°ì‚°ëŸ‰(MACs)ìœ¼ë¡œë„ manually-designed modelë³´ë‹¤ ë” ë†’ì€ accuracyë¥¼ ê°–ëŠ”ë‹¤.

![automatic design](images/automatic_design.png)

---

## 7.3 Neural Architecture Search

**Neural Architecture Search**(NAS)ì˜ ëª©í‘œëŠ” **Search Space**(íƒìƒ‰ ê³µê°„)ì—ì„œ íŠ¹ì • ì „ëµì„ í†µí•´ ìµœì ì˜ neural network architectureë¥¼ ì°¾ëŠ” ê²ƒì´ë‹¤.

> ëª©í‘œë¡œ í•˜ëŠ” ì„±ëŠ¥ì€ accuracy, efficiency, latency ë“±ì´ ë  ìˆ˜ ìˆë‹¤.

![NAS](images/NAS.png)

- **Search Space**(íƒìƒ‰ ê³µê°„) 

  íƒìƒ‰í•  neural network architectureì´ ì •ì˜ëœ ê³µê°„ì´ë‹¤.

  > ì ì ˆí•œ domain ì§€ì‹ì„ ì ‘ëª©í•˜ë©´ search space í¬ê¸°ë¥¼ ì¤„ì¼ ìˆ˜ ìˆë‹¤. í•™ìŠµ ì‹œê°„ì˜ ë‹¨ì¶•ê³¼ accuracy í–¥ìƒì— ë„ì›€ì´ ëœë‹¤.

- **Search Strategy**(íƒìƒ‰ ì „ëµ)

  search spaceë¥¼ ì–´ë–»ê²Œ íƒìƒ‰í• ì§€ ê²°ì •í•œë‹¤. 

- **Performance Estimation Strategy**(ì„±ê³¼ í‰ê°€ ì „ëµ)

  ì„±ëŠ¥ì„ ì¸¡ì •í•  ë°©ë²•ì„ ê²°ì •í•œë‹¤. 

---

## 7.4 Search Space

ëª¨ë¸ êµ¬ì¡°ë¥¼ ì°¾ê¸° ìœ„í•´ì„œëŠ” search spaceë¥¼ ë¨¼ì € ì •ì˜í•´ì•¼ í•œë‹¤. search space ë‚´ ëª¨ë¸ì€ ë‹¤ì–‘í•œ ì˜µì…˜ì„ ê°€ì§€ê²Œ ëœë‹¤. ì•„ë˜ëŠ” ëŒ€í‘œì ì¸ ì˜ˆì‹œë‹¤.

- ë‹¤ì–‘í•œ ë ˆì´ì–´

  convolution layer, fully connected layer, pooling layer ë“±

- ë ˆì´ì–´ê°€ ê°–ëŠ” íŠ¹ì§•

  \#neurons, kernel size, activation function ë“± 

ë˜í•œ search spaceë¥¼ ì–´ë–¤ ë‹¨ìœ„ë¡œ êµ¬ì„±í•˜ëŠ”ê°€ì— ë”°ë¼, í¬ê²Œ **cell-level search space**ì™€ **network-level search space**ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤.

---

### 7.4.1 Cell-level Search Space

> [Learning Transferable Architectures for Scalable Image Recognition ë…¼ë¬¸(2017)](https://arxiv.org/abs/1707.07012)

ImageNet ë°ì´í„°ì…‹ì„ ì…ë ¥ìœ¼ë¡œ ë°›ëŠ” CNN êµ¬ì¡° ì˜ˆì‹œë¥¼ ë³´ì.

![ImageNet dataset arch ex](images/classifier_architecture_ex.png)

- Reduction Cell: í•´ìƒë„ë¥¼ ì¤„ì¸ë‹¤.(stride > 2)

- Normal Cell: í•´ìƒë„ê°€ ìœ ì§€ëœë‹¤.(stride = 1)

ë…¼ë¬¸(NASNet)ì—ì„œëŠ” RNN controllerë¥¼ ì´ìš©í•˜ì—¬ candidate cellë¥¼ ìƒì„±í•œë‹¤. ì´ ë‹¤ì„¯ ë‹¨ê³„ë¡œ êµ¬ì„±ëœ ë‹¨ê³„ë¥¼ ì„¤ëª…í•˜ëŠ” ì•„ë˜ ê·¸ë¦¼ì„ ì‚´í´ë³´ì.

![cell-level search space](images/cell-level_search_space.png)

- $h_{i+1}$ ì„ ë§Œë“¤ê¸° ìœ„í•œ input candidate: $h_i$ hidden layer í˜¹ì€ $h_{i-1}$ hidden layerë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

1. hidden layer A: ì²« ë²ˆì§¸ hidden stateë¥¼ ìƒì„±í•œë‹¤.

2. hidden layer B: ë‘ ë²ˆì§¸ hidden stateë¥¼ ìƒì„±í•œë‹¤.

3. hidden layer Aê°€ ê°€ì§ˆ operationì„ ê³ ë¥¸ë‹¤.(`3x3 conv`)

   > ì˜ˆë¥¼ ë“¤ë©´ convolution/pooling/identityì™€ ê°™ì€ ë ˆì´ì–´ê°€ ë  ìˆ˜ ìˆë‹¤.

4. hidden layer Bê°€ ê°€ì§ˆ operationì„ ê³ ë¥¸ë‹¤.(`2x2 maxpool`)

5. Aì™€ Bë¥¼ í•©ì¹  ë°©ë²•ì„ ê³ ë¥¸ë‹¤.(`add`)

ë‹¤ìŒì€ NASNetì„ ì„¤ëª…í•˜ëŠ” ë„ì‹ì´ë‹¤.

![NASNet ex](images/NASNet_ex.png)

í•˜ì§€ë§Œ ë„ˆë¬´ í° search spaceë¡œ ì¸í•´, **search cost**ì™€ **hardware efficiency**ë©´ì—ì„œ ë” íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ í•„ìš”í•´ì¡Œë‹¤.

### <span style='background-color: #393E46; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ“ ì˜ˆì œ 1: Cell-level Search Space size &nbsp;&nbsp;&nbsp;</span>

ë‹¤ìŒ ì¡°ê±´ì—ì„œ NASNetì˜ search space sizeë¥¼ êµ¬í•˜ë¼.

- 2 candidate input

- M input transform operations

- N combine hidden states operations

- B \#layers

### <span style='background-color: #C2B2B2; color: #F7F7F7'>&nbsp;&nbsp;&nbsp;ğŸ” í’€ì´&nbsp;&nbsp;&nbsp;</span>

search space sizeëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

$$(2 \times 2 \times M \times M \times N)^{B} = 4^{B}M^{2B}N^{B}$$

> $M=5, N=2, B=5$ ë¼ê³  í•˜ë©´ search space í¬ê¸°ëŠ” $3.2 \times 10^{11}$ ì´ ëœë‹¤.

---

### 7.4.2 Network-Level Search Space

> [MnasNet: Platform-Aware Neural Architecture Search for Mobile ë…¼ë¬¸(2018)](https://arxiv.org/abs/1807.11626)

> [Once-for-All: Train One Network and Specialize it for Efficient Deployment ë…¼ë¬¸(2019)](https://arxiv.org/abs/1908.09791)

**network-level search space**ì€ í”íˆ ì“°ì´ëŠ” patternì€ ê³ ì •í•˜ê³ , ì˜¤ì§ ê° stageê°€ ê°–ëŠ” blocksì„ ë³€í™”ì‹œí‚¤ëŠ” ë°©ë²•ì´ë‹¤.

---

#### 7.4.2.1 Network-Level Search Space for Image Segmantation

> [Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation ë…¼ë¬¸(2019)](https://arxiv.org/abs/1901.02985)

**Image Segmentation** domainì—ì„œ network-level search spaceë¥¼ êµ¬ì„±í•œ Auto-DeepLab ë…¼ë¬¸ì—ì„œëŠ”, layerë³„ upsampling/downsamplingì„ search spaceë¡œ ì‚¬ìš©í•œë‹¤. 

> Image Segmentation: imageë¥¼ ì—¬ëŸ¬ í”½ì…€ ì§‘í•©ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” task

![network-level search space](images/network-level_search_space_ex.png)

- ê°€ë¡œ: \#layers, ì„¸ë¡œ Downsample(í•´ìƒë„ê°€ ì¤„ì–´ë“ ë‹¤.)

- íŒŒë€ìƒ‰ nodesë¥¼ ì—°ê²°í•˜ëŠ” pathê°€ candidate architectureê°€ ëœë‹¤.

---

#### 7.4.2.2 Network-Level Search Space for Object Detection

> [NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection](https://arxiv.org/abs/1904.07392)

NAS-FPN ë…¼ë¬¸ì—ì„œëŠ” **Object Detection** domainì—ì„œ í”íˆ ì“°ì´ëŠ” FPN(Feature Pyramid Networks for Object Detection) ëª¨ë¸ì„ Network-Level Search Spaceë¡œ êµ¬ì„±í•œë‹¤.

![NAS-FPN](images/NAS-FPN.png)

> AP: average precision(í‰ê·  ì •ë°€ë„)

ì´ì²˜ëŸ¼ NASë¥¼ í†µí•´ ì–»ì€ ìµœì  network architectureëŠ” ì‚¬ëŒì´ ë””ìì¸í•œ êµ¬ì¡°ì™€ ìƒë‹¹íˆ ë‹¤ë¥¸ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. 

> í•˜ì§€ë§Œ accuracyì™€ irregularity ì‚¬ì´ì—ì„œ ê· í˜•ì„ ë§ì¶°ì•¼ í•œë‹¤. irregularity topologyëŠ” hardwareìƒìœ¼ë¡œ êµ¬í˜„í•˜ê¸° í˜ë“¤ê³ , ë˜í•œ parallelismì˜ ì´ì ì„ ì‚´ë¦¬ê¸° ì–´ë µê¸° ë•Œë¬¸ì´ë‹¤.

---

## 7.5 Design the Search Space

ê·¸ë ‡ë‹¤ë©´ íš¨ìœ¨ì ì¸ search spaceëŠ” ì–´ë–¤ ê¸°ì¤€ìœ¼ë¡œ ì„ íƒí•´ì•¼ í• ê¹Œ?

---

### 7.5.1 Cumulative Error Distribution

> [On Network Design Spaces for Visual Recognition ë…¼ë¬¸(2019)](https://arxiv.org/abs/1905.13214)

ë…¼ë¬¸(RegNet)ì—ì„œëŠ” **cumulative error distribution**ì„ ì§€í‘œë¡œ ì‚¬ìš©í•œë‹¤.

![ResNet cumulative error distribution](images/ResNet_cumulative_error_distribution.png)

- íŒŒë€ìƒ‰ ê³¡ì„ : 38.9% modelì´ 49.4%ê°€ ë„˜ëŠ” errorë¥¼ ê°€ì§„ë‹¤.

- ì£¼í™©ìƒ‰ ê³¡ì„ : 38.7% modelì´ 43.2%ê°€ ë„˜ëŠ” errorë¥¼ ê°€ì§„ë‹¤.

  íŒŒë€ìƒ‰ ê³¡ì„ ë³´ë‹¤ errorê°€ ì ìœ¼ë¯€ë¡œ ë” ìš°ìˆ˜í•œ search spaceì´ë‹¤.

í•˜ì§€ë§Œ cumulative error distributionì„ ì¸¡ì •í•˜ê¸° ìœ„í•´ì„œ êµ‰ì¥íˆ ê¸´ ì‹œê°„ë™ì•ˆ trainingì„ ìˆ˜í–‰í•´ì•¼ í•˜ëŠ” ë‹¨ì ì´ ìˆë‹¤.

---

### 7.5.2 FLOPs distribution

> [MCUNet: Tiny Deep Learning on IoT Devices ë…¼ë¬¸(2020)](https://arxiv.org/abs/2007.10319)

MCUNet ë…¼ë¬¸ì˜ TinyNASëŠ” MCU ì œì•½ì¡°ê±´ì— ì•Œë§ì€ search spaceë¥¼ ì°¾ê¸° ìœ„í•´, ë‹¤ìŒê³¼ ê°™ì€ ìµœì í™” ê³¼ì •ì„ ê±°ì¹œë‹¤.

1. Automated search space optimization

2. Resource-constrained model specialization

![TinyNAS](images/TinyNAS.png)

ì´ëŸ¬í•œ ìµœì í™” ê³¼ì •ì—ì„œ ì œì¼ í•µì‹¬ì´ ë˜ëŠ” heuristicì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

- ë™ì¼í•œ memory constraintì—ì„œëŠ” <U>FLOPsê°€ í´ìˆ˜ë¡ í° model capacityë¥¼ ê°–ëŠ”ë‹¤.</U>

- í° model capacityëŠ” ë†’ì€ accuracyì™€ ì§ê²°ëœë‹¤. 

![FLOPs distribution](images/FLOPs_and_probability.png)

---

## 7.6 Search Strategy

---

### 7.6.1 Grid Search

> [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks ë…¼ë¬¸(2019)](https://arxiv.org/abs/1905.11946)

ê°€ì¥ ê°„ë‹¨í•œ íƒìƒ‰ ë°©ë²•ìœ¼ë¡œëŠ” **grid search**ê°€ ìˆë‹¤. 

![grid search ex](images/grid_search_ex.png)

- resolution, width ì¡°ê±´ì„ ì„¤ì •í•œë‹¤.

- latency constraint: íŒŒë€ìƒ‰ì€ ë§Œì¡± / ë¹¨ê°„ìƒ‰ì€ ë¶ˆë§Œì¡±

í•˜ì§€ë§Œ ìœ„ ì˜ˆì‹œì™€ ë‹¤ë¥´ê²Œ ì‹¤ì œ ì‘ìš©ì—ì„œëŠ” ì„ íƒì§€ì™€ ì´ì— ë”°ë¥¸ ì°¨ì›ì´ êµ‰ì¥íˆ ì»¤ì§€ê²Œ ëœë‹¤. ë„“ì€ ë²”ìœ„ì— stepì„ ì‘ê²Œ ì„¤ì •í• ìˆ˜ë¡ optimalì„ ì°¾ì„ ê°€ëŠ¥ì„±ì€ ì»¤ì§€ì§€ë§Œ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦°ë‹¤.

> ëŒ€ì²´ë¡œ ë„“ì€ ë²”ìœ„ì™€ í° stepìœ¼ë¡œ ì„¤ì •í•œ ë’¤, ì„œì„œíˆ ì¤„ì—¬ë‚˜ê°€ëŠ” ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤.

grid searchë¥¼ ì‚¬ìš©í•œ ëŒ€í‘œì ì¸ ë…¼ë¬¸ìœ¼ë¡œëŠ” **EfficientNet**ê°€ ìˆë‹¤. 

- depth, width, resolutionì„ í•¨ê»˜ **compound scaling**í•œë‹¤.

  ![compound scaling](images/compound_scaling.png)

  - $\phi$ : compound scaling parameter

  - $\alpha \ge 1, \beta \ge 1, \gamma \ge 1$

$$ depth = d = {\alpha}^{\phi} $$

$$ width = w = {\beta}^{\phi} $$

$$ resolution = r = {\gamma}^{\phi} $$

$$ \mathrm{s.t.} \, \alpha \cdot {\beta}^2 \cdot {\gamma}^2 \approx 2 $$

ë‹¤ìŒì€ **depth**, **width**, **resolution**ê°€ ê°ê° accuracyì— ì–´ë– í•œ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ ë‚˜íƒ€ë‚¸ ë„í‘œë‹¤.(ImageNet ë°ì´í„°ì…‹ ì‚¬ìš© ResNet ê¸°ë°˜ search space)

![EfficientNet graph](images/efficientnet_graph.png)

- **depth**( $d$ )

  $d$ ê°€ ì»¤ì§€ë©´ ë‹¤ìŒê³¼ ê°™ì€ ë³€í™”ê°€ ë‚˜íƒ€ë‚œë‹¤.
  
  - model capacityê°€ ì»¤ì§„ë‹¤.

  - \#parametersê°€ ì»¤ì§„ë‹¤. ë”°ë¼ì„œ memory footprintê°€ ëŠ˜ì–´ë‚œë‹¤.

  - \#FLOPsê°€ ì»¤ì§€ë©° accuracyê°€ í–¥ìƒë˜ì§€ë§Œ, latencyë„ ëŠ˜ì–´ë‚œë‹¤.

  - vanishing gradient ë¬¸ì œë¥¼ ê²ªì„ ê°€ëŠ¥ì„±ì´ í¬ë‹¤.
  
    > skip connectionì´ë‚˜ batch normalizationê³¼ ê°™ì€ ë°©ë²•ìœ¼ë¡œ ë°©ì§€í•œë‹¤.

- **width**( $w$ )

  $w$ ê°€ ì»¤ì§€ë©´ ë‹¤ìŒê³¼ ê°™ì€ ë³€í™”ê°€ ë‚˜íƒ€ë‚œë‹¤.

  - ë” fine-grained featureë¥¼ ê°€ì§€ë©° trainingì´ ìš©ì´í•´ì§„ë‹¤.

  > ì°¸ê³ ë¡œ width scalingì€ ì£¼ë¡œ tiny modelì—ì„œ ì‚¬ìš©í•œë‹¤. ë„“ê¸°ë§Œ í•˜ê³  ì–•ì€ networkì—ì„œëŠ” high level featureë¥¼ ì–»ê¸° í˜ë“¤ê¸° ë•Œë¬¸ì´ë‹¤.

- **resolution**( $r$ )

  $r$ ì´ ì»¤ì§€ë©´ ë‹¤ìŒê³¼ ê°™ì€ ë³€í™”ê°€ ë‚˜íƒ€ë‚œë‹¤.

  - ë³´ë‹¤ fine-grained featureë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.

  > ì˜ˆë¥¼ ë“¤ì–´ GPipeì—ì„œëŠ” ê¸°ì¡´ë³´ë‹¤ í›¨ì”¬ í° 480x480 resolutionì„ ì‚¬ìš©í•˜ì—¬ SOTA accuracyë¥¼ ì–»ì€ ì  ìˆë‹¤. í•˜ì§€ë§Œ ë„ˆë¬´ í° resolutionì€ ë°˜ëŒ€ë¡œ accuracy gainì´ ì¤„ì–´ë“œëŠ” ë¶€ì‘ìš©ì„ ë‚³ì„ ìˆ˜ ìˆë‹¤.

---

### 7.6.2 Random Search

> [Single Path One-Shot Neural Architecture Search with Uniform Sampling ë…¼ë¬¸(2019)](https://arxiv.org/abs/1904.00420): SPOSë¡œ ì§€ì¹­

í˜¹ì€ grid searchì˜ ë‹¨ì ì„ ê°œì„ í•œ **random search**ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

- íŠ¹íˆ ì°¨ì›ì´ ì ì„ìˆ˜ë¡ ìµœì„ ì˜ parameter search strategyì¼ ê°€ëŠ¥ì„±ì´ ìˆë‹¤.

![grid search, random search](images/grid_random_search.png)

ì˜ˆë¥¼ ë“¤ì–´ SPOS ë…¼ë¬¸ì—ì„œëŠ” ì¢‹ì€ search spaceë¥¼ ê°€ì§„ë‹¤ë©´, evolutionary searchì™€ ê°™ì€ advanced methodë³´ë‹¤ë„ random searchê°€ êµ‰ì¥íˆ ì¢‹ì€ **baseline**ì„ ì œê³µí•  ìˆ˜ ìˆë‹¤ê³  ì£¼ì¥í•œë‹¤.

![evolution vs random](images/evolution_vs_random.png)

---
 
### 7.6.3 Reinforcement Learning

> [Neural Architecture Search with Reinforcement Learning ë…¼ë¬¸(2016)](https://arxiv.org/abs/1611.01578)

> [Introduction to Neural Architecture Search (Reinforcement Learning approach)](https://smartlabai.medium.com/introduction-to-neural-architecture-search-reinforcement-learning-approach-55604772f173)

> [Policy Gradient Algorithms](https://talkingaboutme.tistory.com/entry/RL-Policy-Gradient-Algorithms)

ì´ë³´ë‹¤ advanced methodë¡œ RNN controllerë¥¼ ì‚¬ìš©í•˜ëŠ” **Reinforcement Learning** ì ‘ê·¼ë²•ì„ ë“¤ ìˆ˜ ìˆë‹¤. 

![RL-based NAS](images/RL-based_NAS.png)

- controller(RNN)ì´ sample architecture(child network)ë¥¼ ìƒì„±í•œë‹¤.

- child network training í›„, validation accuracyë¥¼ ì–»ëŠ”ë‹¤.

- accuracyë¥¼ ë°”íƒ•ìœ¼ë¡œ controller policyë¥¼ updateí•œë‹¤.

ê·¸ëŸ°ë° ë¬¸ì œëŠ” updateë¥¼ ìœ„í•œ R(accuracy)ì€ ë¯¸ë¶„ ê°€ëŠ¥í•˜ì§€ ì•Šë‹¤.

```math
J({\theta}_{c}) = E_{P(a_{1:T};{\theta}_{c})}[R]
```

- $\theta$ : validation accuracy

- $a$ : **action** 

  controllerê°€ sample networkê°€ ê°–ëŠ” hyperparameter í•˜ë‚˜ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê³¼ì •. 
  
  > ì—¬ëŸ¬ actionì„ ê±°ì³ì„œ í•˜ë‚˜ì˜ sample network architectureê°€ ìƒì„±ëœë‹¤.

- $a_{1}:T$ : sampleì„ ìƒì„±í•˜ëŠ” ë° ë°œìƒí•œ action list

```math
J({\theta}_{c}) = E_{P(a_{1:T};{\theta}_{c})}[R]
```

ë”°ë¼ì„œ ë‹¤ìŒê³¼ ê°™ì´ policy gradient methodë¥¼ ì´ìš©í•˜ì—¬ ë¯¸ë¶„ ê°€ëŠ¥í•˜ë„ë¡ ì‹ì„ ìˆ˜ì •í•œë‹¤.

```math
{\nabla}_{{\theta}_{c}}J({\theta}_{c}) = \sum_{t=1}^{T}{E_{P(a_{1:T};{\theta}_{c})}[{\nabla}_{{\theta}_{c}} {\log}P({\alpha}_{t}|{\alpha}_{(t-1):1};{\theta}_{c})R]}
```

> ì‹¤ì œë¡œëŠ” ë” approximateí•˜ë©° baselineì´ ì¶”ê°€ëœ ì‹ì„ ì‚¬ìš©í•œë‹¤. baselineì€ ì§€ë‚œ architectureì˜ í‰ê·  accuracyë¡œ ì‚°ì¶œí•œë‹¤.

---

#### 7.6.3.1 ProxylessNAS

> [ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware ë…¼ë¬¸(2018)](https://arxiv.org/abs/1812.00332)

> ë” ìì„¸í•œ ì •ë¦¬ëŠ” 8.3.1ì ˆ ì°¸ì¡°

RL-based search strategyë¥¼ ì‚¬ìš©í•˜ëŠ” ëŒ€í‘œì ì¸ NASë¡œ ProxylessNASê°€ ìˆë‹¤. 

![update parameters](images/update_parameters.png)

- **gate**

  0ê³¼ 1ë¡œ ì´ë£¨ì–´ì§„ binary vector(**probability**)ë¡œ, architecture pathì˜ í™œì„±í™”/ë¹„í™œì„±í™”ë¥¼ ì œì–´í•œë‹¤.

  - 0: prune, 1: keep

  - í™œì„±í™”ëœ pathë§Œì„ memoryì— ìœ ì§€í•˜ë©´ ëœë‹¤. 

```math
g = \mathrm{binarize}(p_1, \cdots , p_N) = \begin{cases} [1, 0, \cdots, 0] & \mathrm{with} \, \mathrm{probability} \, p_1 \\ \cdots \\ [0, 0, \cdots, 1] & \mathrm{with} \, \mathrm{probability} \, p_N \end{cases}
```

- architecture parametersë¥¼ ì´ìš©í•œ ê·¼ì‚¬ì‹ìœ¼ë¡œ gradient updateê°€ ê°€ëŠ¥í•˜ë‹¤.

---

### 7.6.4 Bayesian optimization

> [Neural Architecture Search with Bayesian Optimisation and Optimal Transport ë…¼ë¬¸(2018)](https://arxiv.org/abs/1802.07191)

> [3Blue1Brown youtube: Bayes theorem](https://youtu.be/HZGCoVF3YvM)

> [exploration(íƒìƒ‰)ê³¼ exploitation(í™œìš©) ê°œë… ì •ë¦¬](https://github.com/erectbranch/Neural_Networks_and_Deep_Learning/tree/master/ch09)

ì‚¬ì „í™•ë¥ ì„ ì´ìš©í•´ ì‚¬í›„í™•ë¥ ì„ êµ¬í•  ìˆ˜ ìˆëŠ” Bayes' theorem(ë² ì´ì¦ˆ ì •ë¦¬)ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í–ˆë‹¤.

$$ P(B|A) = {{P(A|B)P(B)} \over {P(A)}} $$

ë…¼ë¬¸ì—ì„œëŠ” exploitationê³¼ exploration ì¤‘ ì–´ë–¤ ê²ƒì„ ìˆ˜í–‰í• ì§€ **Bayesian optimization**ì„ ì ìš©í•´ì„œ ê²°ì •í•œë‹¤.

![Bayesian optimization](images/bayesian_optimization.png)

- exploration(íƒìƒ‰) ìš°ì„ : 1ë²ˆì§¸ì—ì„œ 3ë²ˆì§¸ë¥¼ ìˆ˜í–‰í•œë‹¤.

- exploitation(í™œìš©) ìš°ì„ : 1ë²ˆì§¸ì—ì„œ 2ë²ˆì§¸ë¥¼ ìˆ˜í–‰í•œë‹¤.

---

### 7.6.5 Gradient-based Search

> [DARTS: Differentiable Architecture Search ë…¼ë¬¸(2018)](https://arxiv.org/abs/1806.09055)

> [DARTS ë…¼ë¬¸ ë¦¬ë·°](https://www.kc-ml2.com/posts/blog_DARTS)

DARTS  ë…¼ë¬¸ì—ì„œëŠ” gradient descentê°€ ê°€ëŠ¥í•˜ë„ë¡ ë¯¸ë¶„ ê°€ëŠ¥í•œ ê·¼ì‚¬ì‹ì„ ì •ì˜í•œ NASë¥¼ ì œì•ˆí•œë‹¤.

![DARTS](images/DARTS.png)

- (a): 0, 1, 2, 3ìœ¼ë¡œ ì´ì–´ì§ˆ probabilityê°€ ì¡´ì¬í•œë‹¤.

  > ì¼ì§ì„ ë§Œì´ ì•„ë‹ˆë¼ skip connectionê³¼ ê°™ì€ ë‹¤ì–‘í•œ ì—°ê²°ì„ í‘œí˜„í•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤.

- ê° pathëŠ” operation candidate setì„ ê°€ì§€ê³  ìˆë‹¤.(**mixed operation**)

  > convolution, avgpool, maxpool ë“±

  - operation $i$ : ì„ íƒí•  í™•ë¥  ${\alpha}_i$ ë¥¼ ê°–ëŠ”ë‹¤.

    ìµœì¢…ì ìœ¼ë¡œ pathì—ì„œëŠ” ë†’ì€ ${\alpha}_i$ ë¥¼ ê°–ëŠ” operationì´ ì„ íƒëœë‹¤.

$$ \sum_{i=1}^{n}{\alpha}_i = 1 $$

- edge $(i, j)$ ì—ì„œì˜ mixed operationì€ ë‹¤ìŒê³¼ ê°™ì€ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.

```math
{\bar{o}}^{(i,j)}(x) = \sum_{o \in \mathcal{O}} {{\exp({\alpha}_{o}^{(i,j)})} \over {\sum_{o' \in \mathcal{O}}{\exp({\alpha}_{o'}^{(i,j)})}}}o(x)
```

í•˜ì§€ë§Œ gradientë¥¼ ì €ì¥í•˜ê¸° ìœ„í•´ êµ‰ì¥íˆ ë§ì€ memoryë¥¼ ì†Œëª¨í•˜ê²Œ ë˜ë©´ì„œ, ProxlessNASì™€ ê°™ì€ ëŒ€ì•ˆì´ ë“±ì¥í•˜ê²Œ ëœë‹¤.

---

### 7.6.6 Evolutionary search

> [Once-for-All: Train One Network and Specialize it for Efficient Deployment ë…¼ë¬¸(2019)](https://arxiv.org/abs/1908.09791)

**Evolutionary Search**, ì„¸ë¶€ì ìœ¼ë¡œëŠ” **Genetic Algorithm**ëŠ” ì£¼ì–´ì§„ network architectureì— **mutation**, **crossover**ì™€ ê°™ì€ ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í•˜ì—¬ ë” ë‚˜ì€ networkë¥¼ ì°¾ëŠ” ë°©ë²•ì´ë‹¤.

> MB{n}: expansion ratioê°€ nì¸ inverted bottleneck.

- **mutation**

  depth, width, operator ë“±ì„ ë³€í™”ì‹œí‚¬ ìˆ˜ ìˆë‹¤.

  ![mutation on depth](images/mutation_depth.png)

  - stage 1: depthê°€ 3ì—ì„œ 2ë¡œ mutate

  - stage 2: depthê°€ 3ì—ì„œ 4ë¡œ mutate

  ![mutation on operator](images/mutation_operator.png)

  - stage 1: ë‘ operatorê°€ 3x3ì—ì„œ 5x5 kernelë¡œ mutate

  - stage 2: ì„¸ operatorì˜ kernelê³¼ expansion ratioê°€ mutate

- **crossover**

  ê° ë ˆì´ì–´ë§ˆë‹¤ parent networkì˜ operatorë¥¼ ì„ì˜ë¡œ ê°€ì ¸ì™€ì„œ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ë¥¼ ìƒì„±í•œë‹¤.

  ![Evolutionary search crossover](images/crossover.png)

---